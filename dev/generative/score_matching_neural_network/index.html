<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Score matching a neural network · Random notes</title><meta name="title" content="Score matching a neural network · Random notes"/><meta property="og:title" content="Score matching a neural network · Random notes"/><meta property="twitter:title" content="Score matching a neural network · Random notes"/><meta name="description" content="Documentation for Random notes."/><meta property="og:description" content="Documentation for Random notes."/><meta property="twitter:description" content="Documentation for Random notes."/><meta property="og:url" content="https://github.com/rmsrosa/random_notes/generative/score_matching_neural_network/"/><meta property="twitter:url" content="https://github.com/rmsrosa/random_notes/generative/score_matching_neural_network/"/><link rel="canonical" href="https://github.com/rmsrosa/random_notes/generative/score_matching_neural_network/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/style.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="Random notes logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Random notes</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Random Notes</a></li><li><span class="tocitem">Probability Essentials</span><ul><li><a class="tocitem" href="../../probability/kernel_density_estimation/">Kernel Density Estimation</a></li><li><a class="tocitem" href="../../probability/convergence_notions/">Convergence notions</a></li></ul></li><li><span class="tocitem">Discrete-time Markov chains</span><ul><li><a class="tocitem" href="../../markov_chains/mc_definitions/">Essential definitions</a></li><li><a class="tocitem" href="../../markov_chains/mc_invariance/">Invariant distributions</a></li><li><input class="collapse-toggle" id="menuitem-3-3" type="checkbox"/><label class="tocitem" for="menuitem-3-3"><span class="docs-label">Countable-space Markov chains</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../markov_chains/mc_countableX_recurrence/">Recurrence in the countable-space case</a></li><li><a class="tocitem" href="../../markov_chains/mc_countableX_connections/">Connected states, irreducibility and uniqueness of invariant measures</a></li><li><a class="tocitem" href="../../markov_chains/mc_countableX_convergencia/">Aperiodicidade e convergência para a distribuição estacionária</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-4" type="checkbox"/><label class="tocitem" for="menuitem-3-4"><span class="docs-label">Continuous-space Markov chains</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../markov_chains/mc_irreducibility_and_recurrence/">Irreducibility and recurrence in the continuous-space case</a></li></ul></li></ul></li><li><span class="tocitem">Sampling methods</span><ul><li><a class="tocitem" href="../../sampling/overview/">Overview</a></li><li><a class="tocitem" href="../../sampling/prng/">Random number generators</a></li><li><input class="collapse-toggle" id="menuitem-4-3" type="checkbox"/><label class="tocitem" for="menuitem-4-3"><span class="docs-label">Transform methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/invFtransform/">Probability integral transform</a></li><li><a class="tocitem" href="../../sampling/box_muller/">Box-Muller transform</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-4" type="checkbox"/><label class="tocitem" for="menuitem-4-4"><span class="docs-label">Accept-Reject methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/rejection_sampling/">Rejection sampling</a></li><li><a class="tocitem" href="../../sampling/empiricalsup_rejection/">Empirical supremum rejection sampling</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-5" type="checkbox"/><label class="tocitem" for="menuitem-4-5"><span class="docs-label">Markov Chain Monte Carlo (MCMC)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/mcmc/">Overview</a></li><li><a class="tocitem" href="../../sampling/metropolis/">Metropolis and Metropolis-Hastings</a></li><li><a class="tocitem" href="../../sampling/convergence_metropolis/">Convergence of Metropolis-Hastings</a></li><li><a class="tocitem" href="../../sampling/gibbs/">Gibbs sampling</a></li><li><a class="tocitem" href="../../sampling/hmc/">Hamiltonian Monte Carlo (HMC)</a></li></ul></li><li><a class="tocitem" href="../../sampling/langevin_sampling/">Langevin sampling</a></li></ul></li><li><span class="tocitem">Bayesian inference</span><ul><li><input class="collapse-toggle" id="menuitem-5-1" type="checkbox"/><label class="tocitem" for="menuitem-5-1"><span class="docs-label">Bayes Theory</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/bayes/">Bayes Theorem</a></li><li><a class="tocitem" href="../../bayesian/bayes_inference/">Bayesian inference</a></li><li><a class="tocitem" href="../../bayesian/bernstein_vonmises/">Bernstein–von Mises theorem</a></li></ul></li><li><a class="tocitem" href="../../bayesian/bayesian_probprog/">Bayesian probabilistic programming</a></li><li><input class="collapse-toggle" id="menuitem-5-3" type="checkbox"/><label class="tocitem" for="menuitem-5-3"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/find_pi/">Estimating π via frequentist and Bayesian methods</a></li><li><a class="tocitem" href="../../bayesian/linear_regression/">Many Ways to Linear Regression</a></li><li><a class="tocitem" href="../../bayesian/tilapia_alometry/">Alometry law for the Nile Tilapia</a></li><li><a class="tocitem" href="../../bayesian/mortality_tables/">Modeling mortality tables</a></li></ul></li></ul></li><li><span class="tocitem">Generative models</span><ul><li><input class="collapse-toggle" id="menuitem-6-1" type="checkbox" checked/><label class="tocitem" for="menuitem-6-1"><span class="docs-label">Score matching</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../overview/">Overview</a></li><li><a class="tocitem" href="../stein_score/">Stein score function</a></li><li><a class="tocitem" href="../score_matching_aapo/">Score matching of Aapo Hyvärinen</a></li><li class="is-active"><a class="tocitem" href>Score matching a neural network</a><ul class="internal"><li><a class="tocitem" href="#Introduction"><span>Introduction</span></a></li><li><a class="tocitem" href="#Loss-function-for-implicit-score-matching"><span>Loss function for implicit score matching</span></a></li><li><a class="tocitem" href="#Numerical-example"><span>Numerical example</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../parzen_estimation_score_matching/">Score matching with Parzen estimation</a></li><li><a class="tocitem" href="../denoising_score_matching/">Denoising score matching of Pascal Vincent</a></li><li><a class="tocitem" href="../sliced_score_matching/">Sliced score matching</a></li><li><a class="tocitem" href="../1d_FD_score_matching/">1D finite-difference score matching</a></li><li><a class="tocitem" href="../2d_FD_score_matching/">2D finite-difference score matching</a></li><li><a class="tocitem" href="../ddpm/">Denoising diffusion probabilistic models</a></li><li><a class="tocitem" href="../mdsm/">Multiple denoising score matching</a></li><li><a class="tocitem" href="../probability_flow/">Probability flow</a></li><li><a class="tocitem" href="../reverse_flow/">Reverse probability flow</a></li><li><a class="tocitem" href="../score_based_sde/">Score-based SDE model</a></li></ul></li></ul></li><li><span class="tocitem">Sensitivity analysis</span><ul><li><a class="tocitem" href="../../sensitivity/overview/">Overview</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Generative models</a></li><li><a class="is-disabled">Score matching</a></li><li class="is-active"><a href>Score matching a neural network</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Score matching a neural network</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/rmsrosa/random_notes" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/rmsrosa/random_notes/blob/main/docs/src/generative/score_matching_neural_network.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Score-matching-a-neural-network"><a class="docs-heading-anchor" href="#Score-matching-a-neural-network">Score matching a neural network</a><a id="Score-matching-a-neural-network-1"></a><a class="docs-heading-anchor-permalink" href="#Score-matching-a-neural-network" title="Permalink"></a></h1><h2 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h2><h3 id="Aim"><a class="docs-heading-anchor" href="#Aim">Aim</a><a id="Aim-1"></a><a class="docs-heading-anchor-permalink" href="#Aim" title="Permalink"></a></h3><p>Apply the score-matching method of <a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Aapo Hyvärinen (2005)</a> to fit a neural network model of the score function to a univariate Gaussian distribution. This borrows ideas from <a href="https://papers.nips.cc/paper_files/paper/2010/hash/6f3e29a35278d71c7f65495871231324-Abstract.html">Kingma and LeCun (2010)</a>, of using automatic differentiation to differentiate the neural network, and from <a href="https://dl.acm.org/doi/10.5555/3454287.3455354">Song and Ermon (2019)</a>, of modeling directly the score function, instead of the pdf or an energy potential for the pdf.</p><h3 id="Motivation"><a class="docs-heading-anchor" href="#Motivation">Motivation</a><a id="Motivation-1"></a><a class="docs-heading-anchor-permalink" href="#Motivation" title="Permalink"></a></h3><p>The motivation is to revisit the original idea of <a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Aapo Hyvärinen (2005)</a> and see how it performs for training a neural network to model the score function.</p><h3 id="Background"><a class="docs-heading-anchor" href="#Background">Background</a><a id="Background-1"></a><a class="docs-heading-anchor-permalink" href="#Background" title="Permalink"></a></h3><p>The idea of <a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Aapo Hyvärinen (2005)</a> is to directly fit the score function from the sample data, using a suitable <strong>implicit score matching</strong> loss function not depending on the unknown score function of the random variable. This loss function is obtained by a simple integration by parts on the <strong>explicit score matching</strong> objective function given by the expected square distance between the score of the model and the score of the unknown target distribution, also known as the <em>Fisher divergence.</em> The integration by parts separates the dependence on the unknown target score function from the parameters of the model, so the fitting process (minimization over the parameters of the model) does not depend on the unknown distribution.</p><p>The implicit score matching method requires, however, the derivative of the score function of the model pdf, which is costly to compute in general. In Hyvärinen&#39;s original work, all the examples considered models for which the gradient can be computed somewhat more explicitly. There was no artificial neural network involved.</p><p>In a subsequent work, <a href="https://doi.org/10.1162/neco_a_00010">Köster and Hyvärinen (2010)</a> applied the method to fit the score function from a model probability with log-likelyhood obtained from a two-layer neural network, so that the gradient of the score function could still be expressed somehow explicitly.</p><p>After that, <a href="https://papers.nips.cc/paper_files/paper/2010/hash/6f3e29a35278d71c7f65495871231324-Abstract.html">Kingma and LeCun (2010)</a> considered a larger artificial neural network and used automatic differentiation to optimize the model. They also proposed a penalization term in the loss function, to regularize and stabilize the optimization process, yielding a <strong>regularized implicit score matching</strong> method. The model in <a href="https://papers.nips.cc/paper_files/paper/2010/hash/6f3e29a35278d71c7f65495871231324-Abstract.html">Kingma and LeCun (2010)</a> was not of the pdf directly, but of an energy potential, i.e. with</p><p class="math-container">\[    p_{\boldsymbol{\theta}}(\mathbf{x}) = \frac{1}{Z(\boldsymbol{\theta})} e^{-U(\mathbf{x}; \boldsymbol{\theta})},\]</p><p>where <span>$U(\mathbf{x}; \boldsymbol{\theta})$</span> is modeled after a neural network.</p><p>Finally, <a href="https://dl.acm.org/doi/10.5555/3454287.3455354">Song and Ermon (2019)</a> proposed modeling directly the score function as a neural network <span>$s(\mathbf{x}; \boldsymbol{\theta})$</span>, i.e.</p><p class="math-container">\[    \boldsymbol{\nabla}_{\mathbf{x}}p_{\boldsymbol{\theta}}(\mathbf{x}) = s(\mathbf{x}; \boldsymbol{\theta}).\]</p><p><a href="https://dl.acm.org/doi/10.5555/3454287.3455354">Song and Ermon (2019)</a>, however, went further and proposed a different method (based on several perturbations of the data, each of which akin to denoising score matching). At this point, we do not address the main method proposed in <a href="https://dl.acm.org/doi/10.5555/3454287.3455354">Song and Ermon (2019)</a>, we only borrow the idea of modeling directly the score function instead of the pdf or an energy potential of the pdf.</p><p>In a sense, we do an analysis in hindsight, combining ideas proposed in subsequent articles, to implement the <strong>implicit score matching</strong> method in a different way. In summary, we illustrate the use of automatic differentiation to allow the application of the <strong>implicit score matching</strong> and the <strong>regularized implicit score matching</strong> methods to directly fit the score function as modeled by a neural networks.</p><h2 id="Loss-function-for-implicit-score-matching"><a class="docs-heading-anchor" href="#Loss-function-for-implicit-score-matching">Loss function for implicit score matching</a><a id="Loss-function-for-implicit-score-matching-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-function-for-implicit-score-matching" title="Permalink"></a></h2><p>The score-matching method of <a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Aapo Hyvärinen (2005)</a> aims to minimize the <strong>empirical implicit score matching</strong> loss function <span>${\tilde J}_{\mathrm{ISM}{\tilde p}_0}$</span> given by</p><p class="math-container">\[    {\tilde J}_{\mathrm{ISM}{\tilde p}_0} = \frac{1}{N}\sum_{n=1}^N \left( \frac{1}{2}\|\boldsymbol{\psi}(\mathbf{x}_n; {\boldsymbol{\theta}})\|^2 + \boldsymbol{\nabla}_{\mathbf{x}} \cdot \boldsymbol{\psi}(\mathbf{x}_n; {\boldsymbol{\theta}}) \right),\]</p><p>where <span>$(\mathbf{x}_n)_{n=1}^N$</span> is the sample data from a unknown target distribution and where <span>$\boldsymbol{\psi}(\mathbf{x}_n; {\boldsymbol{\theta}})$</span> is a parametrized model for the desired score function.</p><p>The method rests on the idea of rewriting the <strong>explicit score matching</strong> loss function <span>$J_{\mathrm{ESM}}({\boldsymbol{\theta}})$</span> (essentially the Fisher divergence) in terms of the <strong>implicit score matching</strong> loss function <span>$J_{\mathrm{ISM}}({\boldsymbol{\theta}})$</span>, showing that </p><p class="math-container">\[J_{\mathrm{ESM}}({\boldsymbol{\theta}}) = J_{\mathrm{ISM}}({\boldsymbol{\theta}}) + C,\]</p><p>and then approximating the latter by the <strong>empirical implicit score matching</strong> loss function <span>${\tilde J}_{\mathrm{ISM}{\tilde p}_0}({\boldsymbol{\theta}})$</span>.</p><h2 id="Numerical-example"><a class="docs-heading-anchor" href="#Numerical-example">Numerical example</a><a id="Numerical-example-1"></a><a class="docs-heading-anchor-permalink" href="#Numerical-example" title="Permalink"></a></h2><p>We illustrate the method, numerically, to model a synthetic univariate Gaussian mixture distribution.</p><h3 id="Julia-language-setup"><a class="docs-heading-anchor" href="#Julia-language-setup">Julia language setup</a><a id="Julia-language-setup-1"></a><a class="docs-heading-anchor-permalink" href="#Julia-language-setup" title="Permalink"></a></h3><p>We use the <a href="https://julialang.org">Julia programming language</a> for the numerical simulations, with suitable packages.</p><h4 id="Packages"><a class="docs-heading-anchor" href="#Packages">Packages</a><a id="Packages-1"></a><a class="docs-heading-anchor-permalink" href="#Packages" title="Permalink"></a></h4><pre><code class="language-julia hljs">using StatsPlots
using Random
using Distributions
using Lux # artificial neural networks explicitly parametrized
using Optimisers
using Zygote # automatic differentiation
using Markdown</code></pre><p>There are several Julia libraries for artificial neural networks and for automatic differentiation (AD). The most established package for artificial neural networks is the <a href="https://github.com/FluxML/Flux.jl">FluxML/Flux.jl</a> library, which handles the parameters implicitly, but it is moving to explicit parameters. A newer library that handles the parameters explicitly is the <a href="https://github.com/LuxDL/Lux.jl">LuxDL/Lux.jl</a> library, which is taylored to the differential equations <a href="https://sciml.ai">SciML</a> ecosystem.</p><p>Since we aim to combine score-matching with neural networks and, eventually, with stochastic differential equations, we thought it was a reasonable idea to experiment with the <a href="https://github.com/LuxDL/Lux.jl">LuxDL/Lux.jl</a> library.</p><p>As we mentioned, the <a href="https://github.com/LuxDL/Lux.jl">LuxDL/Lux.jl</a> library is a newer package and not as well developed. In particular, it seems the only AD that works with it is the <a href="https://github.com/FluxML/Zygote.jl">FluxML/Zygote.jl</a> library. Unfortunately, the <a href="https://github.com/FluxML/Zygote.jl">FluxML/Zygote.jl</a> library is not so much fit to do AD on top of AD, as one can see from e.g. <a href="https://fluxml.ai/Zygote.jl/dev/limitations/#Second-derivatives-1">Zygote: Design limitations</a>. Thus we only illustrate this with a small network on a simple univariate problem.</p><h4 id="Reproducibility"><a class="docs-heading-anchor" href="#Reproducibility">Reproducibility</a><a id="Reproducibility-1"></a><a class="docs-heading-anchor-permalink" href="#Reproducibility" title="Permalink"></a></h4><p>We set the random seed for reproducibility purposes.</p><pre><code class="language-julia hljs">rng = Xoshiro(12345)</code></pre><h3 id="Data"><a class="docs-heading-anchor" href="#Data">Data</a><a id="Data-1"></a><a class="docs-heading-anchor-permalink" href="#Data" title="Permalink"></a></h3><p>We build the target model and draw samples from it.</p><p>The target model is a univariate random variable denoted by <span>$X$</span> and defined by a probability distribution. Associated with that we consider its PDF and its score-function.</p><pre><code class="language-julia hljs">target_prob = MixtureModel([Normal(-3, 1), Normal(3, 1)], [0.1, 0.9])

xrange = range(-10, 10, 200)
dx = Float64(xrange.step)
xx = permutedims(collect(xrange))
target_pdf = pdf.(target_prob, xrange&#39;)
target_score = gradlogpdf.(target_prob, xrange&#39;)

lambda = 0.1
sample_points = permutedims(rand(rng, target_prob, 1024))
data = (sample_points, lambda)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([2.303077959422043 2.8428423932782843 … 3.1410080972036334 2.488464630750972], 0.1)</code></pre><p>Visualizing the sample data drawn from the distribution and the PDF.</p><img src="f091663b.svg" alt="Example block output"/><p>Visualizing the score function.</p><img src="f9d1f9fd.svg" alt="Example block output"/><h3 id="The-neural-network-model"><a class="docs-heading-anchor" href="#The-neural-network-model">The neural network model</a><a id="The-neural-network-model-1"></a><a class="docs-heading-anchor-permalink" href="#The-neural-network-model" title="Permalink"></a></h3><p>The neural network we consider is a simple feed-forward neural network made of a single hidden layer, obtained as a chain of a couple of dense layers. This is implemented with the <a href="https://github.com/LuxDL/Lux.jl">LuxDL/Lux.jl</a> package.</p><p>We will see that we don&#39;t need a big neural network in this simple example. We go as low as it works.</p><pre><code class="language-julia hljs">model = Chain(Dense(1 =&gt; 8, sigmoid), Dense(8 =&gt; 1))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
    layer_1 = Dense(1 =&gt; 8, σ),         <span class="sgr90"># 16 parameters</span>
    layer_2 = Dense(8 =&gt; 1),            <span class="sgr90"># 9 parameters</span>
) <span class="sgr90">        # Total: </span>25 parameters,
<span class="sgr90">          #        plus </span>0 states.</code></pre><p>The <a href="https://github.com/LuxDL/Lux.jl">LuxDL/Lux.jl</a> package uses explicit parameters, that are initialized (or obtained) with the <code>Lux.setup</code> function, giving us the <em>parameters</em> and the <em>state</em> of the model.</p><pre><code class="language-julia hljs">ps, st = Lux.setup(rng, model) # initialize and get the parameters and states of the model</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">((layer_1 = (weight = Float32[-0.0017783825; -0.9357591; … ; 0.33351308; -0.46867305;;], bias = Float32[0.18317068, 0.5787344, -0.18110967, 0.9307035, -0.43067825, -0.46645045, -0.8246051, -0.9340805]), layer_2 = (weight = Float32[0.27326134 -0.2086962 … 0.42855448 0.5658726], bias = Float32[0.09530755])), (layer_1 = NamedTuple(), layer_2 = NamedTuple()))</code></pre><h3 id="Loss-function"><a class="docs-heading-anchor" href="#Loss-function">Loss function</a><a id="Loss-function-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-function" title="Permalink"></a></h3><p>Here it is how we implement the objective <span>${\tilde J}_{\mathrm{ISM{\tilde p}_0}}({\boldsymbol{\theta}})$</span>.</p><pre><code class="language-julia hljs">function loss_function_EISM_Zygote(model, ps, st, sample_points)
    smodel = StatefulLuxLayer{true}(model, ps, st)
    y_pred = smodel(sample_points)
    dy_pred = only(Zygote.gradient(sum ∘ smodel, sample_points))
    loss = mean(dy_pred .+ y_pred .^2 / 2)
    return loss, smodel.st, ()
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">loss_function_EISM_Zygote (generic function with 1 method)</code></pre><p>We also implement a regularized version as proposed by <a href="https://papers.nips.cc/paper_files/paper/2010/hash/6f3e29a35278d71c7f65495871231324-Abstract.html">Kingma and LeCun (2010)</a>.</p><pre><code class="language-julia hljs">function loss_function_EISM_Zygote_regularized(model, ps, st, data)
    sample_points, lambda = data
    smodel = StatefulLuxLayer{true}(model, ps, st)
    y_pred = smodel(sample_points)
    dy_pred = only(Zygote.gradient(sum ∘ smodel, sample_points))
    loss = mean(dy_pred .+ y_pred .^2 / 2 .+ lambda .* dy_pred .^2 )
    return loss, smodel.st, ()
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">loss_function_EISM_Zygote_regularized (generic function with 1 method)</code></pre><h3 id="Optimization-setup"><a class="docs-heading-anchor" href="#Optimization-setup">Optimization setup</a><a id="Optimization-setup-1"></a><a class="docs-heading-anchor-permalink" href="#Optimization-setup" title="Permalink"></a></h3><h4 id="Optimization-method"><a class="docs-heading-anchor" href="#Optimization-method">Optimization method</a><a id="Optimization-method-1"></a><a class="docs-heading-anchor-permalink" href="#Optimization-method" title="Permalink"></a></h4><p>We use the Adam optimiser.</p><pre><code class="language-julia hljs">opt = Adam(0.01)

tstate_org = Lux.Training.TrainState(model, ps, st, opt)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">TrainState
    model: Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.σ), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(1 =&gt; 8, σ), layer_2 = Dense(8 =&gt; 1)), nothing)
    # of parameters: 25
    # of states: 0
    optimizer: Optimisers.Adam(eta=0.01, beta=(0.9, 0.999), epsilon=1.0e-8)
    step: 0</code></pre><h4 id="Automatic-differentiation-in-the-optimization"><a class="docs-heading-anchor" href="#Automatic-differentiation-in-the-optimization">Automatic differentiation in the optimization</a><a id="Automatic-differentiation-in-the-optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Automatic-differentiation-in-the-optimization" title="Permalink"></a></h4><p>As mentioned, we setup differentiation in <a href="https://github.com/LuxDL/Lux.jl">LuxDL/Lux.jl</a> with the <a href="https://github.com/FluxML/Zygote.jl">FluxML/Zygote.jl</a> library.</p><pre><code class="language-julia hljs">vjp_rule = Lux.Training.AutoZygote()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">ADTypes.AutoZygote()</code></pre><h4 id="Processor"><a class="docs-heading-anchor" href="#Processor">Processor</a><a id="Processor-1"></a><a class="docs-heading-anchor-permalink" href="#Processor" title="Permalink"></a></h4><p>We use the CPU instead of the GPU.</p><pre><code class="language-julia hljs">dev_cpu = cpu_device()
## dev_gpu = gpu_device()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(::MLDataDevices.CPUDevice) (generic function with 1 method)</code></pre><h4 id="Check-differentiation"><a class="docs-heading-anchor" href="#Check-differentiation">Check differentiation</a><a id="Check-differentiation-1"></a><a class="docs-heading-anchor-permalink" href="#Check-differentiation" title="Permalink"></a></h4><p>Check if Zygote via Lux is working fine to differentiate the loss functions for training.</p><pre><code class="language-julia hljs">@time Lux.Training.compute_gradients(vjp_rule, loss_function_EISM_Zygote, sample_points, tstate_org)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌ Warning: Mixed-Precision `matmul_cpu_fallback!` detected and Octavian.jl cannot be used for this set of inputs (C [Matrix{Float64}]: A [Matrix{Float32}] x B [Matrix{Float64}]). Falling back to generic implementation. This may be slow.
└ @ LuxLib.Impl ~/.julia/packages/LuxLib/1B1qw/src/impl/matmul.jl:190
 16.401667 seconds (29.57 M allocations: 1.378 GiB, 2.19% gc time, 99.95% compilation time)</code></pre><p>It is pretty slow to run it the first time, since it envolves compiling a specialized method for it. Remember there is already a gradient on the loss function, so this amounts to a double automatic differentiation. The subsequent times are faster, but still slow for training:</p><pre><code class="language-julia hljs">@time Lux.Training.compute_gradients(vjp_rule, loss_function_EISM_Zygote, sample_points, tstate_org)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌ Warning: Mixed-Precision `matmul_cpu_fallback!` detected and Octavian.jl cannot be used for this set of inputs (C [Matrix{Float64}]: A [Matrix{Float32}] x B [Matrix{Float64}]). Falling back to generic implementation. This may be slow.
└ @ LuxLib.Impl ~/.julia/packages/LuxLib/1B1qw/src/impl/matmul.jl:190
  0.004923 seconds (1.48 k allocations: 1.388 MiB)</code></pre><p>Now the version with regularization.</p><pre><code class="language-julia hljs">@time Lux.Training.compute_gradients(vjp_rule, loss_function_EISM_Zygote_regularized, data, tstate_org)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌ Warning: Mixed-Precision `matmul_cpu_fallback!` detected and Octavian.jl cannot be used for this set of inputs (C [Matrix{Float64}]: A [Matrix{Float32}] x B [Matrix{Float64}]). Falling back to generic implementation. This may be slow.
└ @ LuxLib.Impl ~/.julia/packages/LuxLib/1B1qw/src/impl/matmul.jl:190
  1.841675 seconds (1.37 M allocations: 66.134 MiB, 99.64% compilation time)</code></pre><pre><code class="language-julia hljs">@time Lux.Training.compute_gradients(vjp_rule, loss_function_EISM_Zygote_regularized, data, tstate_org)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌ Warning: Mixed-Precision `matmul_cpu_fallback!` detected and Octavian.jl cannot be used for this set of inputs (C [Matrix{Float64}]: A [Matrix{Float32}] x B [Matrix{Float64}]). Falling back to generic implementation. This may be slow.
└ @ LuxLib.Impl ~/.julia/packages/LuxLib/1B1qw/src/impl/matmul.jl:190
  0.004840 seconds (1.51 k allocations: 1.437 MiB)</code></pre><h4 id="Training-loop"><a class="docs-heading-anchor" href="#Training-loop">Training loop</a><a id="Training-loop-1"></a><a class="docs-heading-anchor-permalink" href="#Training-loop" title="Permalink"></a></h4><p>Here is the typical main training loop suggest in the <a href="https://github.com/LuxDL/Lux.jl">LuxDL/Lux.jl</a> tutorials, but sligthly modified to save the history of losses per iteration.</p><pre><code class="language-julia hljs">function train(tstate, vjp, data, loss_function, epochs, numshowepochs=20, numsavestates=0)
    losses = zeros(epochs)
    tstates = [(0, tstate)]
    for epoch in 1:epochs
        grads, loss, stats, tstate = Lux.Training.compute_gradients(vjp,
            loss_function, data, tstate)
        if ( epochs ≥ numshowepochs &gt; 0 ) &amp;&amp; rem(epoch, div(epochs, numshowepochs)) == 0
            println(&quot;Epoch: $(epoch) || Loss: $(loss)&quot;)
        end
        if ( epochs ≥ numsavestates &gt; 0 ) &amp;&amp; rem(epoch, div(epochs, numsavestates)) == 0
            push!(tstates, (epoch, tstate))
        end
        losses[epoch] = loss
        tstate = Lux.Training.apply_gradients(tstate, grads)
    end
    return tstate, losses, tstates
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">train (generic function with 3 methods)</code></pre><h3 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h3><p>Now we train the model with the objective function <span>${\tilde J}_{\mathrm{ISM{\tilde p}_0}}({\boldsymbol{\theta}})$</span>.</p><pre><code class="language-julia hljs">@time tstate, losses, tstates = train(tstate_org, vjp_rule, sample_points, loss_function_EISM_Zygote, 500, 20, 100)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌ Warning: Mixed-Precision `matmul_cpu_fallback!` detected and Octavian.jl cannot be used for this set of inputs (C [Matrix{Float64}]: A [Matrix{Float32}] x B [Matrix{Float64}]). Falling back to generic implementation. This may be slow.
└ @ LuxLib.Impl ~/.julia/packages/LuxLib/1B1qw/src/impl/matmul.jl:190
Epoch: 25 || Loss: -0.04234274469272623
Epoch: 50 || Loss: -0.08655327756298944
Epoch: 75 || Loss: -0.1343044751296767
Epoch: 100 || Loss: -0.1822974429563777
Epoch: 125 || Loss: -0.23317269908345162
Epoch: 150 || Loss: -0.2859906581469113
Epoch: 175 || Loss: -0.33422448787548115
Epoch: 200 || Loss: -0.3719440547320392
Epoch: 225 || Loss: -0.3971120315839102
Epoch: 250 || Loss: -0.41164380228232034
Epoch: 275 || Loss: -0.41910760882592435
Epoch: 300 || Loss: -0.42270944750978817
Epoch: 325 || Loss: -0.42452865608774315
Epoch: 350 || Loss: -0.4256025051363679
Epoch: 375 || Loss: -0.42635755388429913
Epoch: 400 || Loss: -0.42698558082228516
Epoch: 425 || Loss: -0.4276392603294295
Epoch: 450 || Loss: -0.4285093672297579
Epoch: 475 || Loss: -0.4297164300927001
Epoch: 500 || Loss: -0.4310780220137994
  1.608030 seconds (2.40 M allocations: 671.292 MiB, 7.73% gc time, 63.60% compilation time)</code></pre><h3 id="Results"><a class="docs-heading-anchor" href="#Results">Results</a><a id="Results-1"></a><a class="docs-heading-anchor-permalink" href="#Results" title="Permalink"></a></h3><p>Testing out the trained model.</p><pre><code class="language-julia hljs">y_pred = Lux.apply(tstate.model, xrange&#39;, tstate.parameters, tstate.states)[1]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×200 Matrix{Float64}:
 0.297973  0.297006  0.29602  0.295016  …  -2.70585  -2.70799  -2.70999</code></pre><p>Visualizing the result.</p><pre><code class="language-julia hljs">plot(title=&quot;Fitting&quot;, titlefont=10)

plot!(xrange, target_score&#39;, linewidth=4, label=&quot;score function&quot;)

scatter!(sample_points&#39;, s -&gt; gradlogpdf(target_prob, s), label=&quot;data&quot;, markersize=2)

plot!(xx&#39;, y_pred&#39;, linewidth=2, label=&quot;predicted MLP&quot;)</code></pre><img src="42fb2519.svg" alt="Example block output"/><p>Just for the fun of it, let us see an animation of the optimization process.</p><img src="21c8c85e.gif" alt="Example block output"/><p>Recovering the PDF of the distribution from the trained score function.</p><pre><code class="language-julia hljs">paux = exp.(accumulate(+, y_pred) .* dx)
pdf_pred = paux ./ sum(paux) ./ dx
plot(title=&quot;Original PDF and PDF from predicted score function&quot;, titlefont=10)
plot!(xrange, target_pdf&#39;, label=&quot;original&quot;)
plot!(xrange, pdf_pred&#39;, label=&quot;recoverd&quot;)</code></pre><img src="56630148.svg" alt="Example block output"/><p>And the animation of the evolution of the PDF.</p><img src="29fbb99f.gif" alt="Example block output"/><p>We also visualize the evolution of the losses.</p><pre><code class="language-julia hljs">plot(losses, title=&quot;Evolution of the loss&quot;, titlefont=10, xlabel=&quot;iteration&quot;, ylabel=&quot;error&quot;, legend=false)</code></pre><img src="cd535d09.svg" alt="Example block output"/><h3 id="Training-with-the-regularization-term"><a class="docs-heading-anchor" href="#Training-with-the-regularization-term">Training with the regularization term</a><a id="Training-with-the-regularization-term-1"></a><a class="docs-heading-anchor-permalink" href="#Training-with-the-regularization-term" title="Permalink"></a></h3><p>Now we train the model with the objective function <span>${\tilde J}_{\mathrm{ISM{\tilde p}_0}}({\boldsymbol{\theta}})$</span>.</p><pre><code class="language-julia hljs">@time tstate, losses, tstates = train(tstate_org, vjp_rule, data, loss_function_EISM_Zygote_regularized, 500, 20, 100)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌ Warning: Mixed-Precision `matmul_cpu_fallback!` detected and Octavian.jl cannot be used for this set of inputs (C [Matrix{Float64}]: A [Matrix{Float32}] x B [Matrix{Float64}]). Falling back to generic implementation. This may be slow.
└ @ LuxLib.Impl ~/.julia/packages/LuxLib/1B1qw/src/impl/matmul.jl:190
Epoch: 25 || Loss: -0.04172276502849598
Epoch: 50 || Loss: -0.08420411328978389
Epoch: 75 || Loss: -0.12865008000448544
Epoch: 100 || Loss: -0.17142795361066704
Epoch: 125 || Loss: -0.21490748944281216
Epoch: 150 || Loss: -0.2575269557327344
Epoch: 175 || Loss: -0.2940412402890374
Epoch: 200 || Loss: -0.32071721916715445
Epoch: 225 || Loss: -0.33736093613057677
Epoch: 250 || Loss: -0.3464020903520449
Epoch: 275 || Loss: -0.3508801473267845
Epoch: 300 || Loss: -0.3531123917051732
Epoch: 325 || Loss: -0.3544217533225459
Epoch: 350 || Loss: -0.3554098746522306
Epoch: 375 || Loss: -0.35630157707193555
Epoch: 400 || Loss: -0.35717279047013045
Epoch: 425 || Loss: -0.3580561931510393
Epoch: 450 || Loss: -0.3589723573430019
Epoch: 475 || Loss: -0.3599356306741516
Epoch: 500 || Loss: -0.3609567221719558
  0.609387 seconds (436.68 k allocations: 592.848 MiB, 11.83% gc time, 8.14% compilation time)</code></pre><h3 id="Results-2"><a class="docs-heading-anchor" href="#Results-2">Results</a><a class="docs-heading-anchor-permalink" href="#Results-2" title="Permalink"></a></h3><p>Testing out the trained model.</p><pre><code class="language-julia hljs">y_pred = Lux.apply(tstate.model, xrange&#39;, tstate.parameters, tstate.states)[1]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×200 Matrix{Float64}:
 0.508823  0.507413  0.505959  0.504458  …  -2.23494  -2.2364  -2.23776</code></pre><p>Visualizing the result.</p><pre><code class="language-julia hljs">plot(title=&quot;Fitting&quot;, titlefont=10)

plot!(xrange, target_score&#39;, linewidth=4, label=&quot;score function&quot;)

scatter!(sample_points&#39;, s -&gt; gradlogpdf(target_prob, s), label=&quot;data&quot;, markersize=2)

plot!(xx&#39;, y_pred&#39;, linewidth=2, label=&quot;predicted MLP&quot;)</code></pre><img src="c56524c7.svg" alt="Example block output"/><p>Just for the fun of it, let us see an animation of the optimization process.</p><img src="951a04b3.gif" alt="Example block output"/><p>Recovering the PDF of the distribution from the trained score function.</p><pre><code class="language-julia hljs">paux = exp.(accumulate(+, y_pred) .* dx)
pdf_pred = paux ./ sum(paux) ./ dx
plot(title=&quot;Original PDF and PDF from predicted score function&quot;, titlefont=10)
plot!(xrange, target_pdf&#39;, label=&quot;original&quot;)
plot!(xrange, pdf_pred&#39;, label=&quot;recoverd&quot;)</code></pre><img src="086dc601.svg" alt="Example block output"/><p>And the animation of the evolution of the PDF.</p><img src="93a9dccc.gif" alt="Example block output"/><p>We also visualize the evolution of the losses.</p><pre><code class="language-julia hljs">plot(losses, title=&quot;Evolution of the loss&quot;, titlefont=10, xlabel=&quot;iteration&quot;, ylabel=&quot;error&quot;, legend=false)</code></pre><img src="8c6b8ecc.svg" alt="Example block output"/><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><ol><li><a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Aapo Hyvärinen (2005), &quot;Estimation of non-normalized statistical models by score matching&quot;, Journal of Machine Learning Research 6, 695-709</a></li><li><a href="https://doi.org/10.1162/neco_a_00010">U. Köster, A. Hyvärinen (2010), &quot;A two-layer model of natural stimuli estimated with score matching&quot;, Neural. Comput. 22 (no. 9), 2308-33, doi: 10.1162/NECO<em>a</em>00010</a></li><li><a href="https://papers.nips.cc/paper_files/paper/2010/hash/6f3e29a35278d71c7f65495871231324-Abstract.html">Durk P. Kingma, Yann Cun (2010), &quot;Regularized estimation of image statistics by Score Matching&quot;, Advances in Neural Information Processing Systems 23 (NIPS 2010)</a></li><li><a href="https://dl.acm.org/doi/10.5555/3454287.3455354">Y. Song and S. Ermon (2019), &quot;Generative modeling by estimating gradients of the data distribution&quot;, NIPS&#39;19: Proceedings of the 33rd International Conference on Neural Information Processing Systems, no. 1067, 11918-11930</a></li></ol></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../score_matching_aapo/">« Score matching of Aapo Hyvärinen</a><a class="docs-footer-nextpage" href="../parzen_estimation_score_matching/">Score matching with Parzen estimation »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.13.0 on <span class="colophon-date" title="Thursday 26 June 2025 15:36">Thursday 26 June 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
