<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>2D finite-difference score matching · Random notes</title><meta name="title" content="2D finite-difference score matching · Random notes"/><meta property="og:title" content="2D finite-difference score matching · Random notes"/><meta property="twitter:title" content="2D finite-difference score matching · Random notes"/><meta name="description" content="Documentation for Random notes."/><meta property="og:description" content="Documentation for Random notes."/><meta property="twitter:description" content="Documentation for Random notes."/><meta property="og:url" content="https://github.com/rmsrosa/random_notes/generative/2d_FD_score_matching/"/><meta property="twitter:url" content="https://github.com/rmsrosa/random_notes/generative/2d_FD_score_matching/"/><link rel="canonical" href="https://github.com/rmsrosa/random_notes/generative/2d_FD_score_matching/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/style.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="Random notes logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Random notes</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Random Notes</a></li><li><span class="tocitem">Probability Essentials</span><ul><li><a class="tocitem" href="../../probability/kernel_density_estimation/">Kernel Density Estimation</a></li><li><a class="tocitem" href="../../probability/convergence_notions/">Convergence notions</a></li></ul></li><li><span class="tocitem">Discrete-time Markov chains</span><ul><li><a class="tocitem" href="../../markov_chains/mc_definitions/">Essential definitions</a></li><li><a class="tocitem" href="../../markov_chains/mc_invariance/">Invariant distributions</a></li><li><input class="collapse-toggle" id="menuitem-3-3" type="checkbox"/><label class="tocitem" for="menuitem-3-3"><span class="docs-label">Countable-space Markov chains</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../markov_chains/mc_countableX_recurrence/">Recurrence in the countable-space case</a></li><li><a class="tocitem" href="../../markov_chains/mc_countableX_connections/">Connected states, irreducibility and uniqueness of invariant measures</a></li><li><a class="tocitem" href="../../markov_chains/mc_countableX_convergencia/">Aperiodicidade e convergência para a distribuição estacionária</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-4" type="checkbox"/><label class="tocitem" for="menuitem-3-4"><span class="docs-label">Continuous-space Markov chains</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../markov_chains/mc_irreducibility_and_recurrence/">Irreducibility and recurrence in the continuous-space case</a></li></ul></li></ul></li><li><span class="tocitem">Sampling methods</span><ul><li><a class="tocitem" href="../../sampling/overview/">Overview</a></li><li><a class="tocitem" href="../../sampling/prng/">Random number generators</a></li><li><input class="collapse-toggle" id="menuitem-4-3" type="checkbox"/><label class="tocitem" for="menuitem-4-3"><span class="docs-label">Transform methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/invFtransform/">Probability integral transform</a></li><li><a class="tocitem" href="../../sampling/box_muller/">Box-Muller transform</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-4" type="checkbox"/><label class="tocitem" for="menuitem-4-4"><span class="docs-label">Accept-Reject methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/rejection_sampling/">Rejection sampling</a></li><li><a class="tocitem" href="../../sampling/empiricalsup_rejection/">Empirical supremum rejection sampling</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-5" type="checkbox"/><label class="tocitem" for="menuitem-4-5"><span class="docs-label">Markov Chain Monte Carlo (MCMC)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/mcmc/">Overview</a></li><li><a class="tocitem" href="../../sampling/metropolis/">Metropolis and Metropolis-Hastings</a></li><li><a class="tocitem" href="../../sampling/convergence_metropolis/">Convergence of Metropolis-Hastings</a></li><li><a class="tocitem" href="../../sampling/gibbs/">Gibbs sampling</a></li><li><a class="tocitem" href="../../sampling/hmc/">Hamiltonian Monte Carlo (HMC)</a></li></ul></li><li><a class="tocitem" href="../../sampling/langevin_sampling/">Langevin sampling</a></li></ul></li><li><span class="tocitem">Bayesian inference</span><ul><li><input class="collapse-toggle" id="menuitem-5-1" type="checkbox"/><label class="tocitem" for="menuitem-5-1"><span class="docs-label">Bayes Theory</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/bayes/">Bayes Theorem</a></li><li><a class="tocitem" href="../../bayesian/bayes_inference/">Bayesian inference</a></li><li><a class="tocitem" href="../../bayesian/bernstein_vonmises/">Bernstein–von Mises theorem</a></li></ul></li><li><a class="tocitem" href="../../bayesian/bayesian_probprog/">Bayesian probabilistic programming</a></li><li><input class="collapse-toggle" id="menuitem-5-3" type="checkbox"/><label class="tocitem" for="menuitem-5-3"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/find_pi/">Estimating π via frequentist and Bayesian methods</a></li><li><a class="tocitem" href="../../bayesian/linear_regression/">Many Ways to Linear Regression</a></li><li><a class="tocitem" href="../../bayesian/tilapia_alometry/">Alometry law for the Nile Tilapia</a></li><li><a class="tocitem" href="../../bayesian/mortality_tables/">Modeling mortality tables</a></li></ul></li></ul></li><li><span class="tocitem">Generative models</span><ul><li><input class="collapse-toggle" id="menuitem-6-1" type="checkbox" checked/><label class="tocitem" for="menuitem-6-1"><span class="docs-label">Score matching</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../overview/">Overview</a></li><li><a class="tocitem" href="../stein_score/">Stein score function</a></li><li><a class="tocitem" href="../score_matching_aapo/">Score matching of Aapo Hyvärinen</a></li><li><a class="tocitem" href="../score_matching_neural_network/">Score matching a neural network</a></li><li><a class="tocitem" href="../parzen_estimation_score_matching/">Score matching with Parzen estimation</a></li><li><a class="tocitem" href="../denoising_score_matching/">Denoising score matching of Pascal Vincent</a></li><li><a class="tocitem" href="../sliced_score_matching/">Sliced score matching</a></li><li><a class="tocitem" href="../1d_FD_score_matching/">1D finite-difference score matching</a></li><li class="is-active"><a class="tocitem" href>2D finite-difference score matching</a><ul class="internal"><li><a class="tocitem" href="#Introduction"><span>Introduction</span></a></li><li><a class="tocitem" href="#Julia-language-setup"><span>Julia language setup</span></a></li><li><a class="tocitem" href="#Data"><span>Data</span></a></li><li><a class="tocitem" href="#The-neural-network-model"><span>The neural network model</span></a></li><li><a class="tocitem" href="#Loss-functions-for-score-matching"><span>Loss functions for score-matching</span></a></li><li><a class="tocitem" href="#Optimization-setup"><span>Optimization setup</span></a></li><li><a class="tocitem" href="#Cheat-training-with-{\\tilde-J}_{\\mathrm{ESM}{\\tilde-p}_0}"><span>Cheat training with <span>${\tilde J}_{\mathrm{ESM}{\tilde p}_0}$</span></span></a></li><li><a class="tocitem" href="#Real-training-with-{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}"><span>Real training with <span>${\tilde J}_{\mathrm{FDSM}{\tilde p}_0}$</span></span></a></li></ul></li><li><a class="tocitem" href="../ddpm/">Denoising diffusion probabilistic models</a></li><li><a class="tocitem" href="../mdsm/">Multiple denoising score matching</a></li><li><a class="tocitem" href="../probability_flow/">Probability flow</a></li><li><a class="tocitem" href="../reverse_flow/">Reverse probability flow</a></li><li><a class="tocitem" href="../score_based_sde/">Score-based SDE model</a></li></ul></li></ul></li><li><span class="tocitem">Sensitivity analysis</span><ul><li><a class="tocitem" href="../../sensitivity/overview/">Overview</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Generative models</a></li><li><a class="is-disabled">Score matching</a></li><li class="is-active"><a href>2D finite-difference score matching</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>2D finite-difference score matching</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/rmsrosa/random_notes" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/rmsrosa/random_notes/blob/main/docs/src/generative/2d_FD_score_matching.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Finite-difference-score-matching-of-a-two-dimensional-Gaussian-mixture-model"><a class="docs-heading-anchor" href="#Finite-difference-score-matching-of-a-two-dimensional-Gaussian-mixture-model">Finite-difference score-matching of a two-dimensional Gaussian mixture model</a><a id="Finite-difference-score-matching-of-a-two-dimensional-Gaussian-mixture-model-1"></a><a class="docs-heading-anchor-permalink" href="#Finite-difference-score-matching-of-a-two-dimensional-Gaussian-mixture-model" title="Permalink"></a></h1><h2 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h2><p>Here, we modify the previous finite-difference score-matching example to fit a two-dimensional model.</p><h2 id="Julia-language-setup"><a class="docs-heading-anchor" href="#Julia-language-setup">Julia language setup</a><a id="Julia-language-setup-1"></a><a class="docs-heading-anchor-permalink" href="#Julia-language-setup" title="Permalink"></a></h2><p>We use the <a href="https://julialang.org">Julia programming language</a> with suitable packages.</p><pre><code class="language-julia hljs">using StatsPlots
using Random
using Distributions
using Lux # artificial neural networks explicitly parametrized
using Optimisers
using Zygote # automatic differentiation</code></pre><p>We set the random seed for reproducibility purposes.</p><pre><code class="language-julia hljs">rng = Xoshiro(12345)</code></pre><h2 id="Data"><a class="docs-heading-anchor" href="#Data">Data</a><a id="Data-1"></a><a class="docs-heading-anchor-permalink" href="#Data" title="Permalink"></a></h2><p>We build the target model and draw samples from it. This time the target model is a bivariate random variable.</p><pre><code class="language-julia hljs">xrange = range(-8, 8, 120)
yrange = range(-8, 8, 120)
dx = Float64(xrange.step)
dy = Float64(yrange.step)

target_prob = MixtureModel([MvNormal([-3, -3], [1 0; 0 1]), MvNormal([3, 3], [1 0; 0 1]), MvNormal([-1, 1], [1 0; 0 1])], [0.4, 0.4, 0.2])

target_pdf = [pdf(target_prob, [x, y]) for y in yrange, x in xrange]
target_score = reduce(hcat, gradlogpdf(target_prob, [x, y]) for y in yrange, x in xrange)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×14400 Matrix{Float64}:
 5.0  5.0      5.0      5.0      …  -5.0      -5.0      -5.0      -5.0
 5.0  4.86555  4.73109  4.59664     -4.59664  -4.73109  -4.86555  -5.0</code></pre><pre><code class="language-julia hljs">sample_points = rand(rng, target_prob, 1024)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×1024 Matrix{Float64}:
 -3.69692  2.55677  -0.317683  -1.16593   …  2.92216  -3.03365   5.16729
 -3.64622  3.4103    1.34462   -0.286077     2.96657   0.669646  3.21557</code></pre><pre><code class="language-julia hljs">surface(xrange, yrange, target_pdf, title=&quot;PDF&quot;, titlefont=10, legend=false, color=:vik)
scatter!(sample_points[1, :], sample_points[2, :], [pdf(target_prob, [x, y]) for (x, y) in eachcol(sample_points)], markercolor=:lightgreen, markersize=2, alpha=0.5)</code></pre><img src="51e234eb.svg" alt="Example block output"/><pre><code class="language-julia hljs">heatmap(xrange, yrange, target_pdf, title=&quot;PDF&quot;, titlefont=10, legend=false, color=:vik)
scatter!(sample_points[1, :], sample_points[2, :], markersize=2, markercolor=:lightgreen, alpha=0.5)</code></pre><img src="35cae554.svg" alt="Example block output"/><pre><code class="language-julia hljs">surface(xrange, yrange, (x, y) -&gt; logpdf(target_prob, [x, y]), title=&quot;Logpdf&quot;, titlefont=10, legend=false, color=:vik)
scatter!(sample_points[1, :], sample_points[2, :], [logpdf(target_prob, [x, y]) for (x, y) in eachcol(sample_points)], markercolor=:lightgreen, alpha=0.5, markersize=2)</code></pre><img src="f55241c7.svg" alt="Example block output"/><pre><code class="language-julia hljs">meshgrid(x, y) = (repeat(x, outer=length(y)), repeat(y, inner=length(x)))
xx, yy = meshgrid(xrange[begin:8:end], yrange[begin:8:end])
uu = reduce(hcat, gradlogpdf(target_prob, [x, y]) for (x, y) in zip(xx, yy))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×225 Matrix{Float64}:
 5.0  3.92437  2.84874  1.77311  0.697479  …  -1.90756  -2.98319  -4.05882
 5.0  5.0      5.0      5.0      5.0          -4.05882  -4.05882  -4.05882</code></pre><pre><code class="language-julia hljs">heatmap(xrange, yrange, (x, y) -&gt; logpdf(target_prob, [x, y]), title=&quot;Logpdf (heatmap) and score function (vector field)&quot;, titlefont=10, legend=false, color=:vik)
quiver!(xx, yy, quiver = (uu[1, :] ./ 8, uu[2, :] ./ 8), color=:yellow, alpha=0.5)
scatter!(sample_points[1, :], sample_points[2, :], markersize=2, markercolor=:lightgreen, alpha=0.5)</code></pre><img src="9525fde8.svg" alt="Example block output"/><h2 id="The-neural-network-model"><a class="docs-heading-anchor" href="#The-neural-network-model">The neural network model</a><a id="The-neural-network-model-1"></a><a class="docs-heading-anchor-permalink" href="#The-neural-network-model" title="Permalink"></a></h2><p>The neural network we consider is again a simple feed-forward neural network made of a single hidden layer. For the 2d case, we need to bump it a little bit, doubling the width of the hidden layer.</p><pre><code class="language-julia hljs">model = Chain(Dense(2 =&gt; 16, relu), Dense(16 =&gt; 2))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
    layer_1 = Dense(2 =&gt; 16, relu),               <span class="sgr90"># 48 parameters</span>
    layer_2 = Dense(16 =&gt; 2),                     <span class="sgr90"># 34 parameters</span>
) <span class="sgr90">        # Total: </span>82 parameters,
<span class="sgr90">          #        plus </span>0 states.</code></pre><p>The <a href="https://github.com/LuxDL/Lux.jl">LuxDL/Lux.jl</a> package uses explicit parameters, that are initialized (or obtained) with the <code>Lux.setup</code> function, giving us the <em>parameters</em> and the <em>state</em> of the model.</p><pre><code class="language-julia hljs">ps, st = Lux.setup(rng, model) # initialize and get the parameters and states of the model</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">((layer_1 = (weight = Float32[-1.2633736 1.8181845; 2.3867178 0.5713208; … ; -0.7349689 -1.3411301; -1.167693 -1.8823313], bias = Float32[0.48686042, 0.32835403, 0.23500215, -0.6289243, -0.4877348, -0.08177762, 0.57667726, -0.06088416, 0.20568033, -0.66487825, 0.17736456, -0.20175992, 0.4999213, -0.5164128, 0.5682744, 0.33539677]), layer_2 = (weight = Float32[0.3586208 -0.057023764 … -0.17773739 -0.41380876; 0.12927775 -0.22223856 … 0.06703623 0.18231718], bias = Float32[0.067408144, 0.110440105])), (layer_1 = NamedTuple(), layer_2 = NamedTuple()))</code></pre><h2 id="Loss-functions-for-score-matching"><a class="docs-heading-anchor" href="#Loss-functions-for-score-matching">Loss functions for score-matching</a><a id="Loss-functions-for-score-matching-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-functions-for-score-matching" title="Permalink"></a></h2><p>The loss function is again based on <a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Aapo Hyvärinen (2005)</a>, combined with the work of <a href="https://openreview.net/forum?id=LVRoKppWczk">Pang, Xu, Li, Song, Ermon, and Zhu (2020)</a> using finite differences to approximate the divergence of the modeled score function.</p><p>In the multidimensional case, say on <span>$\mathbb{R}^d$</span>, <span>$d\in\mathbb{N}$</span>, the <strong>explicit score matching</strong> loss function is given by</p><p class="math-container">\[    J_{\mathrm{ESM}}({\boldsymbol{\theta}}) = \frac{1}{2}\int_{\mathbb{R}^d} p_{\mathbf{X}}(\mathbf{x}) \|\boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}}) - \boldsymbol{\psi}_{\mathbf{X}}(\mathbf{x})\|^2\;\mathrm{d}\mathbf{x};\]</p><p>where <span>$p_{\mathbf{X}}(\mathbf{x})$</span> is the PDF of the target distribution.</p><p>The integration by parts in the expectation yields <span>$J_{\mathrm{ESM}}({\boldsymbol{\theta}}) = J_{\mathrm{ISM}}({\boldsymbol{\theta}}) + C$</span>, where <span>$C$</span> is constant with respect to the parameters and the <strong>implicit score matching</strong> loss function <span>$J_{\mathrm{ISM}}({\boldsymbol{\theta}})$</span> is given by</p><p class="math-container">\[    J_{\mathrm{ISM}}({\boldsymbol{\theta}}) = \int_{\mathbb{R}} p_{\mathbf{X}}(\mathbf{x}) \left( \frac{1}{2}\|\boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}})\|^2 + \boldsymbol{\nabla}_{\mathbf{x}} \cdot \boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}}) \right)\;\mathrm{d}\mathbf{x},\]</p><p>which does not involve the unknown score function of <span>${\mathbf{X}}$</span>. It does, however, involve the divergence of the modeled score function, which is expensive to compute.</p><p>In practice, the loss function is estimated via the empirical distribution, so the unknown <span>$p_{\mathbf{X}}(\mathbf{x})$</span> is handled implicitly by the sample data <span>$(\mathbf{x}_n)_n$</span>, and we minimize the <strong>empirical implicit score matching</strong> loss function</p><p class="math-container">\[    {\tilde J}_{\mathrm{ISM}{\tilde p}_0} =  \frac{1}{N}\sum_{n=1}^N \left( \frac{1}{2}\|\boldsymbol{\psi}(\mathbf{x}_n; {\boldsymbol{\theta}})\|^2 + \boldsymbol{\nabla}_{\mathbf{x}} \cdot \boldsymbol{\psi}(\mathbf{x}_n; {\boldsymbol{\theta}}) \right).\]</p><p>Componentwise, with <span>$\boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}}) = (\psi_i(\mathbf{x}; {\boldsymbol{\theta}}))_{i=1}^d$</span>, this is written as</p><p class="math-container">\[    {\tilde J}_{\mathrm{ISM}{\tilde p}_0} = \frac{1}{N}\sum_{n=1}^N \sum_{i=1}^d \left( \frac{1}{2}\psi_i(\mathbf{x}_n; {\boldsymbol{\theta}})^2 + \frac{\partial}{\partial x_i} \psi_i(\mathbf{x}_n; {\boldsymbol{\theta}}) \right).\]</p><p>As mentioned before, computing a derivative to form the loss function becomes expensive when combined with the usual optimization methods to fit a neural network, as they require the gradient of the loss function itself, so we approximate the derivative of the modeled score function by centered finite differences. With the model calculated at the displaced points, we just average them to avoid computing the model at the sample point itself. This leads to the <strong>empirical finite-difference (implicit) score matching</strong> loss function</p><p class="math-container">\[    {\tilde J}_{\mathrm{FDSM}{\tilde p}_0} = \frac{1}{N}\sum_{n=1}^N \sum_{i=1}^d \Bigg( \frac{1}{2}\left(\frac{1}{d}\sum_{j=1}^d \frac{\psi_i(\mathbf{x}_n + \delta\mathbf{e}_j; {\boldsymbol{\theta}}) + \psi_i(\mathbf{x}_n - \delta\mathbf{e}_j; {\boldsymbol{\theta}})}{2}\right)^2 \\ \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad + \frac{\psi_i(\mathbf{x}_n + \delta\mathbf{e}_i; {\boldsymbol{\theta}}) - \psi_i(\mathbf{x}_n - \delta\mathbf{e}_i; {\boldsymbol{\theta}})}{2\delta} \Bigg).\]</p><p>Since this is a synthetic problem and we actually know the target distribution, we implement the <strong>empirical explicit score matching</strong> loss function</p><p class="math-container">\[    {\tilde J}_{\mathrm{ESM}{\tilde p}_0}({\boldsymbol{\theta}}) = \frac{1}{2}\frac{1}{N}\sum_{n=1}^N \|\boldsymbol{\psi}(\mathbf{x}_n; {\boldsymbol{\theta}}) - \boldsymbol{\psi}_{\mathbf{X}}(\mathbf{x}_n)\|^2.\]</p><p>This is used as a sure check whether the neural network is sufficient to model the score function and for checking the optimization process, since in theory this should be roughly (apart from the approximations by the empirical distribution, the finite-difference approximation, and the round-off errors) a constant different from the loss function for <span>${\tilde J}_{\mathrm{FDSM}{\tilde p}_0}$</span>.</p><h3 id="Implementation-of-{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}({\\boldsymbol{\\theta}})"><a class="docs-heading-anchor" href="#Implementation-of-{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}({\\boldsymbol{\\theta}})">Implementation of <span>${\tilde J}_{\mathrm{FDSM}{\tilde p}_0}({\boldsymbol{\theta}})$</span></a><a id="Implementation-of-{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}({\\boldsymbol{\\theta}})-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation-of-{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}({\\boldsymbol{\\theta}})" title="Permalink"></a></h3><p>In the two-dimensional case, <span>$d = 2$</span>, this becomes</p><p class="math-container">\[    \begin{align*}
        {\tilde J}_{\mathrm{FDSM}{\tilde p}_0} &amp; = \frac{1}{N}\sum_{n=1}^N \sum_{i=1}^d \Bigg( \frac{1}{2}\left(\frac{1}{d}\sum_{j=1}^d \frac{\psi_i(\mathbf{x}_n + \delta\mathbf{e}_j; {\boldsymbol{\theta}}) + \psi_i(\mathbf{x}_n - \delta\mathbf{e}_j; {\boldsymbol{\theta}})}{2}\right)^2 \\
        &amp; \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad + \frac{\psi_i(\mathbf{x}_n + \delta\mathbf{e}_i; {\boldsymbol{\theta}}) - \psi_i(\mathbf{x}_n - \delta\mathbf{e}_i; {\boldsymbol{\theta}})}{2\delta} \Bigg) \\
        &amp; = \frac{1}{N}\sum_{n=1}^N \sum_{i=1}^2 \Bigg( \frac{1}{2}\left(\sum_{j=1}^2 \frac{\psi_i(\mathbf{x}_n + \delta\mathbf{e}_j; {\boldsymbol{\theta}}) + \psi_i(\mathbf{x}_n - \delta\mathbf{e}_j; {\boldsymbol{\theta}})}{4}\right)^2 \\
        &amp; \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad + \frac{\psi_i(\mathbf{x}_n + \delta\mathbf{e}_i; {\boldsymbol{\theta}}) - \psi_i(\mathbf{x}_n - \delta\mathbf{e}_i; {\boldsymbol{\theta}})}{2\delta} \Bigg) \\
        &amp; = \frac{1}{N} \frac{1}{2} \sum_{n=1}^N \sum_{i=1}^2 \left(\sum_{j=1}^2 \frac{\psi_i(\mathbf{x}_n + \delta\mathbf{e}_j; {\boldsymbol{\theta}}) + \psi_i(\mathbf{x}_n - \delta\mathbf{e}_j; {\boldsymbol{\theta}})}{4}\right)^2 \\
        &amp; \qquad \qquad \qquad \qquad \qquad \qquad + \frac{1}{N}\sum_{n=1}^N \frac{\psi_1(\mathbf{x}_n + \delta\mathbf{e}_1; {\boldsymbol{\theta}}) - \psi_1(\mathbf{x}_n - \delta\mathbf{e}_1; {\boldsymbol{\theta}})}{2\delta} \\
        &amp; \qquad \qquad \qquad \qquad \qquad \qquad + \frac{1}{N}\sum_{n=1}^N \frac{\psi_2(\mathbf{x}_n + \delta\mathbf{e}_2; {\boldsymbol{\theta}}) - \psi_2(\mathbf{x}_n - \delta\mathbf{e}_2; {\boldsymbol{\theta}})}{2\delta}
    \end{align*}\]</p><pre><code class="language-julia hljs">function loss_function(model, ps, st, data)
    sample_points, deltax, deltay = data
    s_pred_fwd_x, = Lux.apply(model, sample_points .+ [deltax, 0.0], ps, st)
    s_pred_bwd_x, = Lux.apply(model, sample_points .- [deltax, 0.0], ps, st)
    s_pred_fwd_y, = Lux.apply(model, sample_points .+ [0.0, deltay], ps, st)
    s_pred_bwd_y, = Lux.apply(model, sample_points .- [0.0, deltay], ps, st)
    s_pred = ( s_pred_bwd_x .+ s_pred_fwd_x .+ s_pred_bwd_y .+ s_pred_fwd_y) ./ 4
    dsdx_pred = (s_pred_fwd_x .- s_pred_bwd_x ) ./ 2deltax
    dsdy_pred = (s_pred_fwd_y .- s_pred_bwd_y ) ./ 2deltay
    loss = mean(abs2, s_pred) + mean(view(dsdx_pred, 1, :)) +  mean(view(dsdy_pred, 2, :))
    return loss, st, ()
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">loss_function (generic function with 1 method)</code></pre><p>We included the steps for the finite difference computations in the <code>data</code> passed to training to avoid repeated computations.</p><pre><code class="language-julia hljs">xmin, xmax = extrema(sample_points[1, :])
ymin, ymax = extrema(sample_points[2, :])
deltax, deltay = (xmax - xmin) / 2size(sample_points, 2), (ymax - ymin) / 2size(sample_points, 2)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(0.00581614558657555, 0.006593323926327331)</code></pre><pre><code class="language-julia hljs">data = sample_points, deltax, deltay</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([-3.696922040577957 2.556772674293886 … -3.033651509981303 5.1672947152544895; -3.6462237060140024 3.4102960875997117 … 0.6696459295015311 3.2155718822745527], 0.00581614558657555, 0.006593323926327331)</code></pre><h3 id="Implementation-of-{\\tilde-J}_{\\mathrm{ESM}{\\tilde-p}_0}({\\boldsymbol{\\theta}})"><a class="docs-heading-anchor" href="#Implementation-of-{\\tilde-J}_{\\mathrm{ESM}{\\tilde-p}_0}({\\boldsymbol{\\theta}})">Implementation of <span>${\tilde J}_{\mathrm{ESM}{\tilde p}_0}({\boldsymbol{\theta}})$</span></a><a id="Implementation-of-{\\tilde-J}_{\\mathrm{ESM}{\\tilde-p}_0}({\\boldsymbol{\\theta}})-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation-of-{\\tilde-J}_{\\mathrm{ESM}{\\tilde-p}_0}({\\boldsymbol{\\theta}})" title="Permalink"></a></h3><p>As a sanity check, we also include the empirical explicit score matching loss function, which uses the know score functions of the target model.</p><p>In the two-dimensional case, this is simply the mean square value of all the components.</p><p class="math-container">\[    {\tilde J}_{\mathrm{ESM}{\tilde p}_0}({\boldsymbol{\theta}}) = \frac{1}{2} \frac{1}{N}\sum_{n=1}^N \|\boldsymbol{\psi}(\mathbf{x}_n; {\boldsymbol{\theta}}) - \boldsymbol{\psi}_{\mathbf{X}}(\mathbf{x}_n)\|^2 = \frac{1}{2} \frac{1}{N}\sum_{n=1}^N \sum_{i=1}^2 \left(\psi_i(\mathbf{x}_n; {\boldsymbol{\theta}}) - \psi_{\mathbf{X}, i}(\mathbf{x}_n) \right)^2.\]</p><pre><code class="language-julia hljs">function loss_function_cheat(model, ps, st, data)
    sample_points, score_cheat = data
    score_pred, st = Lux.apply(model, sample_points, ps, st)
    loss = mean(abs2, score_pred .- score_cheat)
    return loss, st, ()
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">loss_function_cheat (generic function with 1 method)</code></pre><p>The data in this case includes information about the target distribution.</p><pre><code class="language-julia hljs">score_cheat = reduce(hcat, gradlogpdf(target_prob, u) for u in eachcol(sample_points))
data_cheat = sample_points, score_cheat</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([-3.696922040577957 2.556772674293886 … -3.033651509981303 5.1672947152544895; -3.6462237060140024 3.4102960875997117 … 0.6696459295015311 3.2155718822745527], [0.6969228899748048 0.44299201282532824 … 1.9946701191708671 -2.1672947253904584; 0.6462254048076983 -0.41041374404010544 … 0.2523912079807282 -0.2155718873425372])</code></pre><h3 id="Computing-the-constant"><a class="docs-heading-anchor" href="#Computing-the-constant">Computing the constant</a><a id="Computing-the-constant-1"></a><a class="docs-heading-anchor-permalink" href="#Computing-the-constant" title="Permalink"></a></h3><p>The expression <span>${\tilde J}_{\mathrm{ESM}{\tilde p}_0}({\boldsymbol{\theta}}) \approx {\tilde J}_{\mathrm{ISM}{\tilde p}_0}({\boldsymbol{\theta}}) + C$</span> can be used to test the implementation of the different loss functions. For that, we need to compute the constant <span>$C$</span>. This can be computed with a fine mesh or with a Monte-Carlo approximation. We do both just for fun.</p><pre><code class="language-julia hljs">function compute_constante(target_prob, xrange, yrange)
    dx = Float64(xrange.step)
    dy = Float64(yrange.step)
    Jconstant = sum(pdf(target_prob, [x, y]) * sum(abs2, gradlogpdf(target_prob, [x, y])) for y in yrange, x in xrange) * dx * dy / 2
    return Jconstant
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">compute_constante (generic function with 1 method)</code></pre><pre><code class="language-julia hljs">function compute_constante_MC(target_prob, sample_points)
    Jconstant = mean(sum(abs2, gradlogpdf(target_prob, s)) for s in eachcol(sample_points)) / 2
    return Jconstant
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">compute_constante_MC (generic function with 1 method)</code></pre><pre><code class="language-julia hljs">Jconstant = compute_constante(target_prob, xrange, yrange)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.8921462840364986</code></pre><pre><code class="language-julia hljs">Jconstant_MC = compute_constante_MC(target_prob, sample_points)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.9129656846610863</code></pre><pre><code class="language-julia hljs">constants = [(n, compute_constante_MC(target_prob, rand(rng, target_prob, n))) for _ in 1:100 for n in (1, 10, 20, 50, 100, 500, 1000, 2000, 4000)]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">900-element Vector{Tuple{Int64, Float64}}:
 (1, 1.1538119449520563)
 (10, 1.2247351227578578)
 (20, 1.1028524320443522)
 (50, 0.9920784868026302)
 (100, 1.0063955226226398)
 (500, 0.8755541733725885)
 (1000, 0.8342517573145984)
 (2000, 0.8759484336636649)
 (4000, 0.8827405034687669)
 (1, 0.9485837438780702)
 ⋮
 (1, 0.298790143447742)
 (10, 0.7794066301161721)
 (20, 1.1364257738853651)
 (50, 0.6101320519101868)
 (100, 0.8696468922755639)
 (500, 0.9416724546394305)
 (1000, 0.9063918160454828)
 (2000, 0.8914872767709621)
 (4000, 0.871944900929463)</code></pre><pre><code class="language-julia hljs">scatter(constants, markersize=2, title=&quot;constant computed by MC and fine mesh&quot;, titlefont=10, xlabel=&quot;sample size&quot;, ylabel=&quot;value&quot;, label=&quot;via various samples&quot;)
hline!([Jconstant], label=&quot;via fine mesh&quot;)
hline!([Jconstant_MC], label=&quot;via working sample&quot;, linestyle=:dash)</code></pre><img src="1e8023b1.svg" alt="Example block output"/><h3 id="A-test-for-the-implementations-of-the-loss-functions"><a class="docs-heading-anchor" href="#A-test-for-the-implementations-of-the-loss-functions">A test for the implementations of the loss functions</a><a id="A-test-for-the-implementations-of-the-loss-functions-1"></a><a class="docs-heading-anchor-permalink" href="#A-test-for-the-implementations-of-the-loss-functions" title="Permalink"></a></h3><p>Notice that, for a sufficiently large sample and sufficiently small discretization step <span>$\delta$</span>, we should have</p><p class="math-container">\[    {\tilde J}_{\mathrm{ESM}{\tilde p}_0}({\boldsymbol{\theta}}) \approx J_{\mathrm{ESM}}({\boldsymbol{\theta}}) = J_{\mathrm{ISM}}({\boldsymbol{\theta}}) + C \approx {\tilde J}_{\mathrm{FDSM}}({\boldsymbol{\theta}}) + C \approx {\tilde J}_{\mathrm{FDSM}{\tilde p}_0}({\boldsymbol{\theta}}) + C.\]</p><p>which is a good test for the implementations of the loss functions. For example:</p><pre><code class="language-julia hljs">first(loss_function_cheat(model, ps, st, data_cheat))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">32.62664847852629</code></pre><pre><code class="language-julia hljs">first(loss_function(model, ps, st, data)) + Jconstant</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">32.56518812592081</code></pre><p>Let us do a more statistically significant test.</p><pre><code class="language-julia hljs">test_losses = reduce(
    hcat,
    Lux.setup(rng, model) |&gt; pstj -&gt;
    [
        first(loss_function_cheat(model, pstj[1], pstj[2], data_cheat)),
        first(loss_function(model, pstj[1], pstj[2], data))
    ]
    for _ in 1:30
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×30 Matrix{Float64}:
 13.7323  38.8598  5.71619  9.86731  …  38.9853  20.4021  14.7908  7.77597
 13.0453  37.4968  4.90646  8.84147     38.2617  19.6932  13.6616  7.00106</code></pre><pre><code class="language-julia hljs">plot(title=&quot;Loss functions at random model parameters&quot;, titlefont=10)
scatter!(test_losses[1, :], label=&quot;\${\\tilde J}_{\\mathrm{ESM}{\\tilde p}_0}\$&quot;)
scatter!(test_losses[2, :], label=&quot;\${\\tilde J}_{\\mathrm{FDSM}{\\tilde p}_0}\$&quot;)
scatter!(test_losses[2, :] .+ Jconstant, label=&quot;\${\\tilde J}_{\\mathrm{FDSM}{\\tilde p}_0} + C\$&quot;)</code></pre><img src="397afc17.svg" alt="Example block output"/><p>One can check by visual inspection that the agreement between <span>${\tilde J}_{\mathrm{ESM}{\tilde p}_0}({\boldsymbol{\theta}}) - C$</span> and <span>${\tilde J}_{\mathrm{FDSM}{\tilde p}_0}({\boldsymbol{\theta}})$</span> seems reasonably good. Let us estimate the relative error.</p><pre><code class="language-julia hljs">rel_errors = abs.( ( test_losses[2, :] .+ Jconstant .- test_losses[1, :] ) ./ test_losses[1, :] )
plot(title=&quot;Relative error at random model parameters&quot;, titlefont=10, legend=false)
scatter!(rel_errors, markercolor=2, label=&quot;error&quot;)
mm = mean(rel_errors)
mmstd = std(rel_errors)
hline!([mm], label=&quot;mean&quot;)
hspan!([mm+mmstd, mm-mmstd], fillbetween=true, alpha=0.3, label=&quot;65% margin&quot;)</code></pre><img src="0b2ec09f.svg" alt="Example block output"/><p>Ok, good enough, just a few percentage points.</p><h3 id="An-extra-test-for-the-implementations-of-the-loss-functions-and-the-gradient-computation"><a class="docs-heading-anchor" href="#An-extra-test-for-the-implementations-of-the-loss-functions-and-the-gradient-computation">An extra test for the implementations of the loss functions and the gradient computation</a><a id="An-extra-test-for-the-implementations-of-the-loss-functions-and-the-gradient-computation-1"></a><a class="docs-heading-anchor-permalink" href="#An-extra-test-for-the-implementations-of-the-loss-functions-and-the-gradient-computation" title="Permalink"></a></h3><p>We also have</p><p class="math-container">\[    \boldsymbol{\nabla}_{\boldsymbol{\theta}} {\tilde J}_{\mathrm{ESM}{\tilde p}_0}({\boldsymbol{\theta}}) \approx \boldsymbol{\nabla}_{\boldsymbol{\theta}} {\tilde J}_{\mathrm{FDSM}{\tilde p}_0}({\boldsymbol{\theta}}),\]</p><p>which is another good test, which also checks the gradient computation, but everything seems fine, so no need to push this further.</p><h2 id="Optimization-setup"><a class="docs-heading-anchor" href="#Optimization-setup">Optimization setup</a><a id="Optimization-setup-1"></a><a class="docs-heading-anchor-permalink" href="#Optimization-setup" title="Permalink"></a></h2><h3 id="Optimization-method"><a class="docs-heading-anchor" href="#Optimization-method">Optimization method</a><a id="Optimization-method-1"></a><a class="docs-heading-anchor-permalink" href="#Optimization-method" title="Permalink"></a></h3><p>As usual, we use the ADAM optimization.</p><pre><code class="language-julia hljs">opt = Adam(0.003)

tstate_org = Lux.Training.TrainState(model, ps, st, opt)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">TrainState(
    Chain(
        layer_1 = Dense(2 =&gt; 16, relu),           <span class="sgr90"># 48 parameters</span>
        layer_2 = Dense(16 =&gt; 2),                 <span class="sgr90"># 34 parameters</span>
    ),
    number of parameters: 82
    number of states: 0
    optimizer: Optimisers.Adam(eta=0.003, beta=(0.9, 0.999), epsilon=1.0e-8)
    step: 0
)
</code></pre><h3 id="Automatic-differentiation-in-the-optimization"><a class="docs-heading-anchor" href="#Automatic-differentiation-in-the-optimization">Automatic differentiation in the optimization</a><a id="Automatic-differentiation-in-the-optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Automatic-differentiation-in-the-optimization" title="Permalink"></a></h3><p><a href="https://github.com/FluxML/Zygote.jl">FluxML/Zygote.jl</a> is used for the automatic differentiation as it is currently the only AD backend working with <a href="https://github.com/LuxDL/Lux.jl">LuxDL/Lux.jl</a>.</p><pre><code class="language-julia hljs">vjp_rule = Lux.Training.AutoZygote()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">ADTypes.AutoZygote()</code></pre><h3 id="Processor"><a class="docs-heading-anchor" href="#Processor">Processor</a><a id="Processor-1"></a><a class="docs-heading-anchor-permalink" href="#Processor" title="Permalink"></a></h3><p>We use the CPU instead of the GPU.</p><pre><code class="language-julia hljs">dev_cpu = cpu_device()
## dev_gpu = gpu_device()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(::MLDataDevices.CPUDevice{Missing}) (generic function with 1 method)</code></pre><h3 id="Check-differentiation"><a class="docs-heading-anchor" href="#Check-differentiation">Check differentiation</a><a id="Check-differentiation-1"></a><a class="docs-heading-anchor-permalink" href="#Check-differentiation" title="Permalink"></a></h3><p>Check if AD is working fine to differentiate the loss functions for training.</p><pre><code class="language-julia hljs">Lux.Training.compute_gradients(vjp_rule, loss_function, data, tstate_org)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">((layer_1 = (weight = Float32[3.6879158 4.0172615; -2.2284822 -2.6868043; … ; -1.9008694 -1.7861443; -4.612377 -4.320793], bias = Float32[0.8997364, -0.8043432, 1.7370119, 0.45905542, 0.8397131, -1.230505, 0.353755, -1.2257671, 0.9869879, 1.0354862, 0.52109236, 1.3021913, 1.5245895, 0.877233, 0.5858979, 1.4446392]), layer_2 = (weight = Float32[4.2770233 22.516876 … -17.53038 -24.572052; 11.982891 26.250607 … 15.06171 21.036867], bias = Float32[-0.113342285, 5.354232])), 31.673041841884316, (), Lux.Training.TrainState{Nothing, Nothing, Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.relu), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, @NamedTuple{layer_1::@NamedTuple{weight::Matrix{Float32}, bias::Vector{Float32}}, layer_2::@NamedTuple{weight::Matrix{Float32}, bias::Vector{Float32}}}, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}}, Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, @NamedTuple{layer_1::@NamedTuple{weight::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}}, layer_2::@NamedTuple{weight::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}}}}(nothing, nothing, Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.relu), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(2 =&gt; 16, relu), layer_2 = Dense(16 =&gt; 2)), nothing), (layer_1 = (weight = Float32[-1.2633736 1.8181845; 2.3867178 0.5713208; … ; -0.7349689 -1.3411301; -1.167693 -1.8823313], bias = Float32[0.48686042, 0.32835403, 0.23500215, -0.6289243, -0.4877348, -0.08177762, 0.57667726, -0.06088416, 0.20568033, -0.66487825, 0.17736456, -0.20175992, 0.4999213, -0.5164128, 0.5682744, 0.33539677]), layer_2 = (weight = Float32[0.3586208 -0.057023764 … -0.17773739 -0.41380876; 0.12927775 -0.22223856 … 0.06703623 0.18231718], bias = Float32[0.067408144, 0.110440105])), (layer_1 = NamedTuple(), layer_2 = NamedTuple()), Optimisers.Adam(eta=0.003, beta=(0.9, 0.999), epsilon=1.0e-8), (layer_1 = (weight = <span class="sgr32">Leaf(Adam(eta=0.003, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0 0.0; 0.0 0.0; … ; 0.0 0.0; 0.0 0.0], Float32[0.0 0.0; 0.0 0.0; … ; 0.0 0.0; 0.0 0.0], (0.9, 0.999))<span class="sgr32">)</span>, bias = <span class="sgr32">Leaf(Adam(eta=0.003, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))<span class="sgr32">)</span>), layer_2 = (weight = <span class="sgr32">Leaf(Adam(eta=0.003, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))<span class="sgr32">)</span>, bias = <span class="sgr32">Leaf(Adam(eta=0.003, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0, 0.0], Float32[0.0, 0.0], (0.9, 0.999))<span class="sgr32">)</span>)), 0))</code></pre><pre><code class="language-julia hljs">Lux.Training.compute_gradients(vjp_rule, loss_function_cheat, data_cheat, tstate_org)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">((layer_1 = (weight = Float32[3.689433 4.0178785; -2.264155 -2.618133; … ; -1.9069793 -1.7834694; -4.6186814 -4.3203063], bias = Float32[0.8609152, -0.7731151, 1.725719, 0.44558188, 0.84764415, -1.251586, 0.36021093, -1.1504575, 0.975368, 1.044547, 0.5048005, 1.1946136, 1.5058247, 0.893234, 0.58936614, 1.429456]), layer_2 = (weight = Float32[4.235089 22.786686 … -17.630789 -24.70937; 11.946889 26.341808 … 14.838343 20.73272], bias = Float32[-0.11527908, 5.321644])), 32.62664847852629, (), Lux.Training.TrainState{Nothing, Nothing, Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.relu), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, @NamedTuple{layer_1::@NamedTuple{weight::Matrix{Float32}, bias::Vector{Float32}}, layer_2::@NamedTuple{weight::Matrix{Float32}, bias::Vector{Float32}}}, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}}, Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, @NamedTuple{layer_1::@NamedTuple{weight::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}}, layer_2::@NamedTuple{weight::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}}}}(nothing, nothing, Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.relu), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(2 =&gt; 16, relu), layer_2 = Dense(16 =&gt; 2)), nothing), (layer_1 = (weight = Float32[-1.2633736 1.8181845; 2.3867178 0.5713208; … ; -0.7349689 -1.3411301; -1.167693 -1.8823313], bias = Float32[0.48686042, 0.32835403, 0.23500215, -0.6289243, -0.4877348, -0.08177762, 0.57667726, -0.06088416, 0.20568033, -0.66487825, 0.17736456, -0.20175992, 0.4999213, -0.5164128, 0.5682744, 0.33539677]), layer_2 = (weight = Float32[0.3586208 -0.057023764 … -0.17773739 -0.41380876; 0.12927775 -0.22223856 … 0.06703623 0.18231718], bias = Float32[0.067408144, 0.110440105])), (layer_1 = NamedTuple(), layer_2 = NamedTuple()), Optimisers.Adam(eta=0.003, beta=(0.9, 0.999), epsilon=1.0e-8), (layer_1 = (weight = <span class="sgr32">Leaf(Adam(eta=0.003, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0 0.0; 0.0 0.0; … ; 0.0 0.0; 0.0 0.0], Float32[0.0 0.0; 0.0 0.0; … ; 0.0 0.0; 0.0 0.0], (0.9, 0.999))<span class="sgr32">)</span>, bias = <span class="sgr32">Leaf(Adam(eta=0.003, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))<span class="sgr32">)</span>), layer_2 = (weight = <span class="sgr32">Leaf(Adam(eta=0.003, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))<span class="sgr32">)</span>, bias = <span class="sgr32">Leaf(Adam(eta=0.003, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0, 0.0], Float32[0.0, 0.0], (0.9, 0.999))<span class="sgr32">)</span>)), 0))</code></pre><h3 id="Training-loop"><a class="docs-heading-anchor" href="#Training-loop">Training loop</a><a id="Training-loop-1"></a><a class="docs-heading-anchor-permalink" href="#Training-loop" title="Permalink"></a></h3><p>Here is the typical main training loop suggest in the <a href="https://github.com/LuxDL/Lux.jl">LuxDL/Lux.jl</a> tutorials, but sligthly modified to save the history of losses per iteration and the model state for animation.</p><pre><code class="language-julia hljs">function train(tstate, vjp, data, loss_function, epochs, numshowepochs=20, numsavestates=0)
    losses = zeros(epochs)
    tstates = [(0, tstate)]
    for epoch in 1:epochs
        grads, loss, stats, tstate = Lux.Training.compute_gradients(vjp,
            loss_function, data, tstate)
        if ( epochs ≥ numshowepochs &gt; 0 ) &amp;&amp; rem(epoch, div(epochs, numshowepochs)) == 0
            println(&quot;Epoch: $(epoch) || Loss: $(loss)&quot;)
        end
        if ( epochs ≥ numsavestates &gt; 0 ) &amp;&amp; rem(epoch, div(epochs, numsavestates)) == 0
            push!(tstates, (epoch, tstate))
        end
        losses[epoch] = loss
        tstate = Lux.Training.apply_gradients(tstate, grads)
    end
    return tstate, losses, tstates
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">train (generic function with 3 methods)</code></pre><h2 id="Cheat-training-with-{\\tilde-J}_{\\mathrm{ESM}{\\tilde-p}_0}"><a class="docs-heading-anchor" href="#Cheat-training-with-{\\tilde-J}_{\\mathrm{ESM}{\\tilde-p}_0}">Cheat training with <span>${\tilde J}_{\mathrm{ESM}{\tilde p}_0}$</span></a><a id="Cheat-training-with-{\\tilde-J}_{\\mathrm{ESM}{\\tilde-p}_0}-1"></a><a class="docs-heading-anchor-permalink" href="#Cheat-training-with-{\\tilde-J}_{\\mathrm{ESM}{\\tilde-p}_0}" title="Permalink"></a></h2><p>We first train the model with the known score function on the sample data. That is cheating. The aim is a sanity check, to make sure the proposed model is good enough to fit the desired score function and that the setup is right.</p><pre><code class="language-julia hljs">@time tstate_cheat, losses_cheat, tstates_cheat = train(tstate_org, vjp_rule, data_cheat, loss_function_cheat, 2000, 20, 100)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌ Warning: Mixed-Precision `matmul_cpu_fallback!` detected and Octavian.jl cannot be used for this set of inputs (C [Matrix{Float64}]: A [Matrix{Float32}] x B [Matrix{Float64}]). Falling back to generic implementation. This may be slow.
└ @ LuxLib.Impl ~/.julia/packages/LuxLib/R8Czx/src/impl/matmul.jl:190
Epoch: 100 || Loss: 0.8401849278869697
Epoch: 200 || Loss: 0.563607700266948
Epoch: 300 || Loss: 0.4926258151863607
Epoch: 400 || Loss: 0.45830559661430026
Epoch: 500 || Loss: 0.43358616510264614
Epoch: 600 || Loss: 0.41197109354966255
Epoch: 700 || Loss: 0.39327886995951344
Epoch: 800 || Loss: 0.3768739639070251
Epoch: 900 || Loss: 0.3631936077433342
Epoch: 1000 || Loss: 0.35101209994291793
Epoch: 1100 || Loss: 0.33997820230228853
Epoch: 1200 || Loss: 0.3293390319395758
Epoch: 1300 || Loss: 0.31880814244768374
Epoch: 1400 || Loss: 0.30846754084144945
Epoch: 1500 || Loss: 0.2984211525235665
Epoch: 1600 || Loss: 0.28827943967632763
Epoch: 1700 || Loss: 0.2778414128971065
Epoch: 1800 || Loss: 0.26736572655519075
Epoch: 1900 || Loss: 0.25641855404231345
Epoch: 2000 || Loss: 0.2448096823452019
  1.410573 seconds (1.09 M allocations: 981.514 MiB, 48.40% gc time, 4.05% compilation time)</code></pre><p>Testing out the trained model.</p><pre><code class="language-julia hljs">uu_cheat = Lux.apply(tstate_cheat.model, vcat(xx&#39;, yy&#39;), tstate_cheat.parameters, tstate_cheat.states)[1]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×225 Matrix{Float64}:
 2.19923   1.53509  0.870944  0.206801  …   0.502545  -0.277666  -1.05788
 0.997473  1.47914  1.9608    2.44247      -3.00036   -2.75735   -2.51434</code></pre><pre><code class="language-julia hljs">heatmap(xrange, yrange, (x, y) -&gt; logpdf(target_prob, [x, y]), title=&quot;Logpdf (heatmap) and score functions (vector fields)&quot;, titlefont=10, color=:vik, xlims=extrema(xrange), ylims=extrema(yrange), legend=false)
quiver!(xx, yy, quiver = (uu[1, :] ./ 8, uu[2, :] ./ 8), color=:yellow, alpha=0.5)
scatter!(sample_points[1, :], sample_points[2, :], markersize=2, markercolor=:lightgreen, alpha=0.5)
quiver!(xx, yy, quiver = (uu_cheat[1, :] ./ 8, uu_cheat[2, :] ./ 8), color=:cyan, alpha=0.5)</code></pre><img src="434f708e.svg" alt="Example block output"/><img src="74ce20c9.gif" alt="Example block output"/><pre><code class="language-julia hljs">plot(losses_cheat, title=&quot;Evolution of the loss&quot;, titlefont=10, xlabel=&quot;iteration&quot;, ylabel=&quot;error&quot;, legend=false)</code></pre><img src="1c305918.svg" alt="Example block output"/><h2 id="Real-training-with-{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}"><a class="docs-heading-anchor" href="#Real-training-with-{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}">Real training with <span>${\tilde J}_{\mathrm{FDSM}{\tilde p}_0}$</span></a><a id="Real-training-with-{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}-1"></a><a class="docs-heading-anchor-permalink" href="#Real-training-with-{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}" title="Permalink"></a></h2><p>Now we go to the real thing.</p><pre><code class="language-julia hljs">@time tstate, losses, tstates = train(tstate_org, vjp_rule, data, loss_function, 2000, 20, 100)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌ Warning: Mixed-Precision `matmul_cpu_fallback!` detected and Octavian.jl cannot be used for this set of inputs (C [Matrix{Float64}]: A [Matrix{Float32}] x B [Matrix{Float64}]). Falling back to generic implementation. This may be slow.
└ @ LuxLib.Impl ~/.julia/packages/LuxLib/R8Czx/src/impl/matmul.jl:190
Epoch: 100 || Loss: -0.02852607188206327
Epoch: 200 || Loss: -0.3056820935385301
Epoch: 300 || Loss: -0.37357200432482657
Epoch: 400 || Loss: -0.4060071208695771
Epoch: 500 || Loss: -0.4270067649241402
Epoch: 600 || Loss: -0.44428054612672957
Epoch: 700 || Loss: -0.4591184077812442
Epoch: 800 || Loss: -0.4711387063320863
Epoch: 900 || Loss: -0.4809373364465081
Epoch: 1000 || Loss: -0.4897282508077628
Epoch: 1100 || Loss: -0.49795548481343355
Epoch: 1200 || Loss: -0.5057248136083727
Epoch: 1300 || Loss: -0.5133231683129975
Epoch: 1400 || Loss: -0.5203089294538503
Epoch: 1500 || Loss: -0.527191618676005
Epoch: 1600 || Loss: -0.533918647530345
Epoch: 1700 || Loss: -0.5407054119759427
Epoch: 1800 || Loss: -0.5470494965278498
Epoch: 1900 || Loss: -0.5533731989980993
Epoch: 2000 || Loss: -0.5598464396520437
  2.342197 seconds (2.81 M allocations: 4.164 GiB, 9.46% gc time, 2.46% compilation time)</code></pre><p>Testing out the trained model.</p><pre><code class="language-julia hljs">uu_pred = Lux.apply(tstate.model, vcat(xx&#39;, yy&#39;), tstate.parameters, tstate.states)[1]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×225 Matrix{Float64}:
  1.71235   1.17107   0.606229  -0.0336345  …   1.14829   0.627492   0.106691
 -0.196228  0.441023  1.20491    1.90302       -2.18941  -2.07645   -1.9635</code></pre><pre><code class="language-julia hljs">heatmap(xrange, yrange, (x, y) -&gt; logpdf(target_prob, [x, y]), title=&quot;Logpdf (heatmap) and score functions (vector fields)&quot;, titlefont=10, color=:vik, xlims=extrema(xrange), ylims=extrema(yrange), legend=false)
quiver!(xx, yy, quiver = (uu[1, :] ./ 8, uu[2, :] ./ 8), color=:yellow, alpha=0.5)
scatter!(sample_points[1, :], sample_points[2, :], markersize=2, markercolor=:lightgreen, alpha=0.5)
quiver!(xx, yy, quiver = (uu_pred[1, :] ./ 8, uu_pred[2, :] ./ 8), color=:cyan, alpha=0.5)</code></pre><img src="53b415ad.svg" alt="Example block output"/><img src="5d66e43d.gif" alt="Example block output"/><pre><code class="language-julia hljs">plot(losses, title=&quot;Evolution of the losses&quot;, titlefont=10, xlabel=&quot;iteration&quot;, ylabel=&quot;error&quot;, label=&quot;\${\\tilde J}_{\\mathrm{FDSM}{\\tilde p}_0}\$&quot;)
plot!(losses_cheat, linestyle=:dash, label=&quot;\${\\tilde J}_{\\mathrm{ESM}{\\tilde p}_0}\$&quot;)
plot!(losses .+ Jconstant, linestyle=:dash, color=1, label=&quot;\${\\tilde J}_{\\mathrm{FDSM}{\\tilde p}_0} + C\$&quot;)</code></pre><img src="9ccc9931.svg" alt="Example block output"/><p>Ok, that seems visually good enough. We will later check the sampling from this score function via Langevin sampling.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../1d_FD_score_matching/">« 1D finite-difference score matching</a><a class="docs-footer-nextpage" href="../ddpm/">Denoising diffusion probabilistic models »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.15.0 on <span class="colophon-date" title="Thursday 6 November 2025 15:40">Thursday 6 November 2025</span>. Using Julia version 1.12.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
