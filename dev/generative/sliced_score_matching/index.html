<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Sliced score matching · Random notes</title><meta name="title" content="Sliced score matching · Random notes"/><meta property="og:title" content="Sliced score matching · Random notes"/><meta property="twitter:title" content="Sliced score matching · Random notes"/><meta name="description" content="Documentation for Random notes."/><meta property="og:description" content="Documentation for Random notes."/><meta property="twitter:description" content="Documentation for Random notes."/><meta property="og:url" content="https://github.com/rmsrosa/random_notes/generative/sliced_score_matching/"/><meta property="twitter:url" content="https://github.com/rmsrosa/random_notes/generative/sliced_score_matching/"/><link rel="canonical" href="https://github.com/rmsrosa/random_notes/generative/sliced_score_matching/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/style.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="Random notes logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Random notes</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Random Notes</a></li><li><span class="tocitem">Probability Essentials</span><ul><li><a class="tocitem" href="../../probability/kernel_density_estimation/">Kernel Density Estimation</a></li><li><a class="tocitem" href="../../probability/convergence_notions/">Convergence notions</a></li></ul></li><li><span class="tocitem">Discrete-time Markov chains</span><ul><li><a class="tocitem" href="../../markov_chains/mc_definitions/">Essential definitions</a></li><li><a class="tocitem" href="../../markov_chains/mc_invariance/">Invariant distributions</a></li><li><input class="collapse-toggle" id="menuitem-3-3" type="checkbox"/><label class="tocitem" for="menuitem-3-3"><span class="docs-label">Countable-space Markov chains</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../markov_chains/mc_countableX_recurrence/">Recurrence in the countable-space case</a></li><li><a class="tocitem" href="../../markov_chains/mc_countableX_connections/">Connected states, irreducibility and uniqueness of invariant measures</a></li><li><a class="tocitem" href="../../markov_chains/mc_countableX_convergencia/">Aperiodicidade e convergência para a distribuição estacionária</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-4" type="checkbox"/><label class="tocitem" for="menuitem-3-4"><span class="docs-label">Continuous-space Markov chains</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../markov_chains/mc_irreducibility_and_recurrence/">Irreducibility and recurrence in the continuous-space case</a></li></ul></li></ul></li><li><span class="tocitem">Sampling methods</span><ul><li><a class="tocitem" href="../../sampling/overview/">Overview</a></li><li><a class="tocitem" href="../../sampling/prng/">Random number generators</a></li><li><input class="collapse-toggle" id="menuitem-4-3" type="checkbox"/><label class="tocitem" for="menuitem-4-3"><span class="docs-label">Transform methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/invFtransform/">Probability integral transform</a></li><li><a class="tocitem" href="../../sampling/box_muller/">Box-Muller transform</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-4" type="checkbox"/><label class="tocitem" for="menuitem-4-4"><span class="docs-label">Accept-Reject methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/rejection_sampling/">Rejection sampling</a></li><li><a class="tocitem" href="../../sampling/empiricalsup_rejection/">Empirical supremum rejection sampling</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-5" type="checkbox"/><label class="tocitem" for="menuitem-4-5"><span class="docs-label">Markov Chain Monte Carlo (MCMC)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/mcmc/">Overview</a></li><li><a class="tocitem" href="../../sampling/metropolis/">Metropolis and Metropolis-Hastings</a></li><li><a class="tocitem" href="../../sampling/convergence_metropolis/">Convergence of Metropolis-Hastings</a></li><li><a class="tocitem" href="../../sampling/gibbs/">Gibbs sampling</a></li><li><a class="tocitem" href="../../sampling/hmc/">Hamiltonian Monte Carlo (HMC)</a></li></ul></li><li><a class="tocitem" href="../../sampling/langevin_sampling/">Langevin sampling</a></li></ul></li><li><span class="tocitem">Bayesian inference</span><ul><li><input class="collapse-toggle" id="menuitem-5-1" type="checkbox"/><label class="tocitem" for="menuitem-5-1"><span class="docs-label">Bayes Theory</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/bayes/">Bayes Theorem</a></li><li><a class="tocitem" href="../../bayesian/bayes_inference/">Bayesian inference</a></li><li><a class="tocitem" href="../../bayesian/bernstein_vonmises/">Bernstein–von Mises theorem</a></li></ul></li><li><a class="tocitem" href="../../bayesian/bayesian_probprog/">Bayesian probabilistic programming</a></li><li><input class="collapse-toggle" id="menuitem-5-3" type="checkbox"/><label class="tocitem" for="menuitem-5-3"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/find_pi/">Estimating π via frequentist and Bayesian methods</a></li><li><a class="tocitem" href="../../bayesian/linear_regression/">Many Ways to Linear Regression</a></li><li><a class="tocitem" href="../../bayesian/tilapia_alometry/">Alometry law for the Nile Tilapia</a></li><li><a class="tocitem" href="../../bayesian/mortality_tables/">Modeling mortality tables</a></li></ul></li></ul></li><li><span class="tocitem">Generative models</span><ul><li><input class="collapse-toggle" id="menuitem-6-1" type="checkbox" checked/><label class="tocitem" for="menuitem-6-1"><span class="docs-label">Score matching</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../overview/">Overview</a></li><li><a class="tocitem" href="../stein_score/">Stein score function</a></li><li><a class="tocitem" href="../score_matching_aapo/">Score matching of Aapo Hyvärinen</a></li><li><a class="tocitem" href="../score_matching_neural_network/">Score matching a neural network</a></li><li><a class="tocitem" href="../parzen_estimation_score_matching/">Score matching with Parzen estimation</a></li><li><a class="tocitem" href="../denoising_score_matching/">Denoising score matching of Pascal Vincent</a></li><li class="is-active"><a class="tocitem" href>Sliced score matching</a><ul class="internal"><li><a class="tocitem" href="#Aim"><span>Aim</span></a></li><li><a class="tocitem" href="#Background"><span>Background</span></a></li><li><a class="tocitem" href="#The-sliced-score-matching-objective-function"><span>The sliced score matching objective function</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../1d_FD_score_matching/">1D finite-difference score matching</a></li><li><a class="tocitem" href="../2d_FD_score_matching/">2D finite-difference score matching</a></li><li><a class="tocitem" href="../ddpm/">Denoising diffusion probabilistic models</a></li><li><a class="tocitem" href="../mdsm/">Multiple denoising score matching</a></li><li><a class="tocitem" href="../probability_flow/">Probability flow</a></li><li><a class="tocitem" href="../reverse_flow/">Reverse probability flow</a></li><li><a class="tocitem" href="../score_based_sde/">Score-based SDE model</a></li></ul></li></ul></li><li><span class="tocitem">Sensitivity analysis</span><ul><li><a class="tocitem" href="../../sensitivity/overview/">Overview</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Generative models</a></li><li><a class="is-disabled">Score matching</a></li><li class="is-active"><a href>Sliced score matching</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Sliced score matching</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/rmsrosa/random_notes" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/rmsrosa/random_notes/blob/main/docs/src/generative/sliced_score_matching.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Sliced-score-matching"><a class="docs-heading-anchor" href="#Sliced-score-matching">Sliced score matching</a><a id="Sliced-score-matching-1"></a><a class="docs-heading-anchor-permalink" href="#Sliced-score-matching" title="Permalink"></a></h1><h2 id="Aim"><a class="docs-heading-anchor" href="#Aim">Aim</a><a id="Aim-1"></a><a class="docs-heading-anchor-permalink" href="#Aim" title="Permalink"></a></h2><p>Detail the sliced score matching method proposed by <a href="https://proceedings.mlr.press/v115/song20a.html">Song, Garg, Shi, and Ermon (2020)</a> to reduce the computational cost of the score-matching method for high-dimensional problems.</p><h2 id="Background"><a class="docs-heading-anchor" href="#Background">Background</a><a id="Background-1"></a><a class="docs-heading-anchor-permalink" href="#Background" title="Permalink"></a></h2><p>The score-matching method proposed by <a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Aapo Hyvärinen (2005)</a> is based on minimizing the <strong>empirical implicit score matching</strong> objective function <span>$J_{\mathrm{ISM{\tilde p}_0}}({\boldsymbol{\theta}})$</span> given by</p><p class="math-container">\[    {\tilde J}_{\mathrm{ISM{\tilde p}_0}} = \frac{1}{N}\sum_{n=1}^N \left( \frac{1}{2}\|\boldsymbol{\psi}(\mathbf{x}_n; {\boldsymbol{\theta}})\|^2 + \boldsymbol{\nabla}_{\mathbf{x}} \cdot \boldsymbol{\psi}(\mathbf{x}_n; {\boldsymbol{\theta}}) \right),\]</p><p>where <span>$\boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}})$</span> is the score function of a parametrized model probability density function <span>$p(\mathbf{x}; \boldsymbol{\theta})$</span> fitting the unknown score function <span>$\boldsymbol{\psi}_{\mathbf{X}}(\mathbf{x})$</span> of a random variable <span>$\mathbf{X}$</span> in <span>$\mathbb{R}^d$</span>, <span>$d\in\mathbb{N}$</span>, and <span>$\{\mathbf{x}_n; \;n=1, \ldots, N\}$</span> are <span>$N\in \mathbb{N}$</span> sample points of this random variable.</p><p>The objective function <span>$J_{\mathrm{ISM{\tilde p}_0}}({\boldsymbol{\theta}})$</span> is the approximation, using the empirical distribution</p><p class="math-container">\[    {\tilde p}_0(\mathbf{x}) = \frac{1}{N}\sum_{n=1}^N \delta(\mathbf{x} - \mathbf{x}_n),\]</p><p>of the <strong>implicit score matching</strong> objective</p><p class="math-container">\[    J_{\mathrm{ISM}}({\boldsymbol{\theta}}) = \int_{\mathbb{R}} p_{\mathbf{X}}(\mathbf{x}) \left( \frac{1}{2}\left\|\boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}})\right\|^2 + \boldsymbol{\nabla}_{\mathbf{x}} \cdot \boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}}) \right)\;\mathrm{d}\mathbf{x},\]</p><p>where <span>$p_{\mathbf{X}}(\mathbf{x})$</span> is the (also unknown) probability density function of <span>$X$</span>.</p><p>The difficulty of minimizing <span>$J_{\mathrm{ISM{\tilde p}_0}}({\boldsymbol{\theta}})$</span> is the need to compute the divergence of the score function, which amounts to computing the Hessian of the model function, not to mention the gradient of the loss function itself, needed for the parameter optimization. This is very costly and scales badly with the dimension <span>$d$</span> of the random variable (and of the neural network).</p><p>To reduce this cost, <a href="https://proceedings.mlr.press/v115/song20a.html">Song, Garg, Shi, and Ermon (2020)</a> proposed the <em>sliced-score matching</em> method (see also <a href="http://yang-song.net/blog/2019/ssm/">Song&#39;s blog on sliced score matching</a>). In this method, only some directional derivatives are computed at each sample point, choosing randomly before the start of the minimization process.</p><h2 id="The-sliced-score-matching-objective-function"><a class="docs-heading-anchor" href="#The-sliced-score-matching-objective-function">The sliced score matching objective function</a><a id="The-sliced-score-matching-objective-function-1"></a><a class="docs-heading-anchor-permalink" href="#The-sliced-score-matching-objective-function" title="Permalink"></a></h2><p>In this method, for each sample point <span>$x_n$</span>, <span>$n=1, \ldots, N$</span>, one randomly draws <span>$M$</span> unitary vectors <span>$\mathbf{v}_{n, m}$</span>, <span>$m=1, \ldots, M$</span>, in the space <span>$\mathbb{R}^d$</span> (say uniformly on the hypersphere, or with respect to a multivariate standard normal or a multivariate Rademacher distribution uniform on <span>$\{\pm 1\}^d$</span>, as discussed in <a href="https://proceedings.mlr.press/v115/song20a.html">Song, Garg, Shi, and Ermon (2020)</a>), forming the <strong>(empirical, implicit) sliced score matching</strong> objective function</p><p class="math-container">\[    J_{\mathrm{ISSM{\tilde p}_0}{\tilde q}_0}({\boldsymbol{\theta}}) = \frac{1}{NM}\sum_{n=1}^N \sum_{m=1}^M \left( \frac{1}{2} \left(\mathbf{v}_{n,m} \cdot \boldsymbol{\psi}(\mathbf{x}_n; {\boldsymbol{\theta}}) \right)^2 + \boldsymbol{\nabla}_{\mathbf{v}_{n,m}}\boldsymbol{\psi}(\mathbf{x}_n; {\boldsymbol{\theta}}) \cdot \mathbf{v}_{n,m} \right),\]</p><p>where</p><p class="math-container">\[\boldsymbol{\nabla}_{\mathbf{v}_{n,m}}\boldsymbol{\psi}(\mathbf{x}_n; {\boldsymbol{\theta}})\]</p><p>is the directional derivative of <span>$\boldsymbol{\psi}(\mathbf{x}_n; {\boldsymbol{\theta}})$</span> along <span>$\mathbf{v}_{n,m}$</span>.</p><p>Under suitable conditions, the minimizer <span>$\boldsymbol{\theta}_{N, M}$</span> of <span>$J_{\mathrm{ISSM{\tilde p}_0}}({\boldsymbol{\theta}})$</span> converges to the minimizer <span>$\boldsymbol{\theta}$</span> of the implicit score matching <span>$J_{\mathrm{ISM}}({\boldsymbol{\theta}})$</span> (which is the same as that of the explicit score matching), when <span>$N\rightarrow \infty$</span>, even with <span>$M$</span> fixed (but notice that the number <span>$NM$</span> of sample directions grows with <span>$N$</span>, already).</p><p>The divergence term in the original implicit score matching objective <span>$J_{\mathrm{ISM}}({\boldsymbol{\theta}})$</span> is computed as</p><p class="math-container">\[    \boldsymbol{\nabla}_{\mathbf{x}} \cdot \boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}}) = \sum_{i=1}^d \frac{\partial \psi_i}{\partial x_i}(\mathbf{x}; {\boldsymbol{\theta}}) = \sum_{i=1}^d \boldsymbol{\nabla}_{\mathbf{e}_i} \boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}}) \cdot \mathbf{e}_i\]</p><p>and scales with the dimension <span>$d$</span> of the event space. In contrast, the computation of the collection of <span>$M$</span> directional derivatives</p><p class="math-container">\[    \sum_{m=1}^M \boldsymbol{\nabla}_{\mathbf{v}_{n,m}}\boldsymbol{\psi}(\mathbf{x}_n; {\boldsymbol{\theta}}) \cdot \mathbf{v}_{n,m}\]</p><p>scales with <span>$M$</span>. For high-dimensional problems, as in the applications in mind, <span>$d$</span> is very large, and <span>$M$</span> is chosen much smaller than <span>$d$</span>, saving a lot of computational time.</p><p>The objective <span>$J_{\mathrm{ISSM{\tilde p}_0}}({\boldsymbol{\theta}})$</span> can be seen as an approximation, with the empirical distributions (<span>$p_0$</span> on <span>$\mathbf{x}$</span> and <span>$q_0$</span> on <span>$\mathbf{v}$</span>), of the <strong>(implicit) sliced score matching</strong> objective</p><p class="math-container">\[    J_{\mathrm{ISSM}}({\boldsymbol{\theta}}) = \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} p_{\mathbf{X}}(\mathbf{x}) q_{\mathbf{V}}(\mathbf{v})\left( \frac{1}{2} \left(\mathbf{v} \cdot \boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}}) \right)^2 + \boldsymbol{\nabla}_{\mathbf{v}}\boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}}) \cdot \mathbf{v} \right) \mathrm{d}\mathbf{v}\,\mathrm{d}\mathbf{x},\]</p><p>for a probability density function <span>$q_{\mathbf{V}}(\mathbf{v})$</span>, associated with a random vector <span>$\mathbf{V}$</span> for the directions (a uniform distribution on the hypersphere <span>$S_{d-1}\subset \mathbb{R}^d$</span> or a multivariate standard normal or a multivariate Rademacher distribution uniform on <span>$\{\pm 1\}^d$</span>, as mentioned above) independent of <span>$X$</span>.</p><p>The objective <span>$J_{\mathrm{ISSM}}({\boldsymbol{\theta}})$</span>, in turn, can be obtained via integration by parts, from the <strong>explicit score matching</strong> objective</p><p class="math-container">\[    J_{\mathrm{ESSM}}({\boldsymbol{\theta}}) = \frac{1}{2} \int_{\mathbb{R}^d} \int_{\mathbb{R}^d}p_{\mathbf{X}}(\mathbf{x}) q_{\mathbf{V}}(\mathbf{v})\left\| \mathbf{v} \cdot \boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}}) + \boldsymbol{\nabla}_{\mathbf{v}}\boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}}) \cdot \mathbf{v} \right\|^2 \mathrm{d}\mathbf{v}\,\mathrm{d}\mathbf{x}.\]</p><p>We do not implement numerically the sliced score matching method since it is still computationally intensive for automatic differentiation and this approach won&#39;t be extended to our (current) final aim of denoising diffusion.</p><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><ol><li><p><a href="https://proceedings.mlr.press/v115/song20a.html">Y. Song, S. Garg, J. Shi, S. Ermon (2020), Sliced Score Matching: A Scalable Approach to Density and Score Estimation, Proceedings of The 35th Uncertainty in Artificial Intelligence Conference, PMLR 115:574-584</a> – see also the <a href="https://arxiv.org/abs/1905.07088">arxiv version</a></p></li><li><p><a href="http://yang-song.net/blog/2019/ssm/">Y. Song&#39;s blog on &quot;Sliced Score Matching: A Scalable Approach to Density and Score Estimation&quot;</a></p></li><li><p><a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Aapo Hyvärinen (2005), &quot;Estimation of non-normalized statistical models by score matching&quot;, Journal of Machine Learning Research 6, 695-709</a></p></li></ol></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../denoising_score_matching/">« Denoising score matching of Pascal Vincent</a><a class="docs-footer-nextpage" href="../1d_FD_score_matching/">1D finite-difference score matching »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.15.0 on <span class="colophon-date" title="Thursday 6 November 2025 15:40">Thursday 6 November 2025</span>. Using Julia version 1.12.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
