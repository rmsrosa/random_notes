<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Denoising diffusion probabilistic models · Random notes</title><meta name="title" content="Denoising diffusion probabilistic models · Random notes"/><meta property="og:title" content="Denoising diffusion probabilistic models · Random notes"/><meta property="twitter:title" content="Denoising diffusion probabilistic models · Random notes"/><meta name="description" content="Documentation for Random notes."/><meta property="og:description" content="Documentation for Random notes."/><meta property="twitter:description" content="Documentation for Random notes."/><meta property="og:url" content="https://github.com/rmsrosa/random_notes/generative/ddpm/"/><meta property="twitter:url" content="https://github.com/rmsrosa/random_notes/generative/ddpm/"/><link rel="canonical" href="https://github.com/rmsrosa/random_notes/generative/ddpm/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/style.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="Random notes logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Random notes</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Random Notes</a></li><li><span class="tocitem">Probability Essentials</span><ul><li><a class="tocitem" href="../../probability/kernel_density_estimation/">Kernel Density Estimation</a></li><li><a class="tocitem" href="../../probability/convergence_notions/">Convergence notions</a></li></ul></li><li><span class="tocitem">Discrete-time Markov chains</span><ul><li><a class="tocitem" href="../../markov_chains/mc_definitions/">Essential definitions</a></li><li><a class="tocitem" href="../../markov_chains/mc_invariance/">Invariant distributions</a></li><li><input class="collapse-toggle" id="menuitem-3-3" type="checkbox"/><label class="tocitem" for="menuitem-3-3"><span class="docs-label">Countable-space Markov chains</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../markov_chains/mc_countableX_recurrence/">Recurrence in the countable-space case</a></li><li><a class="tocitem" href="../../markov_chains/mc_countableX_connections/">Connected states, irreducibility and uniqueness of invariant measures</a></li><li><a class="tocitem" href="../../markov_chains/mc_countableX_convergencia/">Aperiodicidade e convergência para a distribuição estacionária</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-4" type="checkbox"/><label class="tocitem" for="menuitem-3-4"><span class="docs-label">Continuous-space Markov chains</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../markov_chains/mc_irreducibility_and_recurrence/">Irreducibility and recurrence in the continuous-space case</a></li></ul></li></ul></li><li><span class="tocitem">Sampling methods</span><ul><li><a class="tocitem" href="../../sampling/overview/">Overview</a></li><li><a class="tocitem" href="../../sampling/prng/">Random number generators</a></li><li><input class="collapse-toggle" id="menuitem-4-3" type="checkbox"/><label class="tocitem" for="menuitem-4-3"><span class="docs-label">Transform methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/invFtransform/">Probability integral transform</a></li><li><a class="tocitem" href="../../sampling/box_muller/">Box-Muller transform</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-4" type="checkbox"/><label class="tocitem" for="menuitem-4-4"><span class="docs-label">Accept-Reject methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/rejection_sampling/">Rejection sampling</a></li><li><a class="tocitem" href="../../sampling/empiricalsup_rejection/">Empirical supremum rejection sampling</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-5" type="checkbox"/><label class="tocitem" for="menuitem-4-5"><span class="docs-label">Markov Chain Monte Carlo (MCMC)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/mcmc/">Overview</a></li><li><a class="tocitem" href="../../sampling/metropolis/">Metropolis and Metropolis-Hastings</a></li><li><a class="tocitem" href="../../sampling/convergence_metropolis/">Convergence of Metropolis-Hastings</a></li><li><a class="tocitem" href="../../sampling/gibbs/">Gibbs sampling</a></li><li><a class="tocitem" href="../../sampling/hmc/">Hamiltonian Monte Carlo (HMC)</a></li></ul></li><li><a class="tocitem" href="../../sampling/langevin_sampling/">Langevin sampling</a></li></ul></li><li><span class="tocitem">Bayesian inference</span><ul><li><input class="collapse-toggle" id="menuitem-5-1" type="checkbox"/><label class="tocitem" for="menuitem-5-1"><span class="docs-label">Bayes Theory</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/bayes/">Bayes Theorem</a></li><li><a class="tocitem" href="../../bayesian/bayes_inference/">Bayesian inference</a></li><li><a class="tocitem" href="../../bayesian/bernstein_vonmises/">Bernstein–von Mises theorem</a></li></ul></li><li><a class="tocitem" href="../../bayesian/bayesian_probprog/">Bayesian probabilistic programming</a></li><li><input class="collapse-toggle" id="menuitem-5-3" type="checkbox"/><label class="tocitem" for="menuitem-5-3"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/find_pi/">Estimating π via frequentist and Bayesian methods</a></li><li><a class="tocitem" href="../../bayesian/linear_regression/">Many Ways to Linear Regression</a></li><li><a class="tocitem" href="../../bayesian/tilapia_alometry/">Alometry law for the Nile Tilapia</a></li><li><a class="tocitem" href="../../bayesian/mortality_tables/">Modeling mortality tables</a></li></ul></li></ul></li><li><span class="tocitem">Generative models</span><ul><li><input class="collapse-toggle" id="menuitem-6-1" type="checkbox" checked/><label class="tocitem" for="menuitem-6-1"><span class="docs-label">Score matching</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../overview/">Overview</a></li><li><a class="tocitem" href="../stein_score/">Stein score function</a></li><li><a class="tocitem" href="../score_matching_aapo/">Score matching of Aapo Hyvärinen</a></li><li><a class="tocitem" href="../score_matching_neural_network/">Score matching a neural network</a></li><li><a class="tocitem" href="../parzen_estimation_score_matching/">Score matching with Parzen estimation</a></li><li><a class="tocitem" href="../denoising_score_matching/">Denoising score matching of Pascal Vincent</a></li><li><a class="tocitem" href="../sliced_score_matching/">Sliced score matching</a></li><li><a class="tocitem" href="../1d_FD_score_matching/">1D finite-difference score matching</a></li><li><a class="tocitem" href="../2d_FD_score_matching/">2D finite-difference score matching</a></li><li class="is-active"><a class="tocitem" href>Denoising diffusion probabilistic models</a><ul class="internal"><li><a class="tocitem" href="#Introduction"><span>Introduction</span></a></li><li><a class="tocitem" href="#Details-of-the-method"><span>Details of the method</span></a></li><li><a class="tocitem" href="#More-improvements"><span>More improvements</span></a></li><li><a class="tocitem" href="#Numerical-example"><span>Numerical example</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../mdsm/">Multiple denoising score matching</a></li><li><a class="tocitem" href="../probability_flow/">Probability flow</a></li><li><a class="tocitem" href="../reverse_flow/">Reverse probability flow</a></li><li><a class="tocitem" href="../score_based_sde/">Score-based SDE model</a></li></ul></li></ul></li><li><span class="tocitem">Sensitivity analysis</span><ul><li><a class="tocitem" href="../../sensitivity/overview/">Overview</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Generative models</a></li><li><a class="is-disabled">Score matching</a></li><li class="is-active"><a href>Denoising diffusion probabilistic models</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Denoising diffusion probabilistic models</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/rmsrosa/random_notes" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/rmsrosa/random_notes/blob/main/docs/src/generative/ddpm.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Denoising-diffusion-probabilistic-models"><a class="docs-heading-anchor" href="#Denoising-diffusion-probabilistic-models">Denoising diffusion probabilistic models</a><a id="Denoising-diffusion-probabilistic-models-1"></a><a class="docs-heading-anchor-permalink" href="#Denoising-diffusion-probabilistic-models" title="Permalink"></a></h1><h2 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h2><h3 id="Aim"><a class="docs-heading-anchor" href="#Aim">Aim</a><a id="Aim-1"></a><a class="docs-heading-anchor-permalink" href="#Aim" title="Permalink"></a></h3><p>Review <strong>denoising diffusion probabilistic models (DDPM)</strong> introduced in <a href="https://dl.acm.org/doi/10.5555/3045118.3045358">Sohl-Dickstein, Weiss, Maheswaranathan, Ganguli (2015)</a> and further improved in <a href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html">Ho, Jain, and Abbeel (2020)</a> (and slightly more in <a href="https://openreview.net/forum?id=-NEXDKk8gZ">Nichol and Dhariwal (2021)</a>).</p><h3 id="Motivation"><a class="docs-heading-anchor" href="#Motivation">Motivation</a><a id="Motivation-1"></a><a class="docs-heading-anchor-permalink" href="#Motivation" title="Permalink"></a></h3><p>Build a solid foundation on score-based generative diffusion models, for which DDPM, although not initially seen as score-based, is related to, as a discrete analog of the SDE model.</p><h3 id="Background"><a class="docs-heading-anchor" href="#Background">Background</a><a id="Background-1"></a><a class="docs-heading-anchor-permalink" href="#Background" title="Permalink"></a></h3><p>The main idea in <a href="https://dl.acm.org/doi/10.5555/3045118.3045358">Sohl-Dickstein, Weiss, Maheswaranathan, and Ganguli (2015)</a> is to embed the random variable we want to model into a Markov chain and model the whole reverse process of the Markov chain. This is a much more complex task which greatly increases the dimension of the problem, but which yields more stability to both training and generative processes.</p><p>The desired random variable, for which we only have access to a sample, is considered as an initial condition to a Markov chain which gradually adds noise to the process so that it converges to a simple and tractable distribution, such as a normal or a binomial distribution. The training process fits a model to the reverse process of the Markov chain (starting back from a relatively large time step, where it will be close to the tractable distribution). Then, the model is used to reverse the process and generate (aproximate) samples of our target distribution from samples of the idealized tractable distribution. The tractable asymptotic distribution of the forward process becomes the initial distribution of the model reverse process, and the (initial) desired target distribution is approximated by the final distribution of the reverse model process.</p><p>The original work of <a href="https://dl.acm.org/doi/10.5555/3045118.3045358">Sohl-Dickstein, Weiss, Maheswaranathan, and Ganguli (2015)</a> had great results but the model was a bit complex and training was costly. <a href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html">Ho, Jain, and Abbeel (2020)</a> improved the model by fixing the variance of the model reverse process and simplifying the loss function in a stochastic gradient way, turning the method into an efficient one with great performance.</p><p>Further on, <a href="https://openreview.net/forum?id=-NEXDKk8gZ">Nichol and Dhariwal (2021)</a> showed improvements to the method by changing the linear variance schedule for the forward process proposed in the previous two works into a nonlinear one, which adds noise more gradually in the beginning and at the end. Another improvement was to learn the variance of the model reverse process in a very specific form, which allowed more flexibility in a way that did not hinder the training impractical.</p><p>Besides the original articles, another source, which in the beginning helped me understand the main ideas of the foundational articles, was the blog post <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are diffusion models? Lil’Log by Lilian Weng (2021)</a>.</p><p>In a subsequent work, <a href="https://openreview.net/forum?id=St1giarCHLP">Song, Meng, Ermon (2021)</a> improved the idea and introduced <strong>denoising diffusion implicit models</strong>, which is based on non-Markovian processes but amounts to the same training strategy and model. The advantage, then, is that sampling is greatly expedited with a non-Markovian reverse process that can sample directly from the tractable distribution. We do not detail this method here, though.</p><h2 id="Details-of-the-method"><a class="docs-heading-anchor" href="#Details-of-the-method">Details of the method</a><a id="Details-of-the-method-1"></a><a class="docs-heading-anchor-permalink" href="#Details-of-the-method" title="Permalink"></a></h2><p>We consider the unknown target distribution to be an initial distribution for a Markov process, so we denote the associated random variable by <span>$\mathbf{X}_0$</span> in <span>$\mathbb{R}^d,$</span> <span>$d\in\mathbb{N},$</span> and its probability density function by <span>$p_0(\mathbf{x}).$</span> The sample points used for training are denoted by <span>$\{\mathbf{x}_0^n\}_{n=1}^N.$</span></p><p>The idea is to define a forward Markov process that gradually adds noise to the process and drives the distribution towards a tractable distribution such as a standard Gaussian or binomial distribution. This forward process is pretty well defined and straightforward to implement. From the initial sample points we obtain samples of the Markov process. The idea then is to use those sample trajectories to learn the reverse Markov process. With this model of the reverse process, we can build new sample points out of (new) samples of the standard Gaussian distribution, gradually <em>denoising</em> and <em>noisy</em> sample from the tractable distribution towards a sample of the desired distribution.</p><p>In the case we consider here and as used in image generation, we add Gaussian noise and drive the system towards the standard Gaussian distribution. Since the reverse process starts with the standard Gaussian distribution and the process just adds Gaussian noises, the approximate reverse process is a Gaussian process. Thus we can just parametrized it by its (time-dependent) mean and variance. For the sake of simplicity of the training process, the variance is actually pre-defined, so we only train the mean of each reverse step. In turn, the mean is reparametrized and what is in fact learned is the noise that needs to be removed to go back to the sample space, granting the name <em>denoising diffusion</em> method. As it turns out, this noise is linked to the score function.</p><h3 id="The-forward-Markov-chain"><a class="docs-heading-anchor" href="#The-forward-Markov-chain">The forward Markov chain</a><a id="The-forward-Markov-chain-1"></a><a class="docs-heading-anchor-permalink" href="#The-forward-Markov-chain" title="Permalink"></a></h3><p>The initial random variable <span>$\mathbf{X}_0$</span> is evolved in time as a Markov chain <span>$\{\mathbf{X}_k\}_k$</span> according to</p><p class="math-container">\[    \mathbf{X}_k = \sqrt{1 - \beta_k}\mathbf{X}_{k-1} + \sqrt{\beta_k}\mathbf{Z}_k, \quad \mathbf{Z}_k \sim \mathcal{N}(\mathbf{0}, \mathbf{I}),\]</p><p>where <span>$\boldsymbol{\beta}=\{\beta_k\}_{k\in \mathbb{N}}$</span> is given, with <span>$0 &lt; \beta_k &lt; 1,$</span> for every <span>$k.$</span> In practice, we stop at a sufficiently large step <span>$K\in\mathbb{N},$</span> so that <span>$k=1, 2, \ldots, K.$</span></p><p>The marginal probability density function of the step <span>$k$</span> conditioned on the step <span>$k-1$</span> is</p><p class="math-container">\[    p(\mathbf{x}_k|\mathbf{x}_{k-1}) = \mathcal{N}\left(\mathbf{x}; \sqrt{1 - \beta_k}\mathbf{x}_{k-1}, \beta_k\mathbf{I}\right),\]</p><p>where <span>$\mathcal{N}(\mathbf{x}; \boldsymbol{\mu}, \beta\mathbf{I})$</span> is the Gaussian kernel</p><p class="math-container">\[    \mathcal{N}(\mathbf{x}; \boldsymbol{\mu}, \beta\mathbf{I}) = \frac{1}{\sqrt{2\pi\beta^d}}e^{-\frac{1}{2}\frac{\|\mathbf{x} - \boldsymbol{\mu}\|^2}{\beta}},\]</p><p>with <span>$\mathcal{N}(\mathbf{x}; \mathbf{0}, \mathbf{I})$</span> being the standard Gaussian kernel.</p><p>By taking the expectation of the recurrence relation of the Markov chain, we see that the means <span>${\overline{\mathbf{X}}}_k = \mathbb{E}[\mathbf{X}_k]$</span> evolve according to</p><p class="math-container">\[    {\overline{\mathbf{X}}}_k = \sqrt{1 - \beta_k}{\overline{\mathbf{X}}}_{k-1},\]</p><p>With <span>$0 &lt; \beta_k &lt; 1,$</span> we see that the mean value decays to zero exponentially,</p><p class="math-container">\[    {\overline{\mathbf{X}}}_k \rightarrow 0, \qquad k \rightarrow \infty.\]</p><p>Notice that</p><p class="math-container">\[    {\overline{\mathbf{X}}}_k^2 = (1 - \beta_k){\overline{\mathbf{X}}}_{k-1}^2,\]</p><p>and</p><p class="math-container">\[    \mathbf{X}_k - {\overline{\mathbf{X}}}_k = \sqrt{1 - \beta_k}\left(\mathbf{X}_{k-1} - {\overline{\mathbf{X}}}_{k-1}\right) + \sqrt{\beta_k}\mathbf{Z}_k.\]</p><p>Thus,</p><p class="math-container">\[    \left\|\mathbf{X}_k - {\overline{\mathbf{X}}}_k\right\|^2 = (1 - \beta_k)\left\|\mathbf{X}_{k-1} - {\overline{\mathbf{X}}}_{k-1}\right\|^2 + 2\sqrt{1 - \beta_k}\sqrt{\beta_k}\left(\mathbf{X}_{k-1} - {\overline{\mathbf{X}}}_{k-1}\right)\mathbf{Z}_k + \beta_k \mathbf{Z}_k^2.\]</p><p>Taking the expectation, we find that</p><p class="math-container">\[    \mathbb{E}\left[ \left\|\mathbf{X}_k - {\overline{\mathbf{X}}}_k\right\|^2 \right] = (1 - \beta_k) \mathbb{E}\left[ \left\|\mathbf{X}_{k-1} - {\overline{\mathbf{X}}}_{k-1}\right\|^2 \right] + \beta_k.\]</p><p>This is precisely a recurrence relation for the variance,</p><p class="math-container">\[    \operatorname{Var}(\mathbf{X}_k) = (1 - \beta_k)\operatorname{Var}(\mathbf{X}_{k-1}) + \beta_k.\]</p><p>The parameters <span>$\boldsymbol{\beta}=\{\beta_k\}_k$</span> are said to be a <strong>variance schedule</strong>.</p><p>At the limit <span>$k\rightarrow \infty,$</span> with <span>$0 &lt; \beta_k &lt; 1,$</span> we see that the variance converges exponentially to one,</p><p class="math-container">\[    \operatorname{Var}(\mathbf{X}_k) \rightarrow 1, \qquad k \rightarrow \infty.\]</p><p>Thus, <span>$\{\mathbf{X}_k\}_k$</span> converges to the standard Gaussian distribution.</p><p>We can also write</p><p class="math-container">\[    \operatorname{Var}(\mathbf{X}_k) - \operatorname{Var}(\mathbf{X}_{k-1}) = - \beta_k\operatorname{Var}(\mathbf{X}_{k-1}) + \beta_k\]</p><p>and</p><p class="math-container">\[    \frac{\operatorname{Var}(\mathbf{X}_k) - \operatorname{Var}(\mathbf{X}_{k-1})}{\beta_k} = -\operatorname{Var}(\mathbf{X}_{k-1}) + 1,\]</p><p>so that the variance schedule <span>$\boldsymbol{\beta}=\{\beta_k\}_k$</span> is also interpreted as <strong>step sizes</strong>.</p><p>The probability density function <span>$p(\mathbf{x}_{0:K}) = p(\mathbf{x}_0, \ldots, \mathbf{x}_K)$</span> of the Markov chain, where <span>$\mathbf{x}_{0:K} = (\mathbf{x}_0, \dots, \mathbf{x}_K)$</span> is the portion of a trajectory up to step <span>$K,$</span> satisfies the conditional marginal relation</p><p class="math-container">\[    p(\mathbf{x}_k|\mathbf{x}_{k-1}) \sim \mathcal{N}(\sqrt{1 - \beta_k}\mathbf{x}_{k-1}, \beta_k),\]</p><p>Then,</p><p class="math-container">\[    p(\mathbf{x}_{0:K}|\mathbf{x}_0) = p(\mathbf{x}_{K}|\mathbf{x}_{K-1})\cdots p(\mathbf{x}_1|\mathbf{x}_0) = \prod_{k=1}^{K}p(\mathbf{x}_k|\mathbf{x}_{k-1}).\]</p><p>Thus,</p><p class="math-container">\[    p(\mathbf{x}_{0:K}) = \int_{\mathbb{R}^d} \prod_{k=1}^K p(\mathbf{x}_k|\mathbf{x}_{k-1})p_0(\mathbf{x}_0)\;\mathrm{d}\mathbf{x}_0.\]</p><p>An approximate distribution for the Markov chain is obtained with the empirical distribution based on samples of the initial random variable <span>$\mathbf{X}_0,$</span> but we will not use this explicitly.</p><p>We can iterate the Markov transition formula and write</p><p class="math-container">\[    \begin{align*}
        \mathbf{X}_k &amp; = \sqrt{1 - \beta_{k}}\mathbf{X}_{k-1} + \sqrt{\beta_{k}}\mathbf{Z}_{k} \\
        &amp; = \sqrt{1 - \beta_{k}}\left( \sqrt{1 - \beta_{k-1}}\mathbf{X}_{k-2} + \sqrt{\beta_{k-1}}\mathbf{Z}_{k-1} \right) + \sqrt{\beta_{k}}\mathbf{Z}_{k} \\
        &amp; = \sqrt{1 - \beta_{k}}\sqrt{1 - \beta_{k-1}}\mathbf{X}_{k-2} + \sqrt{1 - \beta_{k}}\sqrt{\beta_{k-1}}\mathbf{Z}_{k-1} + \sqrt{\beta_{k}}\mathbf{Z}_{k} \\
        &amp; = \sqrt{1 - \beta_{k}}\sqrt{1 - \beta_{k-1}}\left( \sqrt{1 - \beta_{k-2}}\mathbf{X}_{k-3} + \sqrt{\beta_{k-2}}\mathbf{Z}_{k-2} \right) + \sqrt{1 - \beta_{k}}\sqrt{\beta_{k-1}}\mathbf{Z}_{k-1} + \sqrt{\beta_{k}}\mathbf{Z}_{k} \\
        &amp; = \cdots \\
        &amp; = \sqrt{1 - \beta_{k}} \cdots \sqrt{1 - \beta_1}\mathbf{X}_0 + \sqrt{1 - \beta_{k}}\cdots \sqrt{1 - \beta_1}\mathbf{Z}_1 + \cdots + \sqrt{1 - \beta_{k}}\sqrt{\beta_{k-1}}\mathbf{Z}_{k-1} + \sqrt{\beta_{k}}\mathbf{Z}_{k}.
    \end{align*}\]</p><p>By defining</p><p class="math-container">\[    \alpha_k = 1 - \beta_k,\]</p><p>we rewrite this as</p><p class="math-container">\[    \mathbf{X}_k = \sqrt{\alpha_{k}\cdots \alpha_1}\mathbf{X}_0 + \sqrt{\alpha_{k}\cdots\alpha_2}\sqrt{1 - \alpha_1}\mathbf{Z}_1 + \cdots + \sqrt{\alpha_{k}}\sqrt{1 - \alpha_{k-1}}\mathbf{Z}_{k-1} + \sqrt{1 - \alpha_{k}}\mathbf{Z}_{k}.\]</p><p>Since the <span>$\mathbf{Z}_k$</span> are standard Gaussian random variables, thus with zero mean and variance one, their linear combination is also a Gaussian with zero mean, while the variance is given by the sum of the variances, which ends up simplifying to</p><p class="math-container">\[\alpha_{k}\cdots\alpha_2 (1 - \alpha_1) + \cdots + \alpha_{k}(1 - \alpha_{k-1}) + (1 - \alpha_{k}) = 1 - \alpha_{k}\cdots \alpha_1.\]</p><p>Defining now</p><p class="math-container">\[    \bar\alpha_k = \alpha_k\cdots\alpha_1,\]</p><p>we obtain</p><p class="math-container">\[    \mathbf{X}_k = \sqrt{\bar{\alpha}_{k}}\mathbf{X}_0 + \sqrt{1 - \bar{\alpha}_{k}}\bar{\mathbf{Z}}_k, \qquad \bar{\mathbf{Z}}_k \sim \mathcal{N}(\mathbf{0}, \mathbf{I}).\]</p><p>In other words, this means we can write</p><p class="math-container">\[    p(\mathbf{x}_k|\mathbf{x}_0) = \mathcal{N}(\mathbf{x}_k; \sqrt{\bar{\alpha}_{k}}\mathbf{x}_0, 1 - \bar{\alpha}_{k}).\]</p><h3 id="The-backward-Markov-chain"><a class="docs-heading-anchor" href="#The-backward-Markov-chain">The backward Markov chain</a><a id="The-backward-Markov-chain-1"></a><a class="docs-heading-anchor-permalink" href="#The-backward-Markov-chain" title="Permalink"></a></h3><p>Now we want to be able to revert the Markov chain. But what would be <span>$\mathbf{X}_{k-1}$</span> given <span>$\mathbf{X}_k = \mathbf{x}_k?$</span> It turns out that we can also condition it on <span>$\mathbf{X}_0,$</span> for training, and this leads to a tractable distribution.</p><p>In fact, when conditioned on the initial sample, we can use Bayes&#39; rule and write</p><p class="math-container">\[    p\left(\mathbf{x}_{k-1}|\mathbf{x}_k, \mathbf{x}_0\right) = \frac{p\left(\mathbf{x}_k|\mathbf{x}_{k-1}, \mathbf{x}_0\right)p\left(\mathbf{x}_{k-1}|\mathbf{x}_0\right)}{p\left(\mathbf{x}_k|\mathbf{x}_0\right)}\]</p><p>Using the Markovian property on the first term of the nominator and ignoring the normalization constant, we know that</p><p class="math-container">\[    p\left(\mathbf{x}_k|\mathbf{x}_{k-1}, \mathbf{x}_0\right) = p\left(\mathbf{x}_k|\mathbf{x}_{k-1}\right) \propto \exp\left(-\frac{1}{2}\frac{\left(\mathbf{x}_k - \sqrt{\alpha_k}\mathbf{x}_{k-1}\right)^2}{\beta_k}\right),\]</p><p>while</p><p class="math-container">\[    p\left(\mathbf{x}_{k-1}|\mathbf{x}_0\right) \propto \exp\left(-\frac{1}{2}\frac{\left(\mathbf{x}_{k-1} - \sqrt{\bar{\alpha}_{k-1}}\mathbf{x}_0\right)^2}{1 - \bar{\alpha}_{k-1}}\right),\]</p><p>and</p><p class="math-container">\[    p\left(\mathbf{x}_k|\mathbf{x}_0\right) \propto \exp\left(-\frac{1}{2}\frac{\left(\mathbf{x}_k - \sqrt{\bar{\alpha}_k}\mathbf{x}_0\right)^2}{1 - \bar{\alpha}_k}\right).\]</p><p>Thus,</p><p class="math-container">\[    p\left(\mathbf{x}_k|\mathbf{x}_{k-1}, \mathbf{x}_0\right) \propto \exp\left( - \frac{1}{2}\left(\frac{\left(\mathbf{x}_k - \sqrt{\alpha_k}\mathbf{x}_{k-1}\right)^2}{\beta_k} + \frac{\left(\mathbf{x}_{k-1} - \sqrt{\bar{\alpha}_{k-1}}\mathbf{x}_0\right)^2}{1 - \bar{\alpha}_{k-1}} - \frac{\left(\mathbf{x}_k - \sqrt{\bar{\alpha}_k}\mathbf{x}_0\right)^2}{1 - \bar{\alpha}_k} \right)\right).\]</p><p>We separate the dependence on the variable <span>$\mathbf{x}_{k-1}$</span> from that on the conditioned variables <span>$\mathbf{x}_k$</span> and <span>$\mathbf{x}_0.$</span></p><p class="math-container">\[    \begin{align*}
        p\left(\mathbf{x}_k|\mathbf{x}_{k-1}, \mathbf{x}_0\right) &amp; \propto \exp\bigg( - \frac{1}{2}\bigg(\frac{\mathbf{x}_k^2 - 2\mathbf{x}_k \sqrt{\alpha_k}\mathbf{x}_{k-1} + \alpha_k\mathbf{x}_{k-1}^2}{\beta_k} \\
        &amp; \qquad \qquad \qquad + \frac{\mathbf{x}_{k-1}^2 - 2\mathbf{x}_{k-1}\sqrt{\bar{\alpha}_{k-1}}\mathbf{x}_0 + \bar{\alpha}_{k-1}\mathbf{x}_0^2}{1 - \bar{\alpha}_{k-1}} \\
        &amp; \qquad \qquad \qquad \qquad \qquad - \frac{\mathbf{x}_k^2 - \mathbf{x}_k\sqrt{\bar{\alpha}_k}\mathbf{x}_0 + \bar{\alpha}_k\mathbf{x}_0^2}{1 - \bar{\alpha}_k} \bigg)\bigg) \\
        &amp; = \exp\bigg( -\frac{1}{2}\bigg( \left( \frac{\alpha_k}{\beta_k} + \frac{1}{1 - \bar{\alpha}_{k-1}}\right)\mathbf{x}_{k-1}^2 \\
        &amp; \qquad \qquad \qquad - \left(\frac{2\sqrt{\alpha_k}}{\beta_k}\mathbf{x}_k + \frac{2\sqrt{\bar{\alpha}_{k-1}}}{1 - \bar{\alpha}_{k-1}}\mathbf{x}_0\right)\mathbf{x}_{k-1} \\
        &amp; \qquad \qquad \qquad + \left( \frac{1}{\beta_k} - \frac{1}{1 - \bar{\alpha}_k}\right)\mathbf{x}_{k}^2 + \frac{\sqrt{\bar{\alpha}_k}}{1 - \bar{\alpha}_k}\mathbf{x}_k\mathbf{x}_0 \\
        &amp; \qquad \qquad \qquad \qquad \qquad + \left( \frac{\bar{\alpha}_{k-1}}{1 - \bar{\alpha}_{k-1}} - \frac{\bar{\alpha}_k}{1 - \bar{\alpha}_k} \right)\mathbf{x}_0^2\bigg)\bigg).
    \end{align*}\]</p><p>Completing the squares, we write</p><p class="math-container">\[    \left( \frac{\alpha_k}{\beta_k} + \frac{1}{1 - \bar{\alpha}_{k-1}}\right)\mathbf{x}_{k-1}^2 - \left(\frac{2\sqrt{\alpha_k}}{\beta_k}\mathbf{x}_k + \frac{2\sqrt{\bar{\alpha}_{k-1}}}{1 - \bar{\alpha}_{k-1}}\mathbf{x}_0\right)\mathbf{x}_{k-1} = \frac{\left(\mathbf{x}_{k-1} - \tilde{\boldsymbol{\mu}}_k\right)^2}{\tilde{\beta}_k} - \frac{{\tilde{\boldsymbol{\mu}}}_k^2}{\tilde \beta_k},\]</p><p>with</p><p class="math-container">\[    \begin{align*}
        \tilde\beta_k &amp; = \frac{1}{\left( \frac{\alpha_k}{\beta_k} + \frac{1}{1 - \bar{\alpha}_{k-1}}\right)}, \\
        \frac{\tilde{\boldsymbol{\mu}}_k}{\tilde\beta_k} &amp; = \frac{\sqrt{\alpha_k}}{\beta_k}\mathbf{x}_k + \frac{\sqrt{\bar{\alpha}_{k-1}}}{1 - \bar{\alpha}_{k-1}}\mathbf{x}_0.
    \end{align*}\]</p><p>Using that <span>$\beta_k = 1 - \alpha_k,$</span> we find the variance of <span>$p\left(\mathbf{x}_k|\mathbf{x}_{k-1}, \mathbf{x}_0\right)$</span> to be</p><p class="math-container">\[    \tilde\beta_k = \frac{1}{\left( \frac{\alpha_k}{\beta_k} + \frac{1}{1 - \bar{\alpha}_{k-1}}\right)} = \frac{\beta_k(1 - \bar{\alpha}_{k-1})}{\alpha_k(1 - \bar{\alpha}_{k-1}) + \beta_k} = \frac{1 - \bar{\alpha}_{k-1}}{1 - \bar{\alpha}_k}\beta_k,\]</p><p>With that, we rewrite the mean of <span>$p\left(\mathbf{x}_k|\mathbf{x}_{k-1}, \mathbf{x}_0\right)$</span> as</p><p class="math-container">\[    \begin{align*}
        \tilde{\boldsymbol{\mu}}_k &amp; = \tilde\beta_k\left(\frac{\sqrt{\alpha_k}}{\beta_k}\mathbf{x}_k + \frac{\sqrt{\bar{\alpha}_{k-1}}}{1 - \bar{\alpha}_{k-1}}\mathbf{x}_0\right) = \frac{1 - \bar{\alpha}_{k-1}}{1 - \bar{\alpha}_k}\beta_k\left(\frac{\sqrt{\alpha_k}}{\beta_k}\mathbf{x}_k + \frac{\sqrt{\bar{\alpha}_{k-1}}}{1 - \bar{\alpha}_{k-1}}\mathbf{x}_0\right) \\
        &amp; = \frac{(1 - \bar{\alpha}_{k-1})\sqrt{\alpha_k}}{1 - \bar{\alpha}_k}\mathbf{x}_k + \frac{\beta_k\sqrt{\bar{\alpha}_{k-1}}}{1 - \bar{\alpha}_k}\mathbf{x}_0.
    \end{align*}\]</p><p>Then, we obtain</p><p class="math-container">\[    \begin{align*}
        \frac{{\tilde{\boldsymbol{\mu}}}_k^2}{\tilde \beta_k} &amp; = \frac{1 - \bar{\alpha}_k}{(1 - \bar{\alpha}_{k-1})\beta_k}\left(\frac{(1 - \bar{\alpha}_{k-1})\sqrt{\alpha_k}}{1 - \bar{\alpha}_k}\mathbf{x}_k + \frac{\beta_k\sqrt{\bar{\alpha}_{k-1}}}{1 - \bar{\alpha}_k}\mathbf{x}_0\right) \\
        &amp; = \frac{\sqrt{\alpha_k}}{\beta_k}\mathbf{x}_k + \frac{\sqrt{\bar{\alpha}_{k-1}}}{(1 - \bar{\alpha}_{k-1})}\mathbf{x}_0.
    \end{align*}\]</p><p>Thus, we write</p><p class="math-container">\[    p\left(\mathbf{x}_k|\mathbf{x}_{k-1}, \mathbf{x}_0\right) \propto \exp\bigg( -\frac{1}{2}\bigg( \frac{\left(\mathbf{x}_{k-1} - \tilde{\boldsymbol{\mu}}_k\right)^2}{\tilde{\beta}_k} + \tilde\gamma_k \bigg)\bigg),\]</p><p>where</p><p class="math-container">\[    \begin{align*}
        \tilde{\boldsymbol{\mu}}_k &amp; = \tilde{\boldsymbol{\mu}}_k(\mathbf{x}_k, \mathbf{x}_0) = \frac{(1 - \bar{\alpha}_{k-1})\sqrt{\alpha_k}}{1 - \bar{\alpha}_k}\mathbf{x}_k + \frac{\beta_k\sqrt{\bar{\alpha}_{k-1}}}{1 - \bar{\alpha}_k}\mathbf{x}_0, \\
        \tilde\beta_k &amp; = \frac{1 - \bar{\alpha}_{k-1}}{1 - \bar{\alpha}_k}\beta_k, \\
        \tilde\gamma_k &amp; = \tilde\gamma_k(\mathbf{x}_k, \mathbf{x}_0) = \left( \frac{1}{\beta_k} - \frac{1}{1 - \bar{\alpha}_k}\right)\mathbf{x}_{k}^2 + \frac{\sqrt{\bar{\alpha}_k}}{1 - \bar{\alpha}_k}\mathbf{x}_k\mathbf{x}_0 \\
        &amp; \qquad \qquad \qquad \qquad + \left( \frac{\bar{\alpha}_{k-1}}{1 - \bar{\alpha}_{k-1}} - \frac{\bar{\alpha}_k}{1 - \bar{\alpha}_k} \right)\mathbf{x}_0^2 - \frac{{\tilde{\boldsymbol{\mu}}}_k^2}{\tilde \beta_k}.
    \end{align*}\]</p><p>Hence, we find that</p><p class="math-container">\[    p\left(\mathbf{x}_k|\mathbf{x}_{k-1}, \mathbf{x}_0\right) = \mathcal{N}\left(\mathbf{x}_k; \tilde{\boldsymbol{\mu}}_k, \tilde \beta_k\mathbf{I}\right).\]</p><p>With that, we can write</p><p class="math-container">\[    p\left(\mathbf{x}_k|\mathbf{x}_{k-1}\right) = \int_{\mathbb{R}^d} p\left(\mathbf{x}_k|\mathbf{x}_{k-1},\mathbf{x}_0\right)p(\mathbf{x}_0)\;\mathrm{d}\mathbf{x}_0.\]</p><p>Notice we can write the (initial) target distribution as</p><p class="math-container">\[    p_0(\mathbf{x}_0) = p(\mathbf{x}_0) = \int_{(\mathbf{R}^d)^{K}} p(\mathbf{x}_{0:K}) \;\mathrm{d}\mathbf{x}_{1:K},\]</p><p>and then</p><p class="math-container">\[    p(\mathbf{x}_{0:K}) = \int_{\mathbb{R}^d} p(\mathbf{x}_{0:K}|\mathbf{x}_0)\;\mathrm{d}\mathbf{x}_0,\]</p><p>with</p><p class="math-container">\[    p(\mathbf{x}_{0:K}|\mathbf{x}_0) = p(\mathbf{x}_0|\mathbf{x}_1, \mathbf{x}_0)p(\mathbf{x}_1|\mathbf{x}_2, \mathbf{x}_0)\cdots p(\mathbf{x}_{K-1}|\mathbf{x}_K, \mathbf{x}_0),\]</p><p>and</p><p class="math-container">\[    p(\mathbf{x}_{k-1}|\mathbf{x}_k, \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{k-1}; \tilde{\boldsymbol{\mu}}_k(\mathbf{x}_k, \mathbf{x}_0), \tilde \beta_k\mathbf{I}).\]</p><p>Another way of writing this reverse process is through the recurrence relation</p><p class="math-container">\[    \mathbf{X}_{k-1} = \tilde{\boldsymbol{\mu}}_k(\mathbf{X}_k, \mathbf{X}_0) + \tilde\beta_k\mathbf{Z}_k, \quad \mathbf{Z}_k \sim \mathcal{N}(\mathbf{0}, \mathbf{I}).\]</p><h3 id="Reparametrization-trick"><a class="docs-heading-anchor" href="#Reparametrization-trick">Reparametrization trick</a><a id="Reparametrization-trick-1"></a><a class="docs-heading-anchor-permalink" href="#Reparametrization-trick" title="Permalink"></a></h3><p>As mentioned earlier, conditioning <span>$p(\mathbf{x}_{k-1}|\mathbf{x}_k, \mathbf{x}_0)$</span> on <span>$\mathbf{x}_0$</span> is good for training, but for not for sampling. Thus we reparametrize the relation and write <span>$\tilde{\boldsymbol{\mu}}_k(\mathbf{X}_k, \mathbf{X}_0)$</span> in terms of <span>$\mathbf{X}_k$</span> and the added noise term. Then we model the added noise, as we will see.</p><p>From the relation <span>$\mathbf{X}_k = \sqrt{\bar{\alpha}_{k}}\mathbf{X}_0 + \sqrt{1 - \bar{\alpha}_{k}}\bar{\mathbf{Z}}_k,$</span> where <span>$\bar{\mathbf{Z}}_k \sim \mathcal{N}(\mathbf{0}, \mathbf{I}),$</span> we find that</p><p class="math-container">\[    \mathbf{x}_k = \sqrt{\bar{\alpha}_{k}}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_{k}}\bar{\boldsymbol{\epsilon}}_k,\]</p><p>for a sample <span>$\boldsymbol{\epsilon}_k$</span> of the standard normal distribution. We use that to rewrite <span>$\tilde{\boldsymbol{\mu}}_k$</span> in terms of <span>$\mathbf{x}_0$</span> and <span>$\boldsymbol{\epsilon}_k,$</span></p><p class="math-container">\[    \begin{align*}
        \tilde{\boldsymbol{\mu}}_k &amp; = \frac{(1 - \bar{\alpha}_{k-1})\sqrt{\alpha_k}}{1 - \bar{\alpha}_k}\mathbf{x}_k + \frac{\beta_k\sqrt{\bar{\alpha}_{k-1}}}{1 - \bar{\alpha}_k}\mathbf{x}_0 \\
        &amp; = \frac{(1 - \bar{\alpha}_{k-1})\sqrt{\alpha_k}}{1 - \bar{\alpha}_k}\left(\sqrt{\bar{\alpha}_{k}}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_{k}}\bar{\boldsymbol{\epsilon}}_k\right) + \frac{\beta_k\sqrt{\bar{\alpha}_{k-1}}}{1 - \bar{\alpha}_k}\mathbf{x}_0 \\
        &amp; = \frac{(1 - \bar{\alpha}_{k-1})\sqrt{\alpha_k}\sqrt{\bar{\alpha}_{k}} + \beta_k\sqrt{\bar{\alpha}_{k-1}}}{1 - \bar{\alpha}_k}\mathbf{x}_0 + \frac{(1 - \bar{\alpha}_{k-1})\sqrt{\alpha_k}}{1 - \bar{\alpha}_k}\sqrt{1 - \bar{\alpha}_{k}}\bar{\boldsymbol{\epsilon}}_k \\
        &amp; = \frac{((1 - \bar{\alpha}_{k-1})\alpha_k + \beta_k)\sqrt{\bar{\alpha}_{k-1}}}{1 - \bar{\alpha}_k}\mathbf{x}_0 + \frac{(1 - \bar{\alpha}_{k-1})\alpha_k}{\sqrt{1 - \bar{\alpha}_k}\sqrt{\alpha_k}}\bar{\boldsymbol{\epsilon}}_k \\
        &amp; = \frac{(\alpha_k - \bar{\alpha}_k + \beta_k)\sqrt{\bar{\alpha}_{k-1}}}{1 - \bar{\alpha}_k}\mathbf{x}_0 + \frac{(1 - \bar{\alpha}_{k-1})\alpha_k}{\sqrt{1 - \bar{\alpha}_k}\sqrt{\alpha_k}}\bar{\boldsymbol{\epsilon}}_k \\
    \end{align*}\]</p><p>Using that <span>$\beta_k = 1 - \alpha_k,$</span> we find</p><p class="math-container">\[    \begin{align*}
        \tilde{\boldsymbol{\mu}}_k &amp; = \frac{(1 - \bar{\alpha}_k)\sqrt{\bar{\alpha}_{k-1}}}{1 - \bar{\alpha}_k}\mathbf{x}_0 + \frac{\alpha_k - \bar{\alpha}_k}{(1 - \bar{\alpha}_k)\sqrt{\alpha_k}}\sqrt{1 - \bar{\alpha}_{k}}\bar{\boldsymbol{\epsilon}}_k \\
        &amp; = \sqrt{\bar{\alpha}_{k-1}}\mathbf{x}_0 + \frac{\alpha_k - \bar{\alpha}_k}{(1 - \bar{\alpha}_k)\sqrt{\alpha_k}}\sqrt{1 - \bar{\alpha}_{k}}\bar{\boldsymbol{\epsilon}}_k \\
    \end{align*}\]</p><p>We can also rewrite <span>$\mathbf{x}_0$</span> in terms of <span>$\mathbf{x}_k,$</span> i.e.</p><p class="math-container">\[    \mathbf{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_{k}}}\mathbf{x}_k - \frac{\sqrt{1 - \bar{\alpha}_{k}}}{\sqrt{\bar{\alpha}_{k}}}\bar{\boldsymbol{\epsilon}}_k.\]</p><p>Plugging this into the formula for the mean <span>$\tilde{\boldsymbol{\mu}}_k,$</span> we obtain</p><p class="math-container">\[    \begin{align*}
        \tilde{\boldsymbol{\mu}}_k &amp; = \frac{(1 - \bar{\alpha}_{k-1})\sqrt{\alpha_k}}{1 - \bar{\alpha}_k}\mathbf{x}_k + \frac{\beta_k\sqrt{\bar{\alpha}_{k-1}}}{1 - \bar{\alpha}_k}\left(\frac{1}{\sqrt{\bar{\alpha}_{k}}}\mathbf{x}_k - \frac{\sqrt{1 - \bar{\alpha}_{k}}}{\sqrt{\bar{\alpha}_{k}}}\bar{\boldsymbol{\epsilon}}_k\right) \\
        &amp; = \left(\frac{(1 - \bar{\alpha}_{k-1})\sqrt{\alpha_k}}{1 - \bar{\alpha}_k} + \frac{\beta_k\sqrt{\bar{\alpha}_{k-1}}}{(1 - \bar{\alpha}_k)\sqrt{\bar\alpha_k}}\right)\mathbf{x}_k - \frac{\beta_k\sqrt{\bar{\alpha}_{k-1}}}{1 - \bar{\alpha}_k}\frac{\sqrt{1 - \bar{\alpha}_{k}}}{\sqrt{\bar{\alpha}_{k}}}\bar{\boldsymbol{\epsilon}}_k \\
        &amp; = \left(\frac{(1 - \bar{\alpha}_{k-1})\sqrt{\alpha_k}}{1 - \bar{\alpha}_k} + \frac{\beta_k}{(1 - \bar{\alpha}_k)\sqrt{\alpha_k}}\right)\mathbf{x}_k - \frac{\beta_k}{\sqrt{1 - \bar{\alpha}_{k}}\sqrt{\alpha_{k}}}\bar{\boldsymbol{\epsilon}}_k \\
        &amp; = \left(\frac{(1 - \bar{\alpha}_{k-1})\alpha_k + \beta_k}{(1 - \bar{\alpha}_k)\sqrt{\alpha_k}}\right)\mathbf{x}_k - \frac{\beta_k}{\sqrt{1 - \bar{\alpha}_{k}}\sqrt{\alpha_{k}}}\bar{\boldsymbol{\epsilon}}_k \\
        &amp; = \left(\frac{\alpha_k - \bar{\alpha}_k + \beta_k}{(1 - \bar{\alpha}_k)\sqrt{\alpha_k}}\right)\mathbf{x}_k - \frac{\beta_k}{\sqrt{1 - \bar{\alpha}_{k}}\sqrt{\alpha_{k}}}\bar{\boldsymbol{\epsilon}}_k
    \end{align*}\]</p><p>Using, again, that <span>$\beta_k = 1 - \alpha_k,$</span> we find</p><p class="math-container">\[    \tilde{\boldsymbol{\mu}}_k = \frac{1}{\sqrt{\alpha_k}}\mathbf{x}_k - \frac{1-\alpha_k}{\sqrt{1 - \bar{\alpha}_{k}}\sqrt{\alpha_{k}}}\bar{\boldsymbol{\epsilon}}_k.\]</p><p>In this way, we <em>reparametrize</em> the mean in terms of <span>$\mathbf{x}_k$</span> and <span>$\bar{\boldsymbol{\epsilon}}_k,$</span> instead of <span>$\mathbf{x}_k$</span> and <span>$\mathbf{x}_0.$</span> This is the form that serves as an inspiration to the model of the reverse process. With that, the reparametrization of <span>$\mathbf{x}_k$</span> in terms of <span>$\mathbf{x}_0$</span> and <span>$\bar{\boldsymbol{\epsilon}}_k$</span> is then used for training, as we will see in what follows.</p><h3 id="The-model"><a class="docs-heading-anchor" href="#The-model">The model</a><a id="The-model-1"></a><a class="docs-heading-anchor-permalink" href="#The-model" title="Permalink"></a></h3><p>We want to approximate the distribution of the Markov process with some model pdf <span>$p_{\boldsymbol{\theta}}(\mathbf{x}_{0:K}),$</span> which yields an approximation of the target pdf <span>$p_0(\mathbf{x}_0)$</span> via</p><p class="math-container">\[    p_{\boldsymbol{\theta}}(\mathbf{x}_{0}) = \int_{(\mathbf{R}^d)^{K}} p_{\boldsymbol{\theta}}(\mathbf{x}_{0:K}) \;\mathrm{d}\mathbf{x}_{1:K}.\]</p><p>One of the key points in the forward Markov chain is  that the limit distribution of <span>$\mathbf{X}_k$</span> as <span>$k\rightarrow \infty$</span> is a standard normal distribution. Thus, for our model, we assume that the distribution at step <span>$K,$</span> with <span>$K$</span> taken relatively large, is precisely a standard normal distribution. With that, the model is written as</p><p class="math-container">\[    p_{\boldsymbol{\theta}}(\mathbf{x}_{0:K}) = \int_{\mathbb{R}^d} p_{\boldsymbol{\theta}}(\mathbf{x}_{0:K}|\mathbf{x}_K)p_{\boldsymbol{\theta}}(\mathbf{x}_K)\;\mathrm{d}\mathbf{x}_K,\]</p><p>with</p><p class="math-container">\[    p_{\boldsymbol{\theta}}(\mathbf{x}_K) = \mathcal{N}(\mathbf{x}_K; \mathbf{0}, \mathbf{I}).\]</p><p>We also have</p><p class="math-container">\[    p_{\boldsymbol{\theta}}(\mathbf{x}_{0:K}|\mathbf{x}_K) = p_{\boldsymbol{\theta}}(\mathbf{x}_0|\mathbf{x}_1)p_{\boldsymbol{\theta}}(\mathbf{x}_1|\mathbf{x}_2)\cdots p_{\boldsymbol{\theta}}(\mathbf{x}_{K-1}|\mathbf{x}_K).\]</p><p>As in the reverse Markov process, we assume each <span>$p_{\boldsymbol{\theta}}(\mathbf{x}_{k-1}|\mathbf{x}_k)$</span> is a Gaussian distribution. Hence, each conditional distribution is parametrized by its mean and its variance,</p><p class="math-container">\[    p_{\boldsymbol{\theta}}(\mathbf{x}_{k-1}|\mathbf{x}_k) = \mathcal{N}(\mathbf{x}_{k-1}; \boldsymbol{\mu}_{\boldsymbol{\theta}}(\mathbf{x}_k, k), \beta_{\boldsymbol{\theta}}(\mathbf{x}_k, k)).\]</p><p><a href="https://dl.acm.org/doi/10.5555/3045118.3045358">Sohl-Dickstein, Weiss, Maheswaranathan, Ganguli (2015)</a> modeled the mean and the covariance kernel of each reverse step and actually used a modified distribution multiplying it by a function <span>$r(\mathbf{x}_k)$</span>, but we do not consider this and go on model implemented in <a href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html">Ho, Jain, and Abbeel (2020)</a>.</p><p>Indeed, due to the reparametrization trick used in the target Markov chain, we also reparametrize <span>$\boldsymbol{\mu}_{\boldsymbol{\theta}}(\mathbf{x}_k, k)$</span> in a similar way:</p><p class="math-container">\[    \boldsymbol{\mu}_{\boldsymbol{\theta}}(\mathbf{x}_k, k) = \frac{1}{\sqrt{\alpha_k}}\mathbf{x}_k - \frac{1-\alpha_k}{\sqrt{1 - \bar{\alpha}_{k}}\sqrt{\alpha_{k}}}\boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\mathbf{x}_k, k).\]</p><p>Although <span>$\beta_{\boldsymbol{\theta}}(\mathbf{x}_k, k)$</span> are also learnable, they are set to constants, for the sake of simplicity of the loss function:</p><p class="math-container">\[    \beta_{\boldsymbol{\theta}}(\mathbf{x}_k, k) = \sigma_k,\]</p><p>for pre-determined constants <span>$\sigma_k,$</span> <span>$k=1, \ldots, K.$</span> <a href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html">Ho, Jain, and Abbeel (2020)</a> experimented with some choices and found out that choosing <span>$\sigma_k^2 = \beta_k$</span> or <span>$\sigma_k^2 = \tilde\beta_k$</span> yields about the same results. They correspond to opposite extremes, with <span>$\sigma_k^2 = \beta_k$</span> being optimal for <span>$\mathbf{X}_0 \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$</span>, while <span>$\sigma_k^2 = \tilde\beta_k$</span> being optimal for a delta distribution <span>$\mathbf{X}_0 = \mathbf{x}_0$</span>, for a given deterministic <span>$\mathbf{x}_0$</span>.</p><p>In this way, the modeled reverse process, once <span>$\boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\mathbf{x}_k, k)$</span> is properly trained, is given by</p><p class="math-container">\[    \mathbf{X}_{k-1} = \frac{1}{\sqrt{\alpha_k}}\mathbf{X}_k - \frac{1-\alpha_k}{\sqrt{1 - \bar{\alpha}_{k}}\sqrt{\alpha_{k}}}\boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\mathbf{X}_k, k) + \sigma_k \mathbf{Z}_k, \qquad \mathbf{Z}_k \sim \mathcal{N}(\mathbf{0}, \mathbf{I}).\]</p><p>Actually, for the final step <span>$p_{\boldsymbol{\theta}}(\mathbf{x}_0|\mathbf{x}_1)$</span> of the reverse process,  <a href="https://dl.acm.org/doi/10.5555/3045118.3045358">Sohl-Dickstein, Weiss, Maheswaranathan, Ganguli (2015)</a> bases it on the first step of the forward trajectory, in order to &quot;remove the edge effect&quot; (see Appendix B.2):</p><p class="math-container">\[    p_{\boldsymbol{\theta}}(\mathbf{x}_0|\mathbf{x}_1) = p(\mathbf{x}_1, \mathbf{x}_0)\frac{\mathcal{N}(\mathbf{x}_0; \mathbf{0}, \mathbf{I})}{\mathcal{N}(\mathbf{x}_1; \mathbf{0}, \mathbf{I})}.\]</p><p>In <a href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html">Ho, Jain, and Abbeel (2020)</a>, however, this is setup differently, being truncated to the support of the original distribution, which is assumed to represent an image, with data in <span>$\{0, 1, \ldots, 255\}$</span> scaled to <span>$[-1, 1],$</span> i.e. each coordinate <span>$x_i,$</span> <span>$i=1, \ldots, d,$</span> in <span>$\{(a - 127.5) / 127.5; \;a=0, \ldots, 255\},$</span> so that</p><p class="math-container">\[    p_{\boldsymbol{\theta}}(\mathbf{x}_0|\mathbf{x}_1) = \prod_{i=1}^d \int_{\delta_-(x_{0i})}^{\delta_+(x_{0i})} \mathcal{N}(x_i; \mu_{\boldsymbol{\theta}, i}(\mathbf{x}_1, 1), \sigma_1^2)\; \mathbf{x}_i,\]</p><p>with</p><p class="math-container">\[    \delta_-(x_{0i}) = \begin{cases}
        -\infty, &amp; x = -1, \\
        x - 1/255, &amp; x &gt; -11,
    \end{cases}
    \qquad 
    \delta_+(x_{0i}) = \begin{cases}
        \infty, &amp; x = 1, \\
        x + 1/255, &amp; x &lt; 1.
    \end{cases}\]</p><p>In any case, our model is completely defined by <span>$\boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\mathbf{x}_k, k),$</span> <span>$k=1, \ldots, K,$</span> the parameters <span>$\sigma_1, \ldots, \sigma_K,$</span> and the (final) conditional distribution <span>$p_{\boldsymbol{\theta}}(\mathbf{x}_0|\mathbf{x}_1).$</span></p><h3 id="The-loss-function"><a class="docs-heading-anchor" href="#The-loss-function">The loss function</a><a id="The-loss-function-1"></a><a class="docs-heading-anchor-permalink" href="#The-loss-function" title="Permalink"></a></h3><p>Now we need a loss function to train the parametrization <span>$\boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\mathbf{x}_k, k)$</span> of our model.</p><h4 id="The-cross-entropy-loss"><a class="docs-heading-anchor" href="#The-cross-entropy-loss">The cross-entropy loss</a><a id="The-cross-entropy-loss-1"></a><a class="docs-heading-anchor-permalink" href="#The-cross-entropy-loss" title="Permalink"></a></h4><p>Ideally, one would maximize the (log-)likelyhood of the model, by minimizing the <strong>cross-entropy loss</strong> function</p><p class="math-container">\[    L_{\mathrm{CE}}(\boldsymbol{\theta}) = H(p_0, p_{\boldsymbol{\theta}}) = \mathbb{E}_{p_0}\left[-\log p_{\boldsymbol{\theta}}(\mathbf{x}_0)\right] = -\int_{\mathbb{R}^d} p_0(\mathbf{x}_0)\log p_{\boldsymbol{\theta}}(\mathbf{x}_0)\;\mathrm{d}\mathbf{x}_0 \approx -\frac{1}{N}\sum_{n=1}^N \log p_{\boldsymbol{\theta}}(\mathbf{x}_0^n).\]</p><p>But <span>$p_{\boldsymbol{\theta}}(\mathbf{x}_{0}),$</span> given as</p><p class="math-container">\[    p_{\boldsymbol{\theta}}(\mathbf{x}_{0}) = \int_{(\mathbf{R}^d)^{K}} p_{\boldsymbol{\theta}}(\mathbf{x}_{0:K}) \;\mathrm{d}\mathbf{x}_{1:K},\]</p><p>is <em>intractable</em>.</p><h4 id="The-variational-lower-bound-loss"><a class="docs-heading-anchor" href="#The-variational-lower-bound-loss">The variational lower bound loss</a><a id="The-variational-lower-bound-loss-1"></a><a class="docs-heading-anchor-permalink" href="#The-variational-lower-bound-loss" title="Permalink"></a></h4><p>We substitute for <span>$p_{\boldsymbol{\theta}}(\mathbf{x}_{0})$</span> and multiply and divide by <span>$p(\mathbf{x}_{1:K}|\mathbf{x}_0)$</span> to find </p><p class="math-container">\[    \begin{align*}
        L_{\mathrm{CE}}(\boldsymbol{\theta}) &amp; = \mathbb{E}_{p_0}\left[-\log p_{\boldsymbol{\theta}}(\mathbf{x}_0)\right] \\
        &amp; = -\int_{\mathbb{R}^d} p_0(\mathbf{x}_0)\log p_{\boldsymbol{\theta}}(\mathbf{x}_0)\;\mathrm{d}\mathbf{x}_0 \\
        &amp; = -\int_{\mathbb{R}^d} p_0(\mathbf{x}_0)\log \left(\int_{(\mathbf{R}^d)^{K}} p_{\boldsymbol{\theta}}(\mathbf{x}_{0:K}) \;\mathrm{d}\mathbf{x}_{1:K}\right)\mathrm{d}\mathbf{x}_0 \\
        &amp; = -\int_{\mathbb{R}^d} p_0(\mathbf{x}_0)\log \left(\int_{(\mathbf{R}^d)^{K}} p(\mathbf{x}_{1:K}|\mathbf{x}_0) \frac{p_{\boldsymbol{\theta}}(\mathbf{x}_{0:K})}{p(\mathbf{x}_{1:K}|\mathbf{x}_0)} \;\mathrm{d}\mathbf{x}_{1:K}\right)\mathrm{d}\mathbf{x}_0.
    \end{align*}\]</p><p>Now we use Jensen&#39;s inequality to obtain the following upper bound for the cross-entropy loss,</p><p class="math-container">\[    \begin{align*}
        L_{\mathrm{CE}}(\boldsymbol{\theta}) &amp; \leq -\int_{\mathbb{R}^d} p_0(\mathbf{x}_0)\int_{(\mathbf{R}^d)^{K}} p(\mathbf{x}_{1:K}|\mathbf{x}_0) \log \left(\frac{p_{\boldsymbol{\theta}}(\mathbf{x}_{0:K})}{p(\mathbf{x}_{1:K}|\mathbf{x}_0)} \right)\mathrm{d}\mathbf{x}_{1:K}\,\mathrm{d}\mathbf{x}_0 \\
        &amp; = -\int_{(\mathbf{R}^d)^{K+1}} p(\mathbf{x}_{0:K}) \log \left(\frac{p_{\boldsymbol{\theta}}(\mathbf{x}_{0:K})}{p(\mathbf{x}_{1:K}|\mathbf{x}_0)} \right)\mathrm{d}\mathbf{x}_{0:K} \\
        &amp; = \int_{(\mathbf{R}^d)^{K+1}} p(\mathbf{x}_{0:K}) \log \left(\frac{p(\mathbf{x}_{1:K}|\mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_{0:K})} \right)\mathrm{d}\mathbf{x}_{0:K}
    \end{align*}\]</p><p>This expression defines what is called the <strong>variational lower bound</strong> loss</p><p class="math-container">\[    L_{\mathrm{VLB}}(\boldsymbol{\theta}) = \mathbb{E}_{p(\mathbf{x}_{0:K})}\left[ \log \frac{p(\mathbf{x}_{1:K}|\mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_{0:K})} \right] = \int_{(\mathbf{R}^d)^{K+1}} p(\mathbf{x}_{0:K}) \log \left(\frac{p(\mathbf{x}_{1:K}|\mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_{0:K})} \right)\mathrm{d}\mathbf{x}_{0:K}.\]</p><p><a href="https://dl.acm.org/doi/10.5555/3045118.3045358">Sohl-Dickstein, Weiss, Maheswaranathan, Ganguli (2015)</a> manipulated this loss to a more tractable form as follows</p><p class="math-container">\[    \begin{align*}
        L_{\mathrm{VLB}}(\boldsymbol{\theta}) &amp; = \mathbb{E}_{p(\mathbf{x}_{0:K})}\left[ \log \frac{p(\mathbf{x}_{1:K}|\mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_{0:K})} \right] \\
        &amp; = \mathbb{E}_{p(\mathbf{x}_{0:K})}\left[ \log \frac{\prod_{k=1}^K p(\mathbf{x}_k|\mathbf{x}_{k-1})}{p_{\boldsymbol{\theta}}(\mathbf{x}_K)\prod_{k=1}^K p_{\boldsymbol{\theta}}(\mathbf{x}_{k-1}|\mathbf{x}_k)} \right] \\
        &amp; = \mathbb{E}_{p(\mathbf{x}_{0:K})}\left[ -\log p_{\boldsymbol{\theta}}(\mathbf{x}_K) + \log \prod_{k=1}^K \frac{p(\mathbf{x}_k|\mathbf{x}_{k-1})}{p_{\boldsymbol{\theta}}(\mathbf{x}_{k-1}|\mathbf{x}_k)} \right] \\
        &amp; = \mathbb{E}_{p(\mathbf{x}_{0:K})}\left[ -\log p_{\boldsymbol{\theta}}(\mathbf{x}_K) + \sum_{k=1}^K \log \frac{p(\mathbf{x}_k|\mathbf{x}_{k-1})}{p_{\boldsymbol{\theta}}(\mathbf{x}_{k-1}|\mathbf{x}_k)} \right] \\
        &amp; = \mathbb{E}_{p(\mathbf{x}_{0:K})}\left[ -\log p_{\boldsymbol{\theta}}(\mathbf{x}_K) + \log \frac{p(\mathbf{x}_1|\mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_0|\mathbf{x}_1)} + \sum_{k=2}^K \log \frac{p(\mathbf{x}_k|\mathbf{x}_{k-1})}{p_{\boldsymbol{\theta}}(\mathbf{x}_{k-1}|\mathbf{x}_k)} \right].
    \end{align*}\]</p><p>From Bayes&#39; rule and the Markovian property of <span>$\{X_k\}_k$</span> (as we derived earlier for <span>$p\left(\mathbf{x}_{k-1}|\mathbf{x}_k, \mathbf{x}_0\right)$</span>), we have</p><p class="math-container">\[    p\left(\mathbf{x}_{k-1}|\mathbf{x}_k, \mathbf{x}_0\right) = \frac{p\left(\mathbf{x}_k|\mathbf{x}_{k-1}, \mathbf{x}_0\right)p\left(\mathbf{x}_{k-1}|\mathbf{x}_0\right)}{p\left(\mathbf{x}_k|\mathbf{x}_0\right)} = \frac{p\left(\mathbf{x}_k|\mathbf{x}_{k-1}\right)p\left(\mathbf{x}_{k-1}|\mathbf{x}_0\right)}{p\left(\mathbf{x}_k|\mathbf{x}_0\right)},\]</p><p>which we can write as</p><p class="math-container">\[    p(\mathbf{x}_k|\mathbf{x}_{k-1}) = \frac{p(\mathbf{x}_{k-1}|\mathbf{x}_k, \mathbf{x}_0)p(\mathbf{x}_k|\mathbf{x}_0)}{p(\mathbf{x}_{k-1}|\mathbf{x}_0)}.\]</p><p>Hence,</p><p class="math-container">\[    \begin{align*}
        L_{\mathrm{VLB}}(\boldsymbol{\theta}) &amp; = \mathbb{E}_{p(\mathbf{x}_{0:K})}\left[ -\log p_{\boldsymbol{\theta}}(\mathbf{x}_K) + \log \frac{p(\mathbf{x}_1|\mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_0|\mathbf{x}_1)} + \sum_{k=2}^K \log \frac{p(\mathbf{x}_k|\mathbf{x}_{k-1})}{p_{\boldsymbol{\theta}}(\mathbf{x}_{k-1}|\mathbf{x}_k)} \right] \\
        &amp; = \mathbb{E}_{p(\mathbf{x}_{0:K})}\left[ -\log p_{\boldsymbol{\theta}}(\mathbf{x}_K) + \log \frac{p(\mathbf{x}_1|\mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_0|\mathbf{x}_1)} + \sum_{k=2}^K \log \frac{p(\mathbf{x}_{k-1}|\mathbf{x}_k, \mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_{k-1}|\mathbf{x}_k)} \frac{p(\mathbf{x}_k|\mathbf{x}_0)}{p(\mathbf{x}_{k-1}|\mathbf{x}_0)} \right] \\
        &amp; = \mathbb{E}_{p(\mathbf{x}_{0:K})}\left[ -\log p_{\boldsymbol{\theta}}(\mathbf{x}_K) + \log \frac{p(\mathbf{x}_1|\mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_0|\mathbf{x}_1)} + \sum_{k=2}^K \log \frac{p(\mathbf{x}_{k-1}|\mathbf{x}_k, \mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_{k-1}|\mathbf{x}_k)} + \sum_{k=2}^K \log \frac{p(\mathbf{x}_k|\mathbf{x}_0)}{p(\mathbf{x}_{k-1}|\mathbf{x}_0)} \right] \\
        &amp; = \mathbb{E}_{p(\mathbf{x}_{0:K})}\left[ -\log p_{\boldsymbol{\theta}}(\mathbf{x}_K) + \log \frac{p(\mathbf{x}_1|\mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_0|\mathbf{x}_1)} + \sum_{k=2}^K \log \frac{p(\mathbf{x}_{k-1}|\mathbf{x}_k, \mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_{k-1}|\mathbf{x}_k)} + \log \prod_{k=2}^K  \frac{p(\mathbf{x}_k|\mathbf{x}_0)}{p(\mathbf{x}_{k-1}|\mathbf{x}_0)} \right] \\
        &amp; = \mathbb{E}_{p(\mathbf{x}_{0:K})}\left[ -\log p_{\boldsymbol{\theta}}(\mathbf{x}_K) + \log \frac{p(\mathbf{x}_1|\mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_0|\mathbf{x}_1)} + \sum_{k=2}^K \log \frac{p(\mathbf{x}_{k-1}|\mathbf{x}_k, \mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_{k-1}|\mathbf{x}_k)} + \log \frac{p(\mathbf{x}_K|\mathbf{x}_0)}{p(\mathbf{x}_1|\mathbf{x}_0)} \right].
    \end{align*}\]</p><p>The first, second and fourth terms combine to yield</p><p class="math-container">\[    \begin{align*}
        -\log p_{\boldsymbol{\theta}}(\mathbf{x}_K) + \log \frac{p(\mathbf{x}_1|\mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_0|\mathbf{x}_1)} + \log \frac{p(\mathbf{x}_K|\mathbf{x}_0)}{p(\mathbf{x}_1|\mathbf{x}_0)} &amp; = -\log p_{\boldsymbol{\theta}}(\mathbf{x}_K) + \log \frac{p(\mathbf{x}_K|\mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_0|\mathbf{x}_1)} \\
        &amp; = \log \frac{p(\mathbf{x}_K|\mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_K)} - \log p_{\boldsymbol{\theta}}(\mathbf{x}_0|\mathbf{x}_1)
    \end{align*}\]</p><p>Thus,</p><p class="math-container">\[    \begin{align*}
        L_{\mathrm{VLB}}(\boldsymbol{\theta}) &amp; = \mathbb{E}_{p(\mathbf{x}_{0:K})}\left[ - \log p_{\boldsymbol{\theta}}(\mathbf{x}_0|\mathbf{x}_1) + \sum_{k=2}^K \log \frac{p(\mathbf{x}_{k-1}|\mathbf{x}_k, \mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_{k-1}|\mathbf{x}_k)} + \log \frac{p(\mathbf{x}_K|\mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_K)} \right].
    \end{align*}\]</p><p>This can be written as</p><p class="math-container">\[    L_{\mathrm{VLB}}(\boldsymbol{\theta}) = L_{\mathrm{VLB}, 0}(\boldsymbol{\theta}) + L_{\mathrm{VLB}, 1}(\boldsymbol{\theta}) + \cdots + L_{\mathrm{VLB}, K}(\boldsymbol{\theta}),\]</p><p>where</p><p class="math-container">\[    \begin{align*}
        L_{\mathrm{VLB, 0}}(\boldsymbol{\theta}) &amp; = \mathbb{E}_{p(\mathbf{x}_{0:K})}\left[ - \log p_{\boldsymbol{\theta}}(\mathbf{x}_0|\mathbf{x}_1) \right], \\
        L_{\mathrm{VLB}, k-1}(\boldsymbol{\theta}) &amp; = \mathbb{E}_{p(\mathbf{x}_{0:K})}\left[ \log \frac{p(\mathbf{x}_{k-1}|\mathbf{x}_k, \mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_{k-1}|\mathbf{x}_k)} \right], \quad k = 2, \ldots, K, \\
        L_{\mathrm{VLB}, K}(\boldsymbol{\theta}) &amp; = \mathbb{E}_{p(\mathbf{x}_{0:K})}\left[ \log \frac{p(\mathbf{x}_K|\mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_K)} \right].
    \end{align*}\]</p><p>Notice the terms with <span>$k&gt;0$</span> involve Kullback-Leibler divergences.</p><p>In the model, the last marginal is taken to be a standard normal distribution, and hence this term is constant and has no parameter to learn:</p><p class="math-container">\[    L_{\mathrm{VLB}, K}(\boldsymbol{\theta}) = L_{\mathrm{VLB}, K} = \mathbb{E}_{p(\mathbf{x}_{0:K})}\left[ \log \frac{p(\mathbf{x}_K|\mathbf{x}_0)}{\mathcal{N}(\mathbf{x}_K; \mathbf{0}, \mathbf{I})} \right].\]</p><p>Thus, the variational lower bound becomes</p><p class="math-container">\[    L_{\mathrm{VLB}}(\boldsymbol{\theta}) = L_{\mathrm{VLB}, 0}(\boldsymbol{\theta}) + L_{\mathrm{VLB}, 1}(\boldsymbol{\theta}) + \cdots + L_{\mathrm{VLB}, K}.\]</p><p><a href="https://dl.acm.org/doi/10.5555/3045118.3045358">Sohl-Dickstein, Weiss, Maheswaranathan, Ganguli (2015)</a> went on to model the mean and the covariance kernel of each reverse step and modifying the distributions by a function <span>$r(\mathbf{x}_k)$</span>, but we do not consider this here and go on with the simplifications and improvements carried on by <a href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html">Ho, Jain, and Abbeel (2020)</a>.</p><h4 id="Simplifications"><a class="docs-heading-anchor" href="#Simplifications">Simplifications</a><a id="Simplifications-1"></a><a class="docs-heading-anchor-permalink" href="#Simplifications" title="Permalink"></a></h4><p>Since the last term in <span>$L_{\mathrm{VLB}}(\boldsymbol{\theta})$</span> is constant, we only need to minimize </p><p class="math-container">\[    L_{\mathrm{VLB}}^*(\boldsymbol{\theta}) = L_{\mathrm{VLB, 0}}(\boldsymbol{\theta}) + L_{\mathrm{VLB}, 1}(\boldsymbol{\theta}) + \cdots + L_{\mathrm{VLB}, K-1}(\boldsymbol{\theta}).\]</p><p>For <span>$L_{\mathrm{VLB}, k-1}(\boldsymbol{\theta}),$</span> with <span>$k=2, \ldots, K,$</span> we use that</p><p class="math-container">\[    p\left(\mathbf{x}_{k-1}|\mathbf{x}_k, \mathbf{x}_0\right) = \mathcal{N}\left(\mathbf{x}_{k-1}; \tilde{\boldsymbol{\mu}}_k, \tilde \beta_k\mathbf{I}\right).\]</p><p>where</p><p class="math-container">\[    \begin{align*}
        \tilde{\boldsymbol{\mu}}_k &amp; = \tilde{\boldsymbol{\mu}}_k(\mathbf{x}_k, \mathbf{x}_0) = \frac{(1 - \bar{\alpha}_{k-1})\sqrt{\alpha_k}}{1 - \bar{\alpha}_k}\mathbf{x}_k + \frac{\beta_k\sqrt{\bar{\alpha}_{k-1}}}{1 - \bar{\alpha}_k}\mathbf{x}_0, \\
        \tilde\beta_k &amp; = \frac{1 - \bar{\alpha}_{k-1}}{1 - \bar{\alpha}_k}\beta_k.
    \end{align*}\]</p><p>We have also modeled <span>$p_{\boldsymbol{\theta}}(\mathbf{x}_{k-1}|\mathbf{x}_k)$</span> with</p><p class="math-container">\[    p_{\boldsymbol{\theta}}(\mathbf{x}_{k-1}|\mathbf{x}_k) = \mathcal{N}(\boldsymbol{\mu}_{\boldsymbol{\theta}}(\mathbf{x}_k, k), \sigma_k\mathbf{I}).\]</p><p>Moreover,</p><p class="math-container">\[    \begin{align*}
        L_{\mathrm{VLB}, k-1}(\boldsymbol{\theta}) &amp; = \mathbb{E}_{p(\mathbf{x}_{0:K})}\left[ \log \frac{p(\mathbf{x}_{k-1}|\mathbf{x}_k, \mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_{k-1}|\mathbf{x}_k)} \right] \\
        &amp; = \mathbb{E}_{p(\mathbf{x}_0, \mathbf{x}_k)p(\mathbf{x}_{k-1}|\mathbf{x}_0, \mathbf{x}_k)}\left[ \log \frac{p(\mathbf{x}_{k-1}|\mathbf{x}_k, \mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_{k-1}|\mathbf{x}_k)} \right] \\
        &amp; = \mathbb{E}_{p(\mathbf{x}_0, \mathbf{x}_k)}\left[\mathbb{E}_{p(\mathbf{x}_{k-1}|\mathbf{x}_0, \mathbf{x}_k)}\left[ \log \frac{p(\mathbf{x}_{k-1}|\mathbf{x}_k, \mathbf{x}_0)}{p_{\boldsymbol{\theta}}(\mathbf{x}_{k-1}|\mathbf{x}_k)} \right]\right] \\
        &amp; = \mathbb{E}_{p(\mathbf{x}_0, \mathbf{x}_k)} \left[D_{\mathrm{KL}}\left(p(\mathbf{x}_{k-1}|\mathbf{x}_k, \mathbf{x}_0) \| p_{\boldsymbol{\theta}}(\mathbf{x}_{k-1}|\mathbf{x}_k)\right)\right].
    \end{align*}\]</p><p>The Kullback-Leibler divergence between two multivariate normals can be computed explicitly. In general, we have</p><p class="math-container">\[    D_{\mathrm{KL}}(\mathcal{N}(\boldsymbol{\mu}_1, \boldsymbol{\Sigma}_1) \| \mathcal{N}(\boldsymbol{\mu}_2, \boldsymbol{\Sigma}_2)) = \frac{1}{2}\left( (\boldsymbol{\mu}_2 - \boldsymbol{\mu}_1) \cdot \boldsymbol{\Sigma}_2^{-1}(\boldsymbol{\mu}_2 - \boldsymbol{\mu}_1)  + \operatorname{tr}(\boldsymbol{\Sigma}_2^{-1}\boldsymbol{\Sigma}_1) - \log \frac{\det\boldsymbol{\Sigma}_1}{\det\boldsymbol{\Sigma}_2} - d\right).\]</p><p>Thus,</p><p class="math-container">\[    \begin{align*}
        L_{\mathrm{VLB}, k-1}(\boldsymbol{\theta}) &amp; = \frac{1}{2}\mathbb{E}_{p(\mathbf{x}_0, \mathbf{x}_k)} \left[\frac{\|\tilde{\boldsymbol{\mu}}_k(\mathbf{x}_k, \mathbf{x}_0) - \boldsymbol{\mu}_{\boldsymbol{\theta}}(\mathbf{x}_k, k) \|^2}{\sigma_k^2} + \frac{\tilde \beta_k}{\sigma_k^2} - \log\frac{{\tilde\beta_k}^d}{\sigma_k^{2d}} - d\right] \\
        &amp; = \frac{1}{2}\mathbb{E}_{p(\mathbf{x}_0, \mathbf{x}_k)} \left[\frac{\|\tilde{\boldsymbol{\mu}}_k(\mathbf{x}_k, \mathbf{x}_0) - \boldsymbol{\mu}_{\boldsymbol{\theta}}(\mathbf{x}_k, k) \|^2}{\sigma_k^2}\right] + C_k,
    \end{align*}\]</p><p>for a constant <span>$C_k$</span> (with respect to the trainable parameters <span>$\boldsymbol{\theta}$</span>), where</p><p class="math-container">\[    \begin{align*}
        \boldsymbol{\mu}_{\boldsymbol{\theta}}(\mathbf{x}_k, k) &amp; = \frac{1}{\sqrt{\alpha_k}}\mathbf{x}_k - \frac{1-\alpha_k}{\sqrt{1 - \bar{\alpha}_{k}}\sqrt{\alpha_{k}}}\boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\mathbf{x}_k, k) \\
        \tilde{\boldsymbol{\mu}}_k(\mathbf{x}_k, \mathbf{x}_0) &amp; = \frac{(1 - \bar{\alpha}_{k-1})\sqrt{\alpha_k}}{1 - \bar{\alpha}_k}\mathbf{x}_k + \frac{\beta_k\sqrt{\bar{\alpha}_{k-1}}}{1 - \bar{\alpha}_k}\mathbf{x}_0.
    \end{align*}\]</p><p>Thanks to the reparametrization,</p><p class="math-container">\[    \tilde{\boldsymbol{\mu}}_k = \frac{1}{\sqrt{\alpha_k}}\mathbf{x}_k - \frac{1-\alpha_k}{\sqrt{1 - \bar{\alpha}_{k}}\sqrt{\alpha_{k}}}\bar{\boldsymbol{\epsilon}}_k.\]</p><p>Thus,</p><p class="math-container">\[    \begin{align*}
        \tilde{\boldsymbol{\mu}}_k(\mathbf{x}_k, \mathbf{x}_0) - \boldsymbol{\mu}_{\boldsymbol{\theta}}(\mathbf{x}_k, k) &amp; = \frac{1}{\sqrt{\alpha_k}}\mathbf{x}_k - \frac{1-\alpha_k}{\sqrt{1 - \bar{\alpha}_{k}}\sqrt{\alpha_{k}}}\bar{\boldsymbol{\epsilon}}_k - \frac{1}{\sqrt{\alpha_k}}\mathbf{x}_k + \frac{1-\alpha_k}{\sqrt{1 - \bar{\alpha}_{k}}\sqrt{\alpha_{k}}}\boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\mathbf{x}_k, k) \\
        &amp; = \frac{1-\alpha_k}{\sqrt{1 - \bar{\alpha}_{k}}\sqrt{\alpha_{k}}}\left(\bar{\boldsymbol{\epsilon}}_k - \boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\mathbf{x}_k, k)\right).
    \end{align*}\]</p><p>Now we reparametrize the loss in terms of <span>$\mathbf{x}_0$</span> and <span>$\bar{\boldsymbol{\epsilon}}_k \sim \mathcal{N}(\mathbf{0}, \mathbf{I}),$</span> by writing <span>$\mathbf{x}_k$</span> as</p><p class="math-container">\[    \mathbf{x}_k = \sqrt{\bar{\alpha}_{k}}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_{k}}\bar{\boldsymbol{\epsilon}}_k.\]</p><p>With this reparametrization, the expectation also becomes in terms of <span>$\mathbf{x}_0$</span> and <span>$\bar{\boldsymbol{\epsilon}}_k,$</span> so the loss becomes</p><p class="math-container">\[    L_{\mathrm{VLB}}^*(\boldsymbol{\theta}) = L_{\mathrm{VLB, 0}}(\boldsymbol{\theta}) + L_{\mathrm{VLB}, 1}(\boldsymbol{\theta}) + \cdots + L_{\mathrm{VLB}, K-1}(\boldsymbol{\theta})\]</p><p>with</p><p class="math-container">\[    L_{\mathrm{VLB}, k-1}(\boldsymbol{\theta}) = \frac{1}{2\sigma_k^2}\frac{1-\alpha_k}{\sqrt{1 - \bar{\alpha}_{k}}\sqrt{\alpha_{k}}}\mathbb{E}_{p_0(\mathbf{x}_0)\mathcal{N}(\bar{\boldsymbol{\epsilon}}_k; \mathbf{0}, \mathbf{I})} \left[\left\|\bar{\boldsymbol{\epsilon}}_k - \boldsymbol{\epsilon}_{\boldsymbol{\theta}}\left(\sqrt{\bar{\alpha}_{k}}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_{k}}\bar{\boldsymbol{\epsilon}}_k, k\right) \right\|^2\right],\]</p><p>for <span>$k=1, \ldots, K.$</span></p><p>At this point, a stochastic gradient descent approach is taken, and instead of descending along the gradient of the full sum of all <span>$L_{\mathrm{VLB}, k-1}(\boldsymbol{\theta})$</span> and taking the expectation with respect to <span>$\bar{\boldsymbol{\epsilon}}_k,$</span> only a single time step <span>$k$</span> and a single noise term <span>$\bar{\boldsymbol{\epsilon}}_k$</span> is taken at random for each sample point <span>$\mathbf{x}_0^n,$</span> <span>$n=1, \ldots, N,$</span> so that the loss function taken in practice is</p><p class="math-container">\[    {\tilde L}_{\mathrm{VLB}}^*(\boldsymbol{\theta}) = \frac{1}{N} \sum_{n=1}^N \frac{1}{2\sigma_{k_n}^2}\frac{1-\alpha_{k_n}}{\sqrt{1 - \bar{\alpha}_{k_n}}\sqrt{\alpha_{k_n}}} \left\|\bar{\boldsymbol{\epsilon}}_{k_n, n} - \boldsymbol{\epsilon}_{\boldsymbol{\theta}}\left(\sqrt{\bar{\alpha}_{k_n}}\mathbf{x}_0^n + \sqrt{1 - \bar{\alpha}_{k_n}}\bar{\boldsymbol{\epsilon}}_{k_n, n}, k_n\right) \right\|^2,\]</p><p>where the <span>$k_n$</span> and the <span>$\bar{\boldsymbol{\epsilon}}_{k_n, n}$</span> are chosen independently, at each epoch, according to</p><p class="math-container">\[     k_n \sim \operatorname{Uniform}(1, \ldots, K), \; \bar{\boldsymbol{\epsilon}}_{k_n, n} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}).\]</p><p>A further simplification proposed by <a href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html">Ho, Jain, and Abbeel (2020)</a>, which was found to perform better in practice, is to simply drop the weighting term and minimize</p><p class="math-container">\[    {\tilde L}_{\mathrm{VLB}}^{\mathrm{simple}}(\boldsymbol{\theta}) = \frac{1}{N} \sum_{n=1}^N \left\|\bar{\boldsymbol{\epsilon}}_{k_n, n} - \boldsymbol{\epsilon}_{\boldsymbol{\theta}}\left(\sqrt{\bar{\alpha}_{k_n}}\mathbf{x}_0^n + \sqrt{1 - \bar{\alpha}_{k_n}}\bar{\boldsymbol{\epsilon}}_{k_n, n}, k_n\right) \right\|^2.\]</p><h3 id="Connection-with-score-matching"><a class="docs-heading-anchor" href="#Connection-with-score-matching">Connection with score matching</a><a id="Connection-with-score-matching-1"></a><a class="docs-heading-anchor-permalink" href="#Connection-with-score-matching" title="Permalink"></a></h3><p>Going back to</p><p class="math-container">\[    \mathbf{x}_k = \sqrt{\bar{\alpha}_{k}}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_{k}}\bar{\boldsymbol{\epsilon}}_k,\]</p><p>we have </p><p class="math-container">\[    \begin{align*}
        \bar{\boldsymbol{\epsilon}}_k - \boldsymbol{\epsilon}_{\boldsymbol{\theta}}\left(\sqrt{\bar{\alpha}_{k}}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_{k}}\bar{\boldsymbol{\epsilon}}_k, k\right) &amp; = \frac{\mathbf{x}_k - \sqrt{\bar{\alpha}_{k}}\mathbf{x}_0}{\sqrt{1 - \bar{\alpha}_{k}}} - \boldsymbol{\epsilon}_{\boldsymbol{\theta}}\left(\mathbf{x}_k, k\right) \\
        &amp; = \sqrt{1 - \bar{\alpha}_{k}} \left(\frac{\mathbf{x}_k - \sqrt{\bar{\alpha}_{k}}\mathbf{x}_0}{1 - \bar{\alpha}_{k}} - \frac{\boldsymbol{\epsilon}_{\boldsymbol{\theta}}\left(\mathbf{x}_k, k\right)}{\sqrt{1 - \bar{\alpha}_{k}}}\right)
    \end{align*}\]</p><p>The first term in the parenthesis can be seen as the score function of the conditional distribution of <span>$\mathbf{X}_k$</span> given <span>$\mathbf{X}_0 = \mathbf{x}_0$</span>, with density</p><p class="math-container">\[    p(\mathbf{x}_k|\mathbf{x}_0) = \mathcal{N}(\mathbf{x}_k; \sqrt{\bar{\alpha}_{k}}\mathbf{x}_0, 1 - \bar{\alpha}_{k}) = \frac{1}{(2\pi (1 - \bar\alpha_k))^{d/2}} e^{-\frac{1}{2}\frac{(\mathbf{x}_k - \sqrt{\bar\alpha_k}\mathbf{x}_0)^2}{1 - \bar\alpha_k}}.\]</p><p>The score function of this distribution reads</p><p class="math-container">\[    \boldsymbol{\nabla}_{\mathbf{x}_k}\log p(\mathbf{x}_k|\mathbf{x}_0) =  - \frac{\mathbf{x}_k - \sqrt{\bar\alpha_k}\mathbf{x}_0}{1 - \bar\alpha_k}.\]</p><p>Introducing</p><p class="math-container">\[    \tilde{\boldsymbol{\epsilon}}_{\boldsymbol{\theta}}\left(\mathbf{x}_k, k\right) = -\frac{\boldsymbol{\epsilon}_{\boldsymbol{\theta}}\left(\mathbf{x}_k, k\right)}{\sqrt{1 - \bar{\alpha}_{k}}},\]</p><p>we rewrite the loss term <span>$L_{\mathrm{VLB}, k-1}(\boldsymbol{\theta})$</span> as</p><p class="math-container">\[    L_{\mathrm{VLB}, k-1}(\boldsymbol{\theta}) = \frac{1}{2\sigma_k^2}\frac{(1-\alpha_k)\sqrt{1 - \bar{\alpha}_{k}}}{\sqrt{\alpha_{k}}}\mathbb{E}_{p_0(\mathbf{x}_0)p(\mathbf{x}_k|\mathbf{x}_0)} \left[\left\|\boldsymbol{\nabla}_{\mathbf{x}_k}\log p(\mathbf{x}_k|\mathbf{x}_0) -  \tilde{\boldsymbol{\epsilon}}_{\boldsymbol{\theta}}\left(\mathbf{x}_k, k\right)\right\|^2\right],\]</p><p>for <span>$k=1, \ldots, K$</span>. </p><p>Thus, <span>$\boldsymbol{\epsilon}_{\boldsymbol{\theta}}\left(\mathbf{x}_k, k\right)$</span> is actually learning a scaled version of the score function of the diffusion conditioned at each sample point. Notice the relation with denoising score matching.</p><h2 id="More-improvements"><a class="docs-heading-anchor" href="#More-improvements">More improvements</a><a id="More-improvements-1"></a><a class="docs-heading-anchor-permalink" href="#More-improvements" title="Permalink"></a></h2><p><a href="https://openreview.net/forum?id=-NEXDKk8gZ">Nichol and Dhariwal (2021)</a> modified a little more the simplifications in <a href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html">Ho, Jain, and Abbeel (2020)</a> and improved even more some of the benchmark results for DDPM.</p><p>While <a href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html">Ho, Jain, and Abbeel (2020)</a> pre-determined the variance <span>$\sigma_k^2$</span> of the reverse model to either one of the extreme values <span>$\beta_k$</span> and <span>$\tilde\beta_k$</span>, <a href="https://openreview.net/forum?id=-NEXDKk8gZ">Nichol and Dhariwal (2021)</a> found best to learn this variance, but in a very specific form, namely, as a combination</p><p class="math-container">\[    \log \sigma_k^2 = v \log\beta_k + (1 - v)\log\tilde\beta_k.\]</p><p>Another improvement was to use a nonlinear cosine variance schedule <span>$\beta_k$</span> changing very little near the extremes <span>$k=1$</span> and <span>$k=K$</span> and with a linear dropoff of <span>$\tilde\alpha_k$</span> in the middle.</p><p>The motivation was that while DDPM had showed very good results with CIFAR-10 and LSUN datasets, it did not achieve log-likelyhood competitiveness with other likelyhood based models. This casted doubts whether DDPM would capture all the modes of a distribution and perform well on a more diverse datasets.</p><p>With the improvements above, <a href="https://openreview.net/forum?id=-NEXDKk8gZ">Nichol and Dhariwal (2021)</a>, showed that log-likelyhood competitiveness could indeed be achieved with DDPM.</p><h2 id="Numerical-example"><a class="docs-heading-anchor" href="#Numerical-example">Numerical example</a><a id="Numerical-example-1"></a><a class="docs-heading-anchor-permalink" href="#Numerical-example" title="Permalink"></a></h2><p>We illustrate the method numerically, modeling a synthetic univariate Gaussian mixture distribution.</p><h3 id="Julia-language-setup"><a class="docs-heading-anchor" href="#Julia-language-setup">Julia language setup</a><a id="Julia-language-setup-1"></a><a class="docs-heading-anchor-permalink" href="#Julia-language-setup" title="Permalink"></a></h3><p>As usual, we use the <a href="https://julialang.org">Julia programming language</a> for the numerical simulations, with suitable packages and set the seed for reproducibility purposes.</p><h3 id="Data"><a class="docs-heading-anchor" href="#Data">Data</a><a id="Data-1"></a><a class="docs-heading-anchor-permalink" href="#Data" title="Permalink"></a></h3><p>We build the target model, draw samples from it, and prepare all the parameters for training and sampling.</p><pre><code class="language-julia hljs">target_prob = MixtureModel([Normal(-3, 1), Normal(3, 1)], [0.1, 0.9])

xrange = range(-10, 10, 200)
dx = Float64(xrange.step)
xx = permutedims(collect(xrange))
target_pdf = pdf.(target_prob, xrange&#39;)

sample_points = permutedims(rand(rng, target_prob, 1024))</code></pre><pre><code class="language-julia hljs">beta_init = 0.02
beta_final = 0.8
beta_len = 80
beta_schedule = range(beta_init, beta_final, beta_len)
alpha_schedule = 1 .- beta_schedule
alpha_tilde = cumprod(alpha_schedule)
beta_tilde = ( 1 .- [0; alpha_tilde[begin:end-1]]) ./ ( 1 .- alpha_tilde ) .* beta_schedule
coeffs_train = (
    krange = 1:beta_len,
    sqrtalphatilde = map(√, alpha_tilde),
    sqrtoneminusalphatilde = map(x -&gt; √(1 - x), alpha_tilde)
)
coeffs_sample = (
    sqrtalpha = map(√, alpha_schedule),
    alpharatio = (1 .- alpha_schedule) ./ map(x -&gt; √(1 - x), alpha_tilde),
    sigmaschedule = copy(beta_schedule)
)
data = (rng, sample_points, coeffs_train)</code></pre><img src="9c90ef8f.svg" alt="Example block output"/><h3 id="Markov-chain"><a class="docs-heading-anchor" href="#Markov-chain">Markov chain</a><a id="Markov-chain-1"></a><a class="docs-heading-anchor-permalink" href="#Markov-chain" title="Permalink"></a></h3><p>Now we evolve the sample as the initial state of a Markov chain <span>$\{\mathbf{X}_k\}_{k=1, \ldots, K},$</span> with</p><p class="math-container">\[    \mathbf{X}_k = \sqrt{1 - \beta_k} \mathbf{X}_{k-1} + \beta_k \mathbf{Z}_k, \quad \mathbf{Z}_k \sim \mathcal{N}(\mathbf{0}, \mathbf{I}),\]</p><div class="markdown"><p>where &#36;\&#123;\beta_k\&#125;_&#123;k&#61;1&#125;^&#123;K&#125;&#36; is a given schedule.We choose the schedule to be a linear schedule from &#36;\beta_1 &#61; 0.02&#36; to &#36;\beta_K &#61; 0.8&#36; in &#36;K &#61; 80&#36; steps.</p>
</div><img src="bbeaf11d.svg" alt="Example block output"/><p>The final histogram and the asymptotic standard normal distribution.</p><img src="528d0b2a.svg" alt="Example block output"/><h3 id="The-neural-network-model"><a class="docs-heading-anchor" href="#The-neural-network-model">The neural network model</a><a id="The-neural-network-model-1"></a><a class="docs-heading-anchor-permalink" href="#The-neural-network-model" title="Permalink"></a></h3><p>There are two natural ways to model <span>$\boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\mathbf{x}_k, k)$</span>, where <span>$\mathbf{x}_k\in \mathbb{R}$</span> and <span>$k\in \{1, 2, \ldots, K\}.$</span> One is to embed it into a two-dimensional function</p><p class="math-container">\[    \mathbb{R}^2 \ni (\mathbf{x}_k, k) \mapsto \boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\mathbf{x}_k, k) \in \mathbb{R},\]</p><p>and the other is as a vector-valued function</p><p class="math-container">\[    \mathbb{R} \ni \mathbf{x}_k \mapsto \left(\boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\mathbf{x}_k, k)\right)_{k=1}^K \in \mathbb{R}^K.\]</p><p>The first one is in a framework compatible with the limit case of a stochastic differential equations, where <span>$k$</span> becomes a time variable <span>$t\in [0, T]$</span>, <span>$T &gt; 0$</span>. The second one is compatible with the fact that we do have a finite number <span>$K$</span> of steps and this also gives a more flexible network. In practice, though, the second case, even increasing the width and depth of the neural network, did not improve the results. So we decide to present here only the first option.</p><p>Thus, the neural network we consider is a two-dimensional feed-forward neural network with scalar values. Since we model the steps of the whole Markov chain, not only, a single random variable, we need to bump it up a little bit.</p><pre><code class="language-julia hljs">model = Chain(Dense(2 =&gt; 64, relu), Dense(64 =&gt; 1))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
    layer_1 = Dense(2 =&gt; 64, relu),     <span class="sgr90"># 192 parameters</span>
    layer_2 = Dense(64 =&gt; 1),           <span class="sgr90"># 65 parameters</span>
) <span class="sgr90">        # Total: </span>257 parameters,
<span class="sgr90">          #        plus </span>0 states.</code></pre><p>We initialize the <em>parameters</em> and the <em>state</em> of the model.</p><pre><code class="language-julia hljs">ps, st = Lux.setup(rng, model) # initialize and get the parameters and states of the model</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">((layer_1 = (weight = Float32[0.9099216 1.7035809; 2.2073193 -1.0544307; … ; -0.18384793 1.2372406; -2.008759 1.904416], bias = Float32[0.32088393, -0.69005346, 0.25754043, -0.14362103, 0.62143624, -0.60737354, -0.62784046, -0.53939164, -0.29663956, -0.51113373  …  -0.18165907, 0.5390017, -0.27764937, -0.18431795, 0.18858354, -0.66726047, 0.075162835, 0.22602992, 0.20575224, -0.312054]), layer_2 = (weight = Float32[0.072181694 0.0481039 … -0.1881795 -0.063174546], bias = Float32[-0.043427452])), (layer_1 = NamedTuple(), layer_2 = NamedTuple()))</code></pre><h3 id="Loss-function"><a class="docs-heading-anchor" href="#Loss-function">Loss function</a><a id="Loss-function-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-function" title="Permalink"></a></h3><p>Here it is how we implement the objective <span>${\tilde L}_{\mathrm{VLB}}^{\mathrm{simple}}(\boldsymbol{\theta}).$</span></p><pre><code class="language-julia hljs">function loss_function_uniform_simple(model, ps, st, data)
    rng, sample_points, coeffs_train = data
    epsilons = randn(rng, size(sample_points))
    ks = rand(rng, coeffs_train.krange, size(sample_points))
    model_input = [coeffs_train.sqrtalphatilde[ks] .* sample_points .+ coeffs_train.sqrtoneminusalphatilde[ks] .* epsilons; ks]
    epsilons_pred, st = Lux.apply(model, model_input, ps, st)
    loss = mean(abs2, epsilons_pred .- epsilons)
    return loss, st, ()
end</code></pre><p>This is how the points used for training look like at a given epoch:</p><img src="83c3eb67.svg" alt="Example block output"/><h3 id="Optimization-setup"><a class="docs-heading-anchor" href="#Optimization-setup">Optimization setup</a><a id="Optimization-setup-1"></a><a class="docs-heading-anchor-permalink" href="#Optimization-setup" title="Permalink"></a></h3><p>We do the usual setup.</p><pre><code class="language-julia hljs">opt = Adam(0.001)

tstate_org = Lux.Training.TrainState(model, ps, st, opt)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">TrainState
    model: Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.relu), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(2 =&gt; 64, relu), layer_2 = Dense(64 =&gt; 1)), nothing)
    # of parameters: 257
    # of states: 0
    optimizer: Optimisers.Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8)
    step: 0</code></pre><pre><code class="language-julia hljs">vjp_rule = Lux.Training.AutoZygote()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">ADTypes.AutoZygote()</code></pre><pre><code class="language-julia hljs">dev_cpu = cpu_device()
## dev_gpu = gpu_device()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(::MLDataDevices.CPUDevice) (generic function with 1 method)</code></pre><pre><code class="language-julia hljs">Lux.Training.compute_gradients(vjp_rule, loss_function_uniform_simple, data, tstate_org)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">((layer_1 = (weight = Float32[1.3822634 463.59445; 0.07508751 0.08460611; … ; -3.6035955 -1208.6024; -1.1588255 -405.70822], bias = Float32[8.826831, 0.021416815, 0.013986928, 10.287506, -0.23812228, 0.49923918, 3.7859986, -22.68573, -14.231682, 23.075134  …  0.47977373, -0.06794904, 1.40501, 0.00071532663, 16.901669, 0.03290332, 0.0037805724, 19.634628, -23.01177, -7.710539]), layer_2 = (weight = Float32[10998.09 1.2837229 … 7967.9463 12155.265], bias = Float32[122.28628])), 4833.970922739681, (), Lux.Training.TrainState{Nothing, Nothing, Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.relu), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, @NamedTuple{layer_1::@NamedTuple{weight::Matrix{Float32}, bias::Vector{Float32}}, layer_2::@NamedTuple{weight::Matrix{Float32}, bias::Vector{Float32}}}, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}}, Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, @NamedTuple{layer_1::@NamedTuple{weight::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}}, layer_2::@NamedTuple{weight::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}}}}(nothing, nothing, Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.relu), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(2 =&gt; 64, relu), layer_2 = Dense(64 =&gt; 1)), nothing), (layer_1 = (weight = Float32[0.9099216 1.7035809; 2.2073193 -1.0544307; … ; -0.18384793 1.2372406; -2.008759 1.904416], bias = Float32[0.32088393, -0.69005346, 0.25754043, -0.14362103, 0.62143624, -0.60737354, -0.62784046, -0.53939164, -0.29663956, -0.51113373  …  -0.18165907, 0.5390017, -0.27764937, -0.18431795, 0.18858354, -0.66726047, 0.075162835, 0.22602992, 0.20575224, -0.312054]), layer_2 = (weight = Float32[0.072181694 0.0481039 … -0.1881795 -0.063174546], bias = Float32[-0.043427452])), (layer_1 = NamedTuple(), layer_2 = NamedTuple()), Optimisers.Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), (layer_1 = (weight = <span class="sgr32">Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0 0.0; 0.0 0.0; … ; 0.0 0.0; 0.0 0.0], Float32[0.0 0.0; 0.0 0.0; … ; 0.0 0.0; 0.0 0.0], (0.9, 0.999))<span class="sgr32">)</span>, bias = <span class="sgr32">Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))<span class="sgr32">)</span>), layer_2 = (weight = <span class="sgr32">Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0], (0.9, 0.999))<span class="sgr32">)</span>, bias = <span class="sgr32">Leaf(Adam(eta=0.001, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0], Float32[0.0], (0.9, 0.999))<span class="sgr32">)</span>)), 0))</code></pre><h3 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h3><p>Now we train the model. Since this is stochastic and it is a Markov chain, it takes some more epochs than the previous ones.</p><pre><code class="language-julia hljs">@time tstate, losses, tstates = train(tstate_org, vjp_rule, data, loss_function_uniform_simple, 1000, 20, 25)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌ Warning: Mixed-Precision `matmul_cpu_fallback!` detected and Octavian.jl cannot be used for this set of inputs (C [Matrix{Float64}]: A [Matrix{Float32}] x B [Matrix{Float64}]). Falling back to generic implementation. This may be slow.
└ @ LuxLib.Impl ~/.julia/packages/LuxLib/1B1qw/src/impl/matmul.jl:190
Epoch: 0 || Loss: 4954.146151722542
Epoch: 50 || Loss: 11.960118757663027
Epoch: 100 || Loss: 1.8538717815733554
Epoch: 150 || Loss: 1.1730596953348047
Epoch: 200 || Loss: 1.0685916129266246
Epoch: 250 || Loss: 0.855112284221514
Epoch: 300 || Loss: 0.7459049432281369
Epoch: 350 || Loss: 0.6600571158186551
Epoch: 400 || Loss: 0.6115033109385754
Epoch: 450 || Loss: 0.5164752037314564
Epoch: 500 || Loss: 0.42767928083481627
Epoch: 550 || Loss: 0.43484322635710465
Epoch: 600 || Loss: 0.43214275658446916
Epoch: 650 || Loss: 0.35482598750891536
Epoch: 700 || Loss: 0.3174688932646217
Epoch: 750 || Loss: 0.32403858852006084
Epoch: 800 || Loss: 0.2862748682905556
Epoch: 850 || Loss: 0.3042753822298034
Epoch: 900 || Loss: 0.27124121828363673
Epoch: 950 || Loss: 0.26418666103080246
Epoch: 1000 || Loss: 0.2552029689546693
  2.339537 seconds (650.45 k allocations: 1.670 GiB, 8.91% gc time, 60.24% compilation time)</code></pre><img src="b0bca778.svg" alt="Example block output"/><h3 id="Results"><a class="docs-heading-anchor" href="#Results">Results</a><a id="Results-1"></a><a class="docs-heading-anchor-permalink" href="#Results" title="Permalink"></a></h3><p>For checking the results, we first need to implement the sampling backward chain.</p><pre><code class="language-julia hljs">function ddpm_backward_chain!(rng, xbwt, xbwK, coeffs_sample, tstate, aux = xbwt[1:2, :])
    @assert axes(xbwt, 1) == only(axes(alpha_schedule))
    i1 = lastindex(axes(xbwt, 1))
    randn!(rng, xbwt)
    xbwt[i1, :] .= xbwK
    for i in Iterators.drop(Iterators.reverse(axes(xbwt, 1)), 1)
        aux[1, :] .= view(xbwt, i1, :)
        aux[2, :] .= i1
        xbwt[i:i, :] .= ( view(xbwt, i1:i1, :) .- coeffs_sample.alpharatio[i1] .* first(tstate.model(aux, tstate.parameters, tstate.states)) ) ./ coeffs_sample.sqrtalpha[i1] .+ coeffs_sample.sigmaschedule[i1] .* view(xbwt, i:i, :)
        i1 = i
    end
    return xbwt
end

function ddpm_backward_chain(rng, xbwK, coeffs_sample, tstate)
    xbwt = alpha_schedule .* xbwK&#39;
    ddpm_backward_chain!(rng, xbwt, xbwK, coeffs_sample, tstate)
    return xbwt
end</code></pre><p>With that implemented, we can generate some samples.</p><pre><code class="language-julia hljs">xbwK = randn(rng, size(x0))
xbwt = ddpm_backward_chain(rng, xbwK, coeffs_sample, tstate)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌ Warning: Mixed-Precision `matmul_cpu_fallback!` detected and Octavian.jl cannot be used for this set of inputs (C [Matrix{Float64}]: A [Matrix{Float32}] x B [Matrix{Float64}]). Falling back to generic implementation. This may be slow.
└ @ LuxLib.Impl ~/.julia/packages/LuxLib/1B1qw/src/impl/matmul.jl:190</code></pre><p>Here is the resulting backward trajectories.</p><img src="dbebbaed.svg" alt="Example block output"/><p>We then visualize the histogram of the generated samples and compare it with the pdf of the synthetic target distribution.</p><img src="d5da23c3.svg" alt="Example block output"/><p>As one can see, it did not generate spurious samples, but it was concentrated near the highest modal of the target distribution. Different linear variance schedules, more training, and different network architectures resulted in somewhat the same pattern. Maybe a nonlinear schedule improves the result? Let&#39;s try.</p><img src="6ddef339.svg" alt="Example block output"/><img src="05ec8ed8.svg" alt="Example block output"/><img src="46ec9c64.svg" alt="Example block output"/><pre><code class="language-julia hljs">@time tstate, losses, tstates = train(tstate_org, vjp_rule, data, loss_function_uniform_simple, 5000, 20, 25)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌ Warning: Mixed-Precision `matmul_cpu_fallback!` detected and Octavian.jl cannot be used for this set of inputs (C [Matrix{Float64}]: A [Matrix{Float32}] x B [Matrix{Float64}]). Falling back to generic implementation. This may be slow.
└ @ LuxLib.Impl ~/.julia/packages/LuxLib/1B1qw/src/impl/matmul.jl:190
Epoch: 0 || Loss: 4757.573961159425
Epoch: 250 || Loss: 1.0479398503588588
Epoch: 500 || Loss: 0.957899806603077
Epoch: 750 || Loss: 0.8026084715244522
Epoch: 1000 || Loss: 0.7372313528400609
Epoch: 1250 || Loss: 0.7501348396679404
Epoch: 1500 || Loss: 0.6501434701077246
Epoch: 1750 || Loss: 0.6622920513058729
Epoch: 2000 || Loss: 0.6183539152632823
Epoch: 2250 || Loss: 0.6347250798299169
Epoch: 2500 || Loss: 0.6275078080690096
Epoch: 2750 || Loss: 0.6166220813508866
Epoch: 3000 || Loss: 0.6191777540370489
Epoch: 3250 || Loss: 0.608400374107472
Epoch: 3500 || Loss: 0.5770053988460424
Epoch: 3750 || Loss: 0.5422083398696724
Epoch: 4000 || Loss: 0.5962286687384684
Epoch: 4250 || Loss: 0.5488858003857225
Epoch: 4500 || Loss: 0.6302290875696327
Epoch: 4750 || Loss: 0.6552777095659245
Epoch: 5000 || Loss: 0.6092394558000195
  4.771129 seconds (1.93 M allocations: 8.277 GiB, 19.93% gc time)</code></pre><img src="8038d1ff.svg" alt="Example block output"/><pre><code class="language-julia hljs">xbwK = randn(rng, size(x0))
xbwt = ddpm_backward_chain(rng, xbwK, coeffs_sample, tstate)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌ Warning: Mixed-Precision `matmul_cpu_fallback!` detected and Octavian.jl cannot be used for this set of inputs (C [Matrix{Float64}]: A [Matrix{Float32}] x B [Matrix{Float64}]). Falling back to generic implementation. This may be slow.
└ @ LuxLib.Impl ~/.julia/packages/LuxLib/1B1qw/src/impl/matmul.jl:190</code></pre><img src="b0db8969.svg" alt="Example block output"/><img src="6406a6c6.svg" alt="Example block output"/><p>I guess not even that... We need to try harder.</p><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><ol><li><a href="https://dl.acm.org/doi/10.5555/3045118.3045358">J. Sohl-Dickstein, E. A. Weiss, N. Maheswaranathan, S. Ganguli (2015), &quot;Deep unsupervised learning using nonequilibrium thermodynamics&quot;, ICML&#39;15: Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, 2256-2265</a></li><li><a href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html">J. Ho, A. Jain, P. Abbeel (2020), &quot;Denoising diffusion probabilistic models&quot;, in Advances in Neural Information Processing Systems 33, NeurIPS2020</a></li><li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">L. Weng (2021), &quot;What are diffusion models?&quot; Lil’Log. lilianweng.github.io/posts/2021-07-11-diffusion-models/</a></li><li><a href="https://openreview.net/forum?id=-NEXDKk8gZ">A. Q. Nichol and P. Dhariwal (2021), &quot;Improved denoising diffusion probabilistic models&quot;, ICLR 2021 Conference</a></li><li><a href="https://openreview.net/forum?id=St1giarCHLP">J. Song, C. Meng, S. Ermon (2021), &quot;Denoising diffusion implicit models&quot;, ICLR 2021 Conference</a></li></ol></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../2d_FD_score_matching/">« 2D finite-difference score matching</a><a class="docs-footer-nextpage" href="../mdsm/">Multiple denoising score matching »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.13.0 on <span class="colophon-date" title="Thursday 26 June 2025 15:37">Thursday 26 June 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
