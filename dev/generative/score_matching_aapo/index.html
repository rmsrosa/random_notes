<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Score matching of Aapo Hyvärinen · Random notes</title><meta name="title" content="Score matching of Aapo Hyvärinen · Random notes"/><meta property="og:title" content="Score matching of Aapo Hyvärinen · Random notes"/><meta property="twitter:title" content="Score matching of Aapo Hyvärinen · Random notes"/><meta name="description" content="Documentation for Random notes."/><meta property="og:description" content="Documentation for Random notes."/><meta property="twitter:description" content="Documentation for Random notes."/><meta property="og:url" content="https://github.com/rmsrosa/random_notes/generative/score_matching_aapo/"/><meta property="twitter:url" content="https://github.com/rmsrosa/random_notes/generative/score_matching_aapo/"/><link rel="canonical" href="https://github.com/rmsrosa/random_notes/generative/score_matching_aapo/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/style.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="Random notes logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Random notes</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Random Notes</a></li><li><span class="tocitem">Probability Essentials</span><ul><li><a class="tocitem" href="../../probability/kernel_density_estimation/">Kernel Density Estimation</a></li><li><a class="tocitem" href="../../probability/convergence_notions/">Convergence notions</a></li></ul></li><li><span class="tocitem">Discrete-time Markov chains</span><ul><li><a class="tocitem" href="../../markov_chains/mc_definitions/">Essential definitions</a></li><li><a class="tocitem" href="../../markov_chains/mc_invariance/">Invariant distributions</a></li><li><input class="collapse-toggle" id="menuitem-3-3" type="checkbox"/><label class="tocitem" for="menuitem-3-3"><span class="docs-label">Countable-space Markov chains</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../markov_chains/mc_countableX_recurrence/">Recurrence in the countable-space case</a></li><li><a class="tocitem" href="../../markov_chains/mc_countableX_connections/">Connected states, irreducibility and uniqueness of invariant measures</a></li><li><a class="tocitem" href="../../markov_chains/mc_countableX_convergencia/">Aperiodicidade e convergência para a distribuição estacionária</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-4" type="checkbox"/><label class="tocitem" for="menuitem-3-4"><span class="docs-label">Continuous-space Markov chains</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../markov_chains/mc_irreducibility_and_recurrence/">Irreducibility and recurrence in the continuous-space case</a></li></ul></li></ul></li><li><span class="tocitem">Sampling methods</span><ul><li><a class="tocitem" href="../../sampling/overview/">Overview</a></li><li><a class="tocitem" href="../../sampling/prng/">Random number generators</a></li><li><input class="collapse-toggle" id="menuitem-4-3" type="checkbox"/><label class="tocitem" for="menuitem-4-3"><span class="docs-label">Transform methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/invFtransform/">Probability integral transform</a></li><li><a class="tocitem" href="../../sampling/box_muller/">Box-Muller transform</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-4" type="checkbox"/><label class="tocitem" for="menuitem-4-4"><span class="docs-label">Accept-Reject methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/rejection_sampling/">Rejection sampling</a></li><li><a class="tocitem" href="../../sampling/empiricalsup_rejection/">Empirical supremum rejection sampling</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-5" type="checkbox"/><label class="tocitem" for="menuitem-4-5"><span class="docs-label">Markov Chain Monte Carlo (MCMC)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/mcmc/">Overview</a></li><li><a class="tocitem" href="../../sampling/metropolis/">Metropolis and Metropolis-Hastings</a></li><li><a class="tocitem" href="../../sampling/convergence_metropolis/">Convergence of Metropolis-Hastings</a></li><li><a class="tocitem" href="../../sampling/gibbs/">Gibbs sampling</a></li><li><a class="tocitem" href="../../sampling/hmc/">Hamiltonian Monte Carlo (HMC)</a></li></ul></li><li><a class="tocitem" href="../../sampling/langevin_sampling/">Langevin sampling</a></li></ul></li><li><span class="tocitem">Bayesian inference</span><ul><li><input class="collapse-toggle" id="menuitem-5-1" type="checkbox"/><label class="tocitem" for="menuitem-5-1"><span class="docs-label">Bayes Theory</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/bayes/">Bayes Theorem</a></li><li><a class="tocitem" href="../../bayesian/bayes_inference/">Bayesian inference</a></li><li><a class="tocitem" href="../../bayesian/bernstein_vonmises/">Bernstein–von Mises theorem</a></li></ul></li><li><a class="tocitem" href="../../bayesian/bayesian_probprog/">Bayesian probabilistic programming</a></li><li><input class="collapse-toggle" id="menuitem-5-3" type="checkbox"/><label class="tocitem" for="menuitem-5-3"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/find_pi/">Estimating π via frequentist and Bayesian methods</a></li><li><a class="tocitem" href="../../bayesian/linear_regression/">Many Ways to Linear Regression</a></li><li><a class="tocitem" href="../../bayesian/tilapia_alometry/">Alometry law for the Nile Tilapia</a></li><li><a class="tocitem" href="../../bayesian/mortality_tables/">Modeling mortality tables</a></li></ul></li></ul></li><li><span class="tocitem">Generative models</span><ul><li><input class="collapse-toggle" id="menuitem-6-1" type="checkbox" checked/><label class="tocitem" for="menuitem-6-1"><span class="docs-label">Score matching</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../overview/">Overview</a></li><li><a class="tocitem" href="../stein_score/">Stein score function</a></li><li class="is-active"><a class="tocitem" href>Score matching of Aapo Hyvärinen</a><ul class="internal"><li><a class="tocitem" href="#Introduction"><span>Introduction</span></a></li><li><a class="tocitem" href="#The-score-function"><span>The score function</span></a></li><li><a class="tocitem" href="#Loss-functions-for-score-matching"><span>Loss functions for score matching</span></a></li><li><a class="tocitem" href="#Concerning-the-gradient-in-the-loss-function"><span>Concerning the gradient in the loss function</span></a></li><li><a class="tocitem" href="#Numerical-example"><span>Numerical example</span></a></li><li><a class="tocitem" href="#Conclusion"><span>Conclusion</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../score_matching_neural_network/">Score matching a neural network</a></li><li><a class="tocitem" href="../parzen_estimation_score_matching/">Score matching with Parzen estimation</a></li><li><a class="tocitem" href="../denoising_score_matching/">Denoising score matching of Pascal Vincent</a></li><li><a class="tocitem" href="../sliced_score_matching/">Sliced score matching</a></li><li><a class="tocitem" href="../1d_FD_score_matching/">1D finite-difference score matching</a></li><li><a class="tocitem" href="../2d_FD_score_matching/">2D finite-difference score matching</a></li><li><a class="tocitem" href="../ddpm/">Denoising diffusion probabilistic models</a></li><li><a class="tocitem" href="../mdsm/">Multiple denoising score matching</a></li><li><a class="tocitem" href="../probability_flow/">Probability flow</a></li><li><a class="tocitem" href="../reverse_flow/">Reverse probability flow</a></li><li><a class="tocitem" href="../score_based_sde/">Score-based SDE model</a></li></ul></li></ul></li><li><span class="tocitem">Sensitivity analysis</span><ul><li><a class="tocitem" href="../../sensitivity/overview/">Overview</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Generative models</a></li><li><a class="is-disabled">Score matching</a></li><li class="is-active"><a href>Score matching of Aapo Hyvärinen</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Score matching of Aapo Hyvärinen</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/rmsrosa/random_notes" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/rmsrosa/random_notes/blob/main/docs/src/generative/score_matching_aapo.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Score-matching-of-Aapo-Hyvärinen"><a class="docs-heading-anchor" href="#Score-matching-of-Aapo-Hyvärinen">Score matching of Aapo Hyvärinen</a><a id="Score-matching-of-Aapo-Hyvärinen-1"></a><a class="docs-heading-anchor-permalink" href="#Score-matching-of-Aapo-Hyvärinen" title="Permalink"></a></h1><h2 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h2><h3 id="Aim"><a class="docs-heading-anchor" href="#Aim">Aim</a><a id="Aim-1"></a><a class="docs-heading-anchor-permalink" href="#Aim" title="Permalink"></a></h3><p>Here we revisit the original score-matching method of <a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Aapo Hyvärinen (2005)</a> and apply it to fit a normal distribution to a sample of a univariate random variable just for illustrative purposes.</p><h3 id="Motivation"><a class="docs-heading-anchor" href="#Motivation">Motivation</a><a id="Motivation-1"></a><a class="docs-heading-anchor-permalink" href="#Motivation" title="Permalink"></a></h3><p>The motivation is to revisit the original idea of <a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Aapo Hyvärinen (2005)</a>, as a first step towards building a solid background on score-matching diffusion.</p><h3 id="Background"><a class="docs-heading-anchor" href="#Background">Background</a><a id="Background-1"></a><a class="docs-heading-anchor-permalink" href="#Background" title="Permalink"></a></h3><p>Generative score-matching diffusion methods use Langevin dynamics to draw samples from a modeled score function. It rests on the idea of <a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Aapo Hyvärinen (2005)</a> that one can directly <em>fit</em> the score function from the sample data, using a suitable <strong>implicit score matching</strong> loss function not depending on the unknown score function of the random variable. This loss function is obtained by a simple integration by parts on the <strong>explicit score matching</strong> objective function given by the expected square distance between the score of the model and score of the unknown target distribution, also known as the <em>Fisher divergence.</em> The integration by parts separates the dependence on the unknown target score function from the parameters of the model, so the fitting process (minimization over the parameters of the model) does not depend on the unknown distribution.</p><p>It is worth noticing, in light of the main objective of score-matching diffusion, that the original work of <a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Aapo Hyvärinen (2005)</a> has no diffusion. It is a direct modeling of the score function in the original probability space. But this is a fundamental work.</p><p>We also mention that the work of <a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Aapo Hyvärinen (2005)</a> uses the modified loss function to fit some very specific predefined models. There are three examples. In these examples, the gradient of the model can be computed somewhat more explicitly. There is no artificial neural network involved and no need for automatic differention (AD) (those were proposed in subsequent works, as we will see).</p><p>In a subsequent work, <a href="https://doi.org/10.1162/neco_a_00010">Köster and Hyvärinen (2010)</a> applied the method to fit the score function from a model probability with log-likelihood obtained from a two-layer neural network, but in this case the gradient of the score function could still be expressed somehow explicitly.</p><p>With that in mind, we illustrate this approach by fitting a Gaussian distribution to samples of a univariate radom variables.</p><h2 id="The-score-function"><a class="docs-heading-anchor" href="#The-score-function">The score function</a><a id="The-score-function-1"></a><a class="docs-heading-anchor-permalink" href="#The-score-function" title="Permalink"></a></h2><p>For the theoretical discussion, we denote the PDF of a multivariate random variable <span>$\mathbf{X}$</span>, with values in <span>$\mathbb{R}^d$</span>, <span>$d\in\mathbb{N}$</span>, by <span>$p_\mathbf{X}(\mathbf{x})$</span> and the score function by</p><p class="math-container">\[    \boldsymbol{\psi}_{\mathbf{X}}(\mathbf{x}) = \boldsymbol{\nabla}_{\mathbf{x}}\log(p_\mathbf{X}(\mathbf{x})) = \left( \frac{\partial}{\partial x_i} \log(p_\mathbf{X}(\mathbf{x}))\right)_{i=1, \ldots, d},\]</p><p>which is a vector field in <span>$\mathbb{R}^d$</span>.</p><p>The parametrized modeled score function is denoted by </p><p class="math-container">\[    \boldsymbol{\psi}(\mathbf{x}; \boldsymbol{\theta}) = \boldsymbol{\nabla}_{\mathbf{x}}p(\mathbf{x}; \boldsymbol{\theta}) = \left( \frac{\partial}{\partial x_j} p(\mathbf{x}; \boldsymbol{\theta})\right)_{j=1, \ldots, d},\]</p><p>with parameter values <span>$\boldsymbol{\theta}$</span>.</p><h2 id="Loss-functions-for-score-matching"><a class="docs-heading-anchor" href="#Loss-functions-for-score-matching">Loss functions for score matching</a><a id="Loss-functions-for-score-matching-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-functions-for-score-matching" title="Permalink"></a></h2><p>The score-matching method of <a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Aapo Hyvärinen (2005)</a> rests on the idea of rewriting the <strong>explicit score matching</strong> loss function <span>$J_{\mathrm{ESM}}({\boldsymbol{\theta}})$</span> in terms of the <strong>implicit score matching</strong> loss function <span>$J_{\mathrm{ISM}}({\boldsymbol{\theta}})$</span> and then approximating the latter by the <strong>empirical implicit score matching</strong> loss function <span>${\tilde J}_{\mathrm{ISM}{\tilde p}_0}({\boldsymbol{\theta}})$</span>, with</p><p class="math-container">\[J_{\mathrm{ESM}}({\boldsymbol{\theta}}) = J_{\mathrm{ISM}}({\boldsymbol{\theta}}) + C \approx {\tilde J}_{\mathrm{ISM}{\tilde p}_0}({\boldsymbol{\theta}}) + C,\]</p><p>for a <em>constant</em> <span>$C$</span> (with respect to the parameters <span>$\boldsymbol{\theta}$</span> of the model), so that the optimization process has (approximately) the same gradients</p><p class="math-container">\[\boldsymbol{\nabla}_{\boldsymbol{\theta}} J_{\mathrm{ESM}}({\boldsymbol{\theta}}) = \boldsymbol{\nabla}_{\boldsymbol{\theta}} J_{\mathrm{ISM}}({\boldsymbol{\theta}}) \approx \boldsymbol{\nabla}_{\boldsymbol{\theta}} {\tilde J}_{\mathrm{ISM}{\tilde p}_0}({\boldsymbol{\theta}}).\]</p><p>More precisly, the idea of the score-matching method is as follows.</p><p><strong>1. Start with the explicit score matching</strong></p><p>Fit the model by minimizing the expected square distance between the score function of the model, <span>$\boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}}),$</span> and the actual score function <span>$\boldsymbol{\psi}_{\mathbf{X}}(\mathbf{x})$</span>, which is termed <strong>explicit score matching (ESM),</strong></p><p class="math-container">\[    J_{\mathrm{ESM}}({\boldsymbol{\theta}}) = \frac{1}{2}\int_{\mathbb{R}^d} p_{\mathbf{X}}(\mathbf{x}) \left\|\boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}}) - \boldsymbol{\psi}_{\mathbf{X}}(\mathbf{x})\right\|^2\;\mathrm{d}\mathbf{x}.\]</p><p>Since the score function is the gradient of the logpdf, this is connected with the Fisher divergence</p><p class="math-container">\[    F(p_{\mathbf{X}}, p_{\boldsymbol{\theta}}) = \int_{\mathbb{R}^d} p_{\mathbf{X}}(\mathbf{x}) \left\| \nabla_{\mathbf{x}}\log p_{\mathbf{X}}(\mathbf{x}) - \nabla_{\mathbf{x}}\log p(\mathbf{x}; \boldsymbol{\theta})\right\|^2 \;\mathrm{d}\mathbf{x},\]</p><p>except that the modeled score function may not be exactly the gradient of a probability density function (the constraint of being the gradient of a function might not be valid for some models such as the usual neural networks).</p><p><strong>2. Rewrite it with the implicit score matching</strong></p><p>Use integration by parts in the expectation to write that</p><p class="math-container">\[    J_{\mathrm{ESM}}({\boldsymbol{\theta}}) = J_{\mathrm{ISM}}({\boldsymbol{\theta}}) + C,\]</p><p>where <span>$C$</span> is constant with respect to the parameters, so we only need to minimize <span>${\tilde J}_{\mathrm{ISM}}$</span>, given by</p><p class="math-container">\[    J_{\mathrm{ISM}}({\boldsymbol{\theta}}) = \int_{\mathbb{R}} p_{\mathbf{X}}(\mathbf{x}) \left( \frac{1}{2}\left\|\boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}})\right\|^2 + \boldsymbol{\nabla}_{\mathbf{x}} \cdot \boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}}) \right)\;\mathrm{d}\mathbf{x},\]</p><p>which does not involve the unknown score function of <span>${\mathbf{X}}$</span>. This is called <strong>implicit score matching (ISM).</strong></p><p>Notice the two functions have the same gradient, hence the minimization is, theoretically, the same (apart from the approximation with the empirical distribution and the different round-off errors). This implicit score matching loss function, however, involves the gradient of the modeled score function, which might be expensive to compute.</p><p><strong>3. Approximate it with the empirical implicit score matching</strong></p><p>In practice, the implicit score-matching loss function, which depends on the unknown <span>$p_\mathbf{X}(\mathbf{x})$</span>, is estimated via the empirical distribution, obtained from the sample data <span>$(\mathbf{x}_n)_{n=1}^N$</span>. Thus, we minimize</p><p class="math-container">\[    {\tilde J}_{\mathrm{ISM}{\tilde p}_0} =  \frac{1}{N}\sum_{n=1}^N \left( \frac{1}{2}\|\boldsymbol{\psi}(\mathbf{x}_n; {\boldsymbol{\theta}})\|^2 + \boldsymbol{\nabla}_{\mathbf{x}} \cdot \boldsymbol{\psi}(\mathbf{x}_n; {\boldsymbol{\theta}}) \right).\]</p><p>where the <em>empirical distribution</em> is given by</p><p class="math-container">\[    {\tilde p}_0 = \frac{1}{N} \sum_{n=1}^N \delta_{\mathbf{x}_n}.\]</p><p>Therefore, we call this the <strong>empirical implicit score matching</strong>.</p><h2 id="Concerning-the-gradient-in-the-loss-function"><a class="docs-heading-anchor" href="#Concerning-the-gradient-in-the-loss-function">Concerning the gradient in the loss function</a><a id="Concerning-the-gradient-in-the-loss-function-1"></a><a class="docs-heading-anchor-permalink" href="#Concerning-the-gradient-in-the-loss-function" title="Permalink"></a></h2><p>As mentioned before, computing a derivative to form the loss function becomes expensive when combined with the usual optimization methods to fit a neural network, as they require the gradient of the loss function itself, i.e. the optimization process involves the gradient of the gradient of something. Because of that, other methods are developed, such as using kernel density estimation, auto-encoders, finite-differences, and so on. We will explore them in due course. For the moment, we will just sketch the proof of <span>$J_{\mathrm{ESM}}({\boldsymbol{\theta}}) = J_{\mathrm{ISM}}({\boldsymbol{\theta}}) + C$</span> and apply the method to models for which the gradient can be computed more explicitly.</p><h3 id="Proof-that-J_{\\mathrm{ESM}}({\\boldsymbol{\\theta}})-J_{\\mathrm{ISM}}({\\boldsymbol{\\theta}})-C"><a class="docs-heading-anchor" href="#Proof-that-J_{\\mathrm{ESM}}({\\boldsymbol{\\theta}})-J_{\\mathrm{ISM}}({\\boldsymbol{\\theta}})-C">Proof that <span>$J_{\mathrm{ESM}}({\boldsymbol{\theta}}) = J_{\mathrm{ISM}}({\boldsymbol{\theta}}) + C$</span></a><a id="Proof-that-J_{\\mathrm{ESM}}({\\boldsymbol{\\theta}})-J_{\\mathrm{ISM}}({\\boldsymbol{\\theta}})-C-1"></a><a class="docs-heading-anchor-permalink" href="#Proof-that-J_{\\mathrm{ESM}}({\\boldsymbol{\\theta}})-J_{\\mathrm{ISM}}({\\boldsymbol{\\theta}})-C" title="Permalink"></a></h3><p>We separate the one-dimensional from the multi-dimensional case for the sake of clarity.</p><h4 id="One-dimensional-case"><a class="docs-heading-anchor" href="#One-dimensional-case">One-dimensional case</a><a id="One-dimensional-case-1"></a><a class="docs-heading-anchor-permalink" href="#One-dimensional-case" title="Permalink"></a></h4><p>We start with the one-dimensional version of the proof from <a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Aapo Hyvärinen (2005)</a>. In this case,</p><p class="math-container">\[    J_{\mathrm{ISM}}({\boldsymbol{\theta}}) = \int_{\mathbb{R}} p_X(x) \left( \frac{1}{2}\psi(x; {\boldsymbol{\theta}})^2 + \frac{\partial}{\partial x} \psi(x; {\boldsymbol{\theta}}) \right)\;\mathrm{d}x.\]</p><p>Since this is a one-dimensional problem, the score function is a scalar and we have</p><p class="math-container">\[    \|\psi(x; {\boldsymbol{\theta}}) - \psi_X(x)\|^2 = \psi(x; {\boldsymbol{\theta}})^2 - 2\psi(x; {\boldsymbol{\theta}}) \psi_X(x) + \psi_X(x)^2.\]</p><p>Thus</p><p class="math-container">\[    J_{\mathrm{ESM}}({\boldsymbol{\theta}}) = \frac{1}{2}\int_{\mathbb{R}} p_X(x) \left(\psi(x; {\boldsymbol{\theta}})^2 - 2\psi(x; {\boldsymbol{\theta}})\psi_X(x)\right)\;\mathrm{d}x + C,\]</p><p>where</p><p class="math-container">\[    C = \frac{1}{2}\int_{\mathbb{R}} p_X(x) \psi_X(x)^2\;\mathrm{d}x\]</p><p>does not depend on <span>${\boldsymbol{\theta}}$</span>.</p><p>For the mixed term, we use that the score function is</p><p class="math-container">\[    \psi_X(x) = \frac{\mathrm{d}}{\mathrm{d}x}\log(p_X(x)).\]</p><p>Differentiating the logarithm and using integration by parts, we find</p><p class="math-container">\[\begin{align*}
    -\int_{\mathbb{R}} p_X(x) \psi(x; {\boldsymbol{\theta}})\psi_X(x)\;\mathrm{d}x &amp; = -\int_{\mathbb{R}} p_X(x) \psi(x; {\boldsymbol{\theta}})\frac{\mathrm{d}}{\mathrm{d}x}\log(p_X(x))\;\mathrm{d}x \\
    &amp; = -\int_{\mathbb{R}} p_X(x) \psi(x; {\boldsymbol{\theta}})\frac{1}{p_X(x)}\frac{\mathrm{d}}{\mathrm{d}x}p_X(x)\;\mathrm{d}x \\
    &amp; = -\int_{\mathbb{R}} \psi(x; {\boldsymbol{\theta}})\frac{\mathrm{d}}{\mathrm{d}x}p_X(x)\;\mathrm{d}x \\
    &amp; = \int_{\mathbb{R}} \frac{\partial}{\partial x}\psi(x; {\boldsymbol{\theta}})p_X(x)\;\mathrm{d}x.
\end{align*}\]</p><p>Thus, we rewrite <span>$J_{\mathrm{ESM}}({\boldsymbol{\theta}})$</span> as</p><p class="math-container">\[    J_{\mathrm{ESM}}({\boldsymbol{\theta}}) = \int_{\mathbb{R}} p_X(x) \left(\frac{1}{2}\psi(x; {\boldsymbol{\theta}})^2 + \frac{\partial}{\partial x}\psi(x; {\boldsymbol{\theta}})\right)\;\mathrm{d}x + C,\]</p><p>which is precisely <span>$J_{\mathrm{ESM}}({\boldsymbol{\theta}}) = J_{\mathrm{ISM}}({\boldsymbol{\theta}}) + C$</span>.</p><p>For this proof to be justified, we need the constant to be finite,</p><p class="math-container">\[    C = \frac{1}{2}\int_{\mathbb{R}} p_X(x) \psi_X(x)^2\;\mathrm{d}x &lt; \infty;\]</p><p>the score function of the model not to grow too fast at infinity,</p><p class="math-container">\[    \psi(x; {\boldsymbol{\theta}}) p_X(x) \rightarrow 0, \quad |x| \rightarrow \infty,\]</p><p>for every value <span>${\boldsymbol{\theta}}$</span> of the parameter; and the score function of the model to be smooth everywhere on the support of the distribution, again for every value of the parameter.</p><h4 id="Multi-dimensional-case"><a class="docs-heading-anchor" href="#Multi-dimensional-case">Multi-dimensional case</a><a id="Multi-dimensional-case-1"></a><a class="docs-heading-anchor-permalink" href="#Multi-dimensional-case" title="Permalink"></a></h4><p>For the multi-dimensional version of the proof, we have</p><p class="math-container">\[    \|\boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}}) - \boldsymbol{\psi}_{\mathbf{X}}(\mathbf{x})\|^2 = \|\boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}})\|^2 - 2\boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}}) \cdot \boldsymbol{\psi}_{\mathbf{X}}(\mathbf{x}) + \|\boldsymbol{\psi}_{\mathbf{X}}(\mathbf{x})\|^2.\]</p><p>Thus,</p><p class="math-container">\[    J_{\mathrm{ESM}}({\boldsymbol{\theta}}) = \frac{1}{2}\int_{\mathbb{R}} p_{\mathbf{X}}(\mathbf{x}) \left(\|\boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}})\|^2 - 2\boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}})\boldsymbol{\psi}_{\mathbf{X}}(\mathbf{x})\right)\;\mathrm{d}\mathbf{x} + C,\]</p><p>where</p><p class="math-container">\[    C = \frac{1}{2}\int_{\mathbb{R}} p_{\mathbf{X}}(\mathbf{x}) \boldsymbol{\psi}_{\mathbf{X}}(\mathbf{x})^2\;\mathrm{d}\mathbf{x}\]</p><p>does not depend on <span>${\boldsymbol{\theta}}$</span>.</p><p>For the middle term, we use explicitly that the score function is the gradient of the log of the pdf of the distribution,</p><p class="math-container">\[    \boldsymbol{\psi}_{\mathbf{X}}(\mathbf{x}) = \boldsymbol{\nabla}_{\mathbf{x}}\log(p_{\mathbf{X}}(\mathbf{x})).\]</p><p>Differentiating the logarithm and using the Divergence Theorem for the integration by parts, we find</p><p class="math-container">\[\begin{align*}
    -\int_{\mathbb{R}} p_{\mathbf{X}}(\mathbf{x}) \boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}}) \cdot \boldsymbol{\psi}_{\mathbf{X}}(\mathbf{x})\;\mathrm{d}\mathbf{x} &amp; = -\int_{\mathbb{R}} p_{\mathbf{X}}(\mathbf{x}) \boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}})\boldsymbol{\nabla}_{\mathbf{x}}\log(p_{\mathbf{X}}(\mathbf{x}))\;\mathrm{d}\mathbf{x} \\
    &amp; = -\int_{\mathbb{R}} p_{\mathbf{X}}(\mathbf{x}) \boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}})\frac{1}{p_{\mathbf{X}}(x)}\boldsymbol{\nabla}_{\mathbf{x}}p_{\mathbf{X}}(\mathbf{x})\;\mathrm{d}\mathbf{x} \\
    &amp; = -\int_{\mathbb{R}} \boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}})\boldsymbol{\nabla}_{\mathbf{x}}p_{\mathbf{X}}(\mathbf{x})\;\mathrm{d}\mathbf{x} \\
    &amp; = \int_{\mathbb{R}} \boldsymbol{\nabla}_{\mathbf{x}} \cdot \boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}})p_{\mathbf{X}}(\mathbf{x})\;\mathrm{d}\mathbf{x}.
\end{align*}\]</p><p>Thus, we rewrite <span>$J_{\mathrm{ESM}}({\boldsymbol{\theta}})$</span> as</p><p class="math-container">\[    J_{\mathrm{ESM}}({\boldsymbol{\theta}}) = \int_{\mathbb{R}} p_{\mathbf{X}}(\mathbf{x}) \left(\frac{1}{2}\|\boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}})\|^2 + \boldsymbol{\nabla}_{\mathbf{x}} \cdot \boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}})\right)\;\mathrm{d}\mathbf{x} + C,\]</p><p>which is precisely <span>$J_{\mathrm{ESM}}({\boldsymbol{\theta}}) = J_{\mathrm{ISM}}({\boldsymbol{\theta}}) + C$</span>.</p><p>Similarly to the one-dimensional case, for this proof to be justified, we need the constant to be finite,</p><p class="math-container">\[    C = \frac{1}{2}\int_{\mathbb{R}} p_{\mathbf{X}}(\mathbf{x}) \boldsymbol{\psi}_{\mathbf{X}}(\mathbf{x})^2\;\mathrm{d}\mathbf{x} &lt; \infty;\]</p><p>the score function of the model not to grow too fast at infinity,</p><p class="math-container">\[    \boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}}) p_{\mathbf{X}}(\mathbf{x}) \rightarrow \mathbf{0}, \quad |\mathbf{x}| \rightarrow \infty,\]</p><p>for every value <span>${\boldsymbol{\theta}}$</span> of the parameter; and the score function of the model to be smooth everywhere on the support of the distribution, again for every value of the parameter.</p><h4 id="About-the-conditions-on-the-model-function"><a class="docs-heading-anchor" href="#About-the-conditions-on-the-model-function">About the conditions on the model function</a><a id="About-the-conditions-on-the-model-function-1"></a><a class="docs-heading-anchor-permalink" href="#About-the-conditions-on-the-model-function" title="Permalink"></a></h4><p>The conditions on the smoothness and on the growth of the score function of the model distribution are usually fine for the common neural network models when using smooth and uniformly bounded activation functions. Piecewise smooth and/or growing activation functions might fail these requirements, depending on the unkown target distribution.</p><h2 id="Numerical-example"><a class="docs-heading-anchor" href="#Numerical-example">Numerical example</a><a id="Numerical-example-1"></a><a class="docs-heading-anchor-permalink" href="#Numerical-example" title="Permalink"></a></h2><p>We exemplify the score-matching method fitting a Normal distribution to a synthetic univariate random variable <span>$X$</span>. We take as the target model a Gaussian mixture with the corresponding means relatively close to each other so that fitting a single Normal distribution to them is not too far off.</p><p>Say we have a sample <span>$\{x_n\}_{n=1}^N$</span> of <span>$X$</span>, where <span>$N\in\mathbb{N}$</span>.</p><img src="data:image/png;base64,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" /><p>The model is a score function of a Gaussian distribution <span>$\mathcal{N}(\mu, \sigma^2)$</span>, whose PDF is</p><p class="math-container">\[    p_{\theta}(x) = p(x; \mu, \sigma) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2},\]</p><p>with parameters <span>$\boldsymbol{\theta} = (\mu, \sigma)$</span>. The logpdf is</p><p class="math-container">\[    \log p(x; \mu, \sigma) = -\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2 - \log\left(\sqrt{2\pi}\sigma\right).\]</p><p>And the score function is</p><p class="math-container">\[    \psi(x; \mu, \sigma) = \frac{\partial}{\partial x} p(x; \mu, \sigma) = - \frac{x - \mu}{\sigma^2},\]</p><p>The derivative of the score function, needed for the loss function, is constant with respect to <span>$x$</span>, but depends on the parameter <span>$\sigma$</span>,</p><p class="math-container">\[    \frac{\partial}{\partial x} \psi(x; \mu, \theta) = -\frac{1}{\sigma^2}.\]</p><p>Thus, the implicit score matching loss becomes</p><p class="math-container">\[    J_{\mathrm{ISM}}({\boldsymbol{\theta}}) = {\tilde J}_{\mathrm{ISM}}(\mu, \sigma) = \int_{\mathbb{R}} p_X(x) \left( \frac{1}{2}\left(\frac{x - \mu}{\sigma^2}\right)^2 - \frac{1}{\sigma^2} \right)\;\mathrm{d}x.\]</p><p>The approximation with the empirical distribution is</p><p class="math-container">\[    {\tilde J}_{\mathrm{ISM}{\tilde p}_0}({\boldsymbol{\theta}}) = {\tilde J}_{\mathrm{ISM}{\tilde p}_0}(\mu, \sigma) = \frac{1}{N} \sum_{n=1}^N \left( \frac{1}{2}\left(\frac{x_n - \mu}{\sigma^2}\right)^2 - \frac{1}{\sigma^2} \right).\]</p><p>Computing this loss for the given model yields the following plot over a reasonable range of values for <span>$\mu$</span> and <span>$\sigma$</span>.</p><img src="52ff1e3d.svg" alt="Example block output"/><img src="91faecfa.svg" alt="Example block output"/><p>In this example, the optimal parameters can be found numerically to be approximately <span>$\mu = 3.2$</span> and <span>$\sigma = 1.5$</span>, or more precisely,</p><pre class="documenter-example-output"><code class="nohighlight hljs ansi">μ = 3.0819, σ = 1.4045</code></pre><p>We do not actually perform a minimization in this case. We simply sweep the values computed for the previous two plots and find the location of the smallest one. This is good enough for this illustrative example.</p><p>With that approximate minimizer, we have our modeled Normal distribution fitting the sample. The result can be visualized as follows.</p><img src="5274566e.svg" alt="Example block output"/><h2 id="Conclusion"><a class="docs-heading-anchor" href="#Conclusion">Conclusion</a><a id="Conclusion-1"></a><a class="docs-heading-anchor-permalink" href="#Conclusion" title="Permalink"></a></h2><p>This concludes our review of <a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Aapo Hyvärinen (2005)</a> and illustrates the use of <em>empirical implicit score matching</em> to model a univariate random variable by a closed-form model.</p><p>The work of <a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Aapo Hyvärinen (2005)</a> has some more elaborate models, namely a i) multivariate Gaussian model; a ii) basic independent component analysis model; and an iii) overcomplete model for image data.</p><p>As we mentioned earlier, our interest, however, is on modeling directly the score function using a neural network and for which the gradient needs to be handled properly. For that, other techniques were developed, which will be examined next.</p><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><ol><li><a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Aapo Hyvärinen (2005), &quot;Estimation of non-normalized statistical models by score matching&quot;, Journal of Machine Learning Research 6, 695-709</a></li><li><a href="https://doi.org/10.1162/neco_a_00010">U. Köster, A. Hyvärinen (2010), &quot;A two-layer model of natural stimuli estimated with score matching&quot;, Neural. Comput. 22 (no. 9), 2308-33, doi: 10.1162/NECO<em>a</em>00010</a></li></ol></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../stein_score/">« Stein score function</a><a class="docs-footer-nextpage" href="../score_matching_neural_network/">Score matching a neural network »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.15.0 on <span class="colophon-date" title="Thursday 6 November 2025 15:39">Thursday 6 November 2025</span>. Using Julia version 1.12.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
