<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Probability flow · Random notes</title><meta name="title" content="Probability flow · Random notes"/><meta property="og:title" content="Probability flow · Random notes"/><meta property="twitter:title" content="Probability flow · Random notes"/><meta name="description" content="Documentation for Random notes."/><meta property="og:description" content="Documentation for Random notes."/><meta property="twitter:description" content="Documentation for Random notes."/><meta property="og:url" content="https://github.com/rmsrosa/random_notes/generative/probability_flow/"/><meta property="twitter:url" content="https://github.com/rmsrosa/random_notes/generative/probability_flow/"/><link rel="canonical" href="https://github.com/rmsrosa/random_notes/generative/probability_flow/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/style.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="Random notes logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Random notes</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Random Notes</a></li><li><span class="tocitem">Probability Essentials</span><ul><li><a class="tocitem" href="../../probability/kernel_density_estimation/">Kernel Density Estimation</a></li><li><a class="tocitem" href="../../probability/convergence_notions/">Convergence notions</a></li></ul></li><li><span class="tocitem">Discrete-time Markov chains</span><ul><li><a class="tocitem" href="../../markov_chains/mc_definitions/">Essential definitions</a></li><li><a class="tocitem" href="../../markov_chains/mc_invariance/">Invariant distributions</a></li><li><input class="collapse-toggle" id="menuitem-3-3" type="checkbox"/><label class="tocitem" for="menuitem-3-3"><span class="docs-label">Countable-space Markov chains</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../markov_chains/mc_countableX_recurrence/">Recurrence in the countable-space case</a></li><li><a class="tocitem" href="../../markov_chains/mc_countableX_connections/">Connected states, irreducibility and uniqueness of invariant measures</a></li><li><a class="tocitem" href="../../markov_chains/mc_countableX_convergencia/">Aperiodicidade e convergência para a distribuição estacionária</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-4" type="checkbox"/><label class="tocitem" for="menuitem-3-4"><span class="docs-label">Continuous-space Markov chains</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../markov_chains/mc_irreducibility_and_recurrence/">Irreducibility and recurrence in the continuous-space case</a></li></ul></li></ul></li><li><span class="tocitem">Sampling methods</span><ul><li><a class="tocitem" href="../../sampling/overview/">Overview</a></li><li><a class="tocitem" href="../../sampling/prng/">Random number generators</a></li><li><input class="collapse-toggle" id="menuitem-4-3" type="checkbox"/><label class="tocitem" for="menuitem-4-3"><span class="docs-label">Transform methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/invFtransform/">Probability integral transform</a></li><li><a class="tocitem" href="../../sampling/box_muller/">Box-Muller transform</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-4" type="checkbox"/><label class="tocitem" for="menuitem-4-4"><span class="docs-label">Accept-Reject methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/rejection_sampling/">Rejection sampling</a></li><li><a class="tocitem" href="../../sampling/empiricalsup_rejection/">Empirical supremum rejection sampling</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-5" type="checkbox"/><label class="tocitem" for="menuitem-4-5"><span class="docs-label">Markov Chain Monte Carlo (MCMC)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/mcmc/">Overview</a></li><li><a class="tocitem" href="../../sampling/metropolis/">Metropolis and Metropolis-Hastings</a></li><li><a class="tocitem" href="../../sampling/convergence_metropolis/">Convergence of Metropolis-Hastings</a></li><li><a class="tocitem" href="../../sampling/gibbs/">Gibbs sampling</a></li><li><a class="tocitem" href="../../sampling/hmc/">Hamiltonian Monte Carlo (HMC)</a></li></ul></li><li><a class="tocitem" href="../../sampling/langevin_sampling/">Langevin sampling</a></li></ul></li><li><span class="tocitem">Bayesian inference</span><ul><li><input class="collapse-toggle" id="menuitem-5-1" type="checkbox"/><label class="tocitem" for="menuitem-5-1"><span class="docs-label">Bayes Theory</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/bayes/">Bayes Theorem</a></li><li><a class="tocitem" href="../../bayesian/bayes_inference/">Bayesian inference</a></li><li><a class="tocitem" href="../../bayesian/bernstein_vonmises/">Bernstein–von Mises theorem</a></li></ul></li><li><a class="tocitem" href="../../bayesian/bayesian_probprog/">Bayesian probabilistic programming</a></li><li><input class="collapse-toggle" id="menuitem-5-3" type="checkbox"/><label class="tocitem" for="menuitem-5-3"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/find_pi/">Estimating π via frequentist and Bayesian methods</a></li><li><a class="tocitem" href="../../bayesian/linear_regression/">Many Ways to Linear Regression</a></li><li><a class="tocitem" href="../../bayesian/tilapia_alometry/">Alometry law for the Nile Tilapia</a></li><li><a class="tocitem" href="../../bayesian/mortality_tables/">Modeling mortality tables</a></li></ul></li></ul></li><li><span class="tocitem">Generative models</span><ul><li><input class="collapse-toggle" id="menuitem-6-1" type="checkbox" checked/><label class="tocitem" for="menuitem-6-1"><span class="docs-label">Score matching</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../overview/">Overview</a></li><li><a class="tocitem" href="../stein_score/">Stein score function</a></li><li><a class="tocitem" href="../score_matching_aapo/">Score matching of Aapo Hyvärinen</a></li><li><a class="tocitem" href="../score_matching_neural_network/">Score matching a neural network</a></li><li><a class="tocitem" href="../parzen_estimation_score_matching/">Score matching with Parzen estimation</a></li><li><a class="tocitem" href="../denoising_score_matching/">Denoising score matching of Pascal Vincent</a></li><li><a class="tocitem" href="../sliced_score_matching/">Sliced score matching</a></li><li><a class="tocitem" href="../1d_FD_score_matching/">1D finite-difference score matching</a></li><li><a class="tocitem" href="../2d_FD_score_matching/">2D finite-difference score matching</a></li><li><a class="tocitem" href="../ddpm/">Denoising diffusion probabilistic models</a></li><li><a class="tocitem" href="../mdsm/">Multiple denoising score matching</a></li><li class="is-active"><a class="tocitem" href>Probability flow</a><ul class="internal"><li><a class="tocitem" href="#Aim"><span>Aim</span></a></li><li><a class="tocitem" href="#Background"><span>Background</span></a></li><li><a class="tocitem" href="#Probability-flow-(random)-ODEs"><span>Probability flow (random) ODEs</span></a></li><li><a class="tocitem" href="#Splitted-up-probability-flow-SDE"><span>Splitted-up probability flow SDE</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../reverse_flow/">Reverse probability flow</a></li><li><a class="tocitem" href="../score_based_sde/">Score-based SDE model</a></li></ul></li></ul></li><li><span class="tocitem">Sensitivity analysis</span><ul><li><a class="tocitem" href="../../sensitivity/overview/">Overview</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Generative models</a></li><li><a class="is-disabled">Score matching</a></li><li class="is-active"><a href>Probability flow</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Probability flow</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/rmsrosa/random_notes" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/rmsrosa/random_notes/blob/main/docs/src/generative/probability_flow.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Probability-flow-ODEs-for-Itô-diffusions"><a class="docs-heading-anchor" href="#Probability-flow-ODEs-for-Itô-diffusions">Probability flow ODEs for Itô diffusions</a><a id="Probability-flow-ODEs-for-Itô-diffusions-1"></a><a class="docs-heading-anchor-permalink" href="#Probability-flow-ODEs-for-Itô-diffusions" title="Permalink"></a></h1><h2 id="Aim"><a class="docs-heading-anchor" href="#Aim">Aim</a><a id="Aim-1"></a><a class="docs-heading-anchor-permalink" href="#Aim" title="Permalink"></a></h2><p>The aim is to review the probability flow introduced by <a href="https://doi.org/10.3390/e22080802">Maoutsa, Reich, Opper (2020)</a> and generalized by <a href="https://arxiv.org/abs/2011.13456">Song, Sohl-Dickstein, Kingma, Kumar, Ermon, Poole (2020)</a> and <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/a98846e9d9cc01cfb87eb694d946ce6b-Abstract-Conference.html">Karras, Aittala, Aila, Laine (2022)</a>.</p><h2 id="Background"><a class="docs-heading-anchor" href="#Background">Background</a><a id="Background-1"></a><a class="docs-heading-anchor-permalink" href="#Background" title="Permalink"></a></h2><p><a href="https://arxiv.org/abs/2011.13456">Song, Sohl-Dickstein, Kingma, Kumar, Ermon, Poole (2020)</a> extended the <strong>denoising diffusion probabilistic models (DDPM)</strong> of <a href="https://dl.acm.org/doi/10.5555/3045118.3045358">Sohl-Dickstein, Weiss, Maheswaranathan, Ganguli (2015)</a> and <a href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html">Ho, Jain, and Abbeel (2020)</a> and the <strong>(multiple denoising) score matching with (annealed) Langevin dynamics (SMLD)</strong> of <a href="https://dl.acm.org/doi/10.5555/3454287.3455354">Song and Ermon (2019)</a> to the continuous case. This lead to a noising schedule based on a stochastic differential equation of the form</p><p class="math-container">\[    \mathrm{d}X_t = f(t, X_t)\;\mathrm{d}t + G(t, X_t)\;\mathrm{d}W_t,\]</p><p>for suitable choices of <span>$f=f(t, x)$</span> and <span>$G=G(t, x)$</span> (usually with <span>$f(t, x) = -a(t)x$</span> linear in <span>$x$</span> and with <span>$G(t, x) = g(t)\mathbf{I}$</span> diagonal and only time dependent).</p><p><a href="https://arxiv.org/abs/2011.13456">Song, Sohl-Dickstein, Kingma, Kumar, Ermon, Poole (2020)</a> then showed that the probability density <span>$p(t, x)$</span> of <span>$X_t$</span> can also be obtained with the (random) ODE</p><p class="math-container">\[    \frac{\mathrm{d}Y_t}{\mathrm{d}t} = f(t, Y_t) - \frac{1}{2} \nabla_y \cdot ( G(t, Y_t)G(t, Y_t)^{\mathrm{tr}} ) - \frac{1}{2} G(t, Y_t)G(t, Y_t)^{\mathrm{tr}}\nabla_y \log p(t, Y_t).\]</p><p>This <strong>probability flow ODE,</strong> as so they termed, was based on the work by <a href="https://doi.org/10.3390/e22080802">Maoutsa, Reich, Opper (2020)</a>, who derived this equation in the particular case of a time-independent drift term and a constant diagonal noise factor, i.e. with</p><p class="math-container">\[    f(t, x) = f(x), \qquad G(t, x) = \sigma \mathbf{I}.\]</p><p>Both the SDE for <span>$\{X_t\}_t$</span> and the random ODE for <span>$\{Y_t\}_t$</span> have a reverse-time counterpart, which is then used for sampling, provided the Stein score <span>$\nabla \log p(t, x)$</span> has been properly modeled by a suitable neural network.</p><p>In theory, both formulations are equivalent to each other, in the sense of yielding the same probability density <span>$p(t, x).$</span> In practice, however, their numerical implementations and their Monte Carlo approximations differ considerably. Sampling via the reverse probability flow ODE has some advantages such as the fact that the sample trajectories are smoother and easier to integrate numerically, allowing for higher order schemes with lower computational cost, and that there are less parameters to fiddle with. It is also easier to go back and forth with the ODE. Meanwhile, in some situations and with well chosen parameters, sampling with the reverse SDE seems to somehow generate better sample distributions.</p><p>Notice that we denoted the probability flow ODE with <span>$\{Y_t\}_t$</span> to stress the fact that this is a different process from <span>$\{X_t\}_t$</span> but with the remarkable fact that they have same probability density function <span>$p(t, x).$</span></p><p>Later, <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/a98846e9d9cc01cfb87eb694d946ce6b-Abstract-Conference.html">Karras, Aittala, Aila, Laine (2022)</a> considered a particular type of SDE and obtained a sort of probability flow SDE with two main characteristics: a desired specific variance schedule and a mixture of the original SDE and the probability flow ODE. In essence, the desired variance is simply a reparametrization of the terms of the equation, while the mixture of the SDE and the ODE is just a split-up of the way the diffusion term is handled. More precisely, for the passage from the SDE to the flow ODE, the whole diffusion term is rewritten as a flux. For the mixture, just part of it is rewritten.</p><h2 id="Probability-flow-(random)-ODEs"><a class="docs-heading-anchor" href="#Probability-flow-(random)-ODEs">Probability flow (random) ODEs</a><a id="Probability-flow-(random)-ODEs-1"></a><a class="docs-heading-anchor-permalink" href="#Probability-flow-(random)-ODEs" title="Permalink"></a></h2><p>The probability flow ODEs are actually ordinary differential equations with random initial conditions, which is a special form of a <em>Random ODE (RODE).</em> The main point is that they have the same probability distributions as their original stochastic versions. In what follows, we will build that up in different context.</p><h3 id="With-a-constant-scalar-diagonal-noise-amplitude"><a class="docs-heading-anchor" href="#With-a-constant-scalar-diagonal-noise-amplitude">With a constant scalar diagonal noise amplitude</a><a id="With-a-constant-scalar-diagonal-noise-amplitude-1"></a><a class="docs-heading-anchor-permalink" href="#With-a-constant-scalar-diagonal-noise-amplitude" title="Permalink"></a></h3><p>This is the original result given by <a href="https://doi.org/10.3390/e22080802">Maoutsa, Reich, Opper (2020)</a>. The SDE is the Itô diffusion with a constant scalar diagonal noise term</p><p class="math-container">\[    \mathrm{d}X_t = f(X_t)\;\mathrm{d}t + \sigma\;\mathrm{d}W_t,\]</p><p>where the unknown <span>$\{X_t\}_t$</span> is a vector valued process with values in <span>$\mathbb{R}^d,$</span> <span>$d\in\mathbb{R};$</span> <span>$\{W_t\}_t$</span> is a vector valued process in the same event space <span>$\mathbb{R}^d$</span> with components made of independent Wiener processes; the drift term is a vector field <span>$f:\mathbb{R}^d \rightarrow \mathbb{R}^d;$</span> and <span>$\sigma &gt; 0$</span> is a constant noise amplitude factor.</p><p>In this case, given a initial probability distribution <span>$p_0$</span> for the initial random variable <span>$X_0,$</span> the probability distribution <span>$p(t, x)$</span> for <span>$X_t$</span> is the solution of the Fokker-Planck (Kolmogorov forward) equation</p><p class="math-container">\[    \frac{\partial p}{\partial t} + \nabla_x \cdot (f(x) p(t, x)) = \frac{\sigma^2}{2}\Delta_x p(t, x).\]</p><p>The idea is that the diffusion term can also be expressed as a divergence, namely</p><p class="math-container">\[    \frac{\sigma^2}{2}\Delta_x p(t, x) = \frac{\sigma^2}{2}\nabla_x \cdot \nabla_x p(t, x) = \nabla_x \cdot \left(\frac{\sigma^2}{2}\nabla_x p(t, x)\right).\]</p><p>Another key point is that the gradient above can be written as a multiple of the probability density, with the help of the Stein score function <span>$\nabla_x\log p(t, x).$</span> Indeed, in the region where <span>$p(t, x) &gt; 0,$</span></p><p class="math-container">\[    \nabla_x p(t, x) = p(t, x) \frac{\nabla_x p(t, x)}{p(t, x)} = p(t, x) \nabla_x \log p(t, x).\]</p><p>Then, with the understanding that <span>$s\log s$</span> vanishes for <span>$s = 0$</span>, which extends this function continuously to the region <span>$s\geq 0,$</span> we can assume this relation holds everywhere. Thus, the Fokker-Planck equation takes the form</p><p class="math-container">\[    \frac{\partial p}{\partial t} + \nabla_x \cdot (f(x) p(t, x)) = \nabla_x \cdot (\frac{\sigma^2}{2}p(t, x) \nabla_x \log p(t, x)).\]</p><p>We now rewrite this equation as</p><p class="math-container">\[    \frac{\partial p}{\partial t} + \nabla_x \cdot \left(f(x) p(t, x) - \frac{\sigma^2}{2}p(t, x) \nabla_x \log p(t, x) \right) = 0.\]</p><p>Defining</p><p class="math-container">\[    \tilde f(t, x) = f(x) - \frac{\sigma^2}{2} \nabla_x \log p(t, x),\]</p><p>the Fokker-Planck equation becomes</p><p class="math-container">\[    \frac{\partial p}{\partial t} + \nabla_x \cdot \left(\tilde f(x) p(t, x) \right) = 0,\]</p><p>which can be viewed as the Liouville equation associated with the evolution of a distribution governed by the ODE, with no diffusion,</p><p class="math-container">\[    \mathrm{d}Y_t = \tilde f(t, Y_t)\;\mathrm{d}t,\]</p><p>i.e. just a Random ODE (more specifically an ODE with random initial data) of the form</p><p class="math-container">\[    \frac{\mathrm{d}Y_t}{\mathrm{d}t} = f(t, Y_t) - \frac{\sigma^2}{2} \nabla_y \log p(t, Y_t).\]</p><p>This equation did not receive any special name in the original work <a href="https://doi.org/10.3390/e22080802">Maoutsa, Reich, Opper (2020)</a>, but got the name <strong>probability flow ODE</strong> in the work <a href="https://arxiv.org/abs/2011.13456">Song, Sohl-Dickstein, Kingma, Kumar, Ermon, Poole (2020)</a>, which we discuss next.</p><p>Before that, we remark that one difficulty with the probability flow ODE is that it requires knownledge of the Stein score function <span>$\nabla_x \log p(t, x)$</span> of the supposedly unknown distribution. This is circumvented in <a href="https://doi.org/10.3390/e22080802">Maoutsa, Reich, Opper (2020)</a> by using a gradient log density estimator obtained from samples of the forward diffusion process, i.e. from a maximum likelihood estimation based on the evolution of the empiral distribution of <span>$X_0.$</span></p><p>On the other hand, <a href="https://arxiv.org/abs/2011.13456">Song, Sohl-Dickstein, Kingma, Kumar, Ermon, Poole (2020)</a> models the score function directly as a (trained) neural network.</p><h3 id="With-a-nonlinear-scalar-diagonal-noise-amplitude"><a class="docs-heading-anchor" href="#With-a-nonlinear-scalar-diagonal-noise-amplitude">With a nonlinear scalar diagonal noise amplitude</a><a id="With-a-nonlinear-scalar-diagonal-noise-amplitude-1"></a><a class="docs-heading-anchor-permalink" href="#With-a-nonlinear-scalar-diagonal-noise-amplitude" title="Permalink"></a></h3><p>In <a href="https://arxiv.org/abs/2011.13456">Song, Sohl-Dickstein, Kingma, Kumar, Ermon, Poole (2020)</a>, the authors consider the more general SDE</p><p class="math-container">\[    \mathrm{d}X_t = f(t, X_t)\;\mathrm{d}t + G(t, X_t)\;\mathrm{d}W_t,\]</p><p>where the diffusion factor is not a scalar diagonal anymore but is a  matrix-valued, time-dependent function <span>$G:I\times \mathbb{R}^d \rightarrow \mathbb{R}^{d\times d},$</span> and with the drift term also time dependent, <span>$f:I\times \mathbb{R}^d \rightarrow \mathbb{R}^d,$</span> on an interval of interest <span>$I=[0, T].$</span></p><p>Just for clarity, let us consider first the case in which <span>$G=G(t, x)$</span> is still diagonal but not a constant anymore, i.e.</p><p class="math-container">\[    G(t, x) = g(t, x)\mathbf{I},\]</p><p>so that</p><p class="math-container">\[    \mathrm{d}X_t = f(t, X_t)\;\mathrm{d}t + g(t, X_t)\;\mathrm{d}W_t,\]</p><p>In this case, the Fokker-Planck equation takes the form</p><p class="math-container">\[    \frac{\partial p}{\partial t} + \nabla_x \cdot (f(x) p(t, x)) = \frac{1}{2}\Delta_x (g(t, x)^2 p(t, x)).\]</p><p>As before, the diffusion term can be written as</p><p class="math-container">\[    \begin{align*}
        \frac{1}{2}\Delta_x (g(t, x)^2 p(t, x)) &amp; = \frac{1}{2}\nabla_x \cdot \nabla_x (g(t, x)^2 p(t, x)) \\
        &amp; = \frac{1}{2}\nabla_x \cdot \left( \nabla_x g(t, x)^2 p(t, x) + g(t, x)^2 \nabla_x p(t, x) \right) \\
        &amp; = \frac{1}{2}\nabla_x \cdot \left( \nabla_x g(t, x)^2 p(t, x) + g(t, x)^2 p(t, x) \nabla_x \log p(t, x) \right)
    \end{align*}\]</p><p>Thus, the Fokker-Planck equation becomes</p><p class="math-container">\[    \frac{\partial p}{\partial t} + \nabla_x \cdot \left( f(x) p(t, x) - \frac{1}{2} \nabla_x g(t, x)^2 p(t, x) - g(t, x)^2 p(t, x) \nabla_x \log p(t, x) \right) = 0,\]</p><p>which can also be viewed as the Fokker-Planck equation for the SDE with no diffusion</p><p class="math-container">\[    \mathrm{d}Y_t = \tilde f(t, Y_t)\;\mathrm{d}t,\]</p><p>with</p><p class="math-container">\[    \tilde f(t, x) = f(t, x) - \frac{1}{2} \nabla_x g(t, x)^2 - g(t, x)^2\nabla_x \log p(t, x),\]</p><p>which is actually a random ODE (more specifically an ODE with random initial condition),</p><p class="math-container">\[    \frac{\mathrm{d}Y_t}{\mathrm{d}t} = f(t, Y_t) - \frac{1}{2} \nabla_y g(t, Y_t)^2 - g(t, Y_t)^2\nabla_y \log p(t, Y_t),\]</p><p>with the equation for <span>$p(t, x)$</span> being the associated Liouville equation.</p><h3 id="With-arbitrary-drift-and-diffusion"><a class="docs-heading-anchor" href="#With-arbitrary-drift-and-diffusion">With arbitrary drift and diffusion</a><a id="With-arbitrary-drift-and-diffusion-1"></a><a class="docs-heading-anchor-permalink" href="#With-arbitrary-drift-and-diffusion" title="Permalink"></a></h3><p>Now we address the more general case considered in <a href="https://arxiv.org/abs/2011.13456">Song, Sohl-Dickstein, Kingma, Kumar, Ermon, Poole (2020)</a>, with an SDE of the form</p><p class="math-container">\[    \mathrm{d}X_t = f(t, X_t)\;\mathrm{d}t + G(t, X_t)\;\mathrm{d}W_t,\]</p><p>where the diffusion factor is now a matrix-valued, time-dependent function <span>$G:I\times \mathbb{R}^d \rightarrow \mathbb{R}^{d\times d}.$</span></p><p>In this case, the Fokker-Planck equation takes the form</p><p class="math-container">\[    \frac{\partial p}{\partial t} + \nabla_x \cdot (f(t, x) p(t, x)) = \frac{1}{2}\nabla_x^2 : \left( (G(t, x)G(t, x)^{\mathrm{tr}}) p(t, x)\right).\]</p><p>Notice that the term within the parentheses, on the right hand side, is a tensor field which at each point <span>$(t, x)$</span> yields a certain matrix <span>$A(t, x) = (A_{ij}(t, x))_{i,j=1}^d.$</span> The differential operation <span>$\nabla_x^2 :$</span> acting on such a tensor field is given by</p><p class="math-container">\[    \nabla_x^2 : A(t, x) = \sum_{i=1}^d\sum_{j=1}^d \frac{\partial^2}{\partial x_i\partial x_j} A_{ij}(t, x).\]</p><p>We may write everything in coordinates, starting with</p><p class="math-container">\[    X_t = (X_t^i)_{i=1}^d, \quad W_t = (W_t^i)_{i=1}^d, \quad f(t, x) = (f_i(t, x))_{i=1}^d, \quad G(t, X_t) = (G_{ij}(t, X_t))_{i, j=1}^d,\]</p><p>so that the SDE reads</p><p class="math-container">\[    \mathrm{d}X_t^i = f_i(t, X_t^1, \ldots, X_t^d)\;\mathrm{d}t + \sum_{j=1}^d G_{ij}(t, X_t^1, \ldots, X_t^d)\;\mathrm{d}W_t^j,\]</p><p>and the Fokker-Planck equation reads</p><p class="math-container">\[    \frac{\partial p}{\partial t} + \sum_{i=1}^d \frac{\partial}{\partial x_i} (f(t, x) p(t, x)) = \frac{1}{2}\sum_{i=1}^d \sum_{j=1}^d \frac{\partial^2}{\partial x_i \partial x_j} (G_{ik}(t, x)G_{jk}(t, x) p(t, x)).\]</p><p>Writing the divergence of a tensor field <span>$A(t, x) = (A_{ij}(x))_{i,j=1}^d$</span> as the vector</p><p class="math-container">\[    \nabla_x \cdot A(t, x) = \left( \nabla_x \cdot A_{i\cdot}(t, x)\right)_{i=1}^d = \left( \sum_{j=1}^d \frac{\partial}{\partial x_j} A_{ij}(t, x)\right)_{i=1}^d,\]</p><p>we can write the Fokker-Planck equation in divergence form,</p><p class="math-container">\[    \frac{\partial p}{\partial t} + \nabla_x \cdot (f(t, x) p(t, x)) = \frac{1}{2}\nabla_x \cdot \left(\nabla_x \cdot (G(t, x)G(t, x)^{\mathrm{tr}} p(t, x))\right).\]</p><p>As before, the diffusion term can be written as</p><p class="math-container">\[    \begin{align*}
        \frac{1}{2}\nabla_x \cdot &amp; \left( \nabla_x \cdot (G(t, x)G(t, x)^{\mathrm{tr}} p(t, x)) \right) \\
        &amp; = \frac{1}{2}\nabla_x \cdot \bigg( \nabla_x \cdot ( G(t, x)G(t, x)^{\mathrm{tr}}) p(t, x) + G(t, x)G(t, x)^{\mathrm{tr}}\nabla_x p(t, x) \bigg) \\
        &amp; = \frac{1}{2}\nabla_x \cdot \bigg( \nabla_x \cdot ( G(t, x)G(t, x)^{\mathrm{tr}}) p(t, x) + G(t, x)G(t, x)^{\mathrm{tr}}p(t, x)\nabla_x \log p(t, x) \bigg).
    \end{align*}\]</p><p>With that, the Fokker-Planck equation reads</p><p class="math-container">\[    \begin{align*}
        \frac{\partial p}{\partial t} + &amp; \nabla_x \cdot (f(t, x) p(t, x)) \\
        &amp; = \frac{1}{2}\nabla_x \cdot \left( \nabla_x \cdot ( G(t, x)G(t, x)^{\mathrm{tr}}) p(t, x) + G(t, x)G(t, x)^{\mathrm{tr}}p(t, x)\nabla_x \log p(t, x) \right).
    \end{align*}\]</p><p>Rearranging it, we obtain the following equation</p><p class="math-container">\[    \frac{\partial p}{\partial t} + \nabla_x \cdot \left( \left( f(t, x) - \frac{1}{2} \nabla_x \cdot ( G(t, x)G(t, x)^{\mathrm{tr}} ) - \frac{1}{2} G(t, x)G(t, x)^{\mathrm{tr}}\nabla_x \log p(t, x) \right) p(t, x) \right) = 0,\]</p><p>which can be viewed as the Liouville equation for the random ODE</p><p class="math-container">\[    \frac{\mathrm{d}Y_t}{\mathrm{d}t} = f(t, Y_t) - \frac{1}{2} \nabla_y \cdot ( G(t, Y_t)G(t, Y_t)^{\mathrm{tr}} ) - \frac{1}{2} G(t, Y_t)G(t, Y_t)^{\mathrm{tr}}\nabla_y \log p(t, Y_t).\]</p><p>This is the <strong>probability flow (random) ODE</strong> of <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/a98846e9d9cc01cfb87eb694d946ce6b-Abstract-Conference.html">Karras, Aittala, Aila, Laine (2022)</a> (except for the symbol <span>$\{Y_t\}_t$</span> instead of <span>$\{X_t\}_t$</span>).</p><h2 id="Splitted-up-probability-flow-SDE"><a class="docs-heading-anchor" href="#Splitted-up-probability-flow-SDE">Splitted-up probability flow SDE</a><a id="Splitted-up-probability-flow-SDE-1"></a><a class="docs-heading-anchor-permalink" href="#Splitted-up-probability-flow-SDE" title="Permalink"></a></h2><p>The change from the Fokker-Planck equation</p><p class="math-container">\[    \frac{\partial p}{\partial t} + \nabla_x \cdot (f(t, x) p(t, x)) = \frac{1}{2}\nabla_x^2 : \left( (G(t, x)G(t, x)^{\mathrm{tr}}) p(t, x) \right).\]</p><p>for the SDE</p><p class="math-container">\[    \mathrm{d}X_t = f(t, X_t)\;\mathrm{d}t + G(t, X_t)\;\mathrm{d}W_t,\]</p><p>to the Liouville equation</p><p class="math-container">\[    \frac{\partial p}{\partial t} + \nabla_x \cdot \left( \left( f(t, x) - \frac{1}{2} \nabla_x \cdot ( G(t, x)G(t, x)^{\mathrm{tr}} ) - \frac{1}{2} G(t, x)G(t, x)^{\mathrm{tr}}\nabla_x \log p(t, x) \right) p(t, x) \right) = 0,\]</p><p>for the random ODE</p><p class="math-container">\[    \frac{\mathrm{d}Y_t}{\mathrm{d}t} = f(t, Y_t) - \frac{1}{2} \nabla_y \cdot ( G(t, Y_t)G(t, Y_t)^{\mathrm{tr}} ) - \frac{1}{2} G(t, Y_t)G(t, Y_t)^{\mathrm{tr}}\nabla_y \log p(t, Y_t),\]</p><p>boils down to expressing the diffusion term completely as a flux term:</p><p class="math-container">\[    \begin{align*}
        \frac{1}{2}\nabla_x^2 : &amp; \left( (G(t, x)G(t, x)^{\mathrm{tr}}) p(t, x) \right) \\
        &amp; = \frac{1}{2}\nabla_x \cdot \left( \left( \nabla_x \cdot ( G(t, x)G(t, x)^{\mathrm{tr}} ) + G(t, x)G(t, x)^{\mathrm{tr}}\nabla_x \log p(t, x) \right) p(t, x) \right).
    \end{align*}\]</p><p>As discussed in the Introduction, both formulations seem to have their advantages. So one idea is to split up the diffusion term and handle one part as the ODE flow and leave the other part as the SDE diffusion, in an attempt to leverage the advantages of both formulations. This is what we do next.</p><h3 id="For-a-general-Itô-diffusion"><a class="docs-heading-anchor" href="#For-a-general-Itô-diffusion">For a general Itô diffusion</a><a id="For-a-general-Itô-diffusion-1"></a><a class="docs-heading-anchor-permalink" href="#For-a-general-Itô-diffusion" title="Permalink"></a></h3><p>We may introduce a weight parameter, say <span>$\theta(t),$</span> which can even be time dependent, and split up the diffusion term of the Fokker-Planck equation into</p><p class="math-container">\[   \begin{align*}
        \frac{1}{2}\nabla_x^2 : &amp; \left( (G(t, x)G(t, x)^{\mathrm{tr}}) p(t, x) \right) \\
        &amp; = \frac{1 - \theta(t)}{2}\nabla_x^2 : \left( (G(t, x)G(t, x)^{\mathrm{tr}}) p(t, x) \right) + \frac{\theta(t)}{2}\nabla_x^2 : \left( (G(t, x)G(t, x)^{\mathrm{tr}}) p(t, x) \right).
    \end{align*}\]</p><p>Rewriting only the first term as a flux we obtain</p><p class="math-container">\[    \begin{align*}
        \frac{1}{2}\nabla_x^2 : &amp; \left( (G(t, x)G(t, x)^{\mathrm{tr}}) p(t, x) \right) \\
        &amp; = \frac{1 - \theta(t)}{2}\nabla_x \cdot \left( \left( \nabla_x \cdot ( G(t, x)G(t, x)^{\mathrm{tr}} ) + G(t, x)G(t, x)^{\mathrm{tr}}\nabla_x \log p(t, x) \right) p(t, x) \right) \\
        &amp; \qquad +  \frac{\theta(t)}{2}\nabla_x^2 : \left( (G(t, x)G(t, x)^{\mathrm{tr}}) p(t, x) \right).
    \end{align*}\]</p><p>In this way, the Fokker-Planck equation becomes</p><p class="math-container">\[    \begin{align*}
        \frac{\partial p}{\partial t} + \nabla_x \cdot \bigg( \bigg( &amp; f(t, x) - \frac{1-\theta(t)}{2} \nabla_x \cdot ( G(t, x)G(t, x)^{\mathrm{tr}} ) \\
        &amp; \qquad \qquad - \frac{1-\theta(t)}{2} G(t, x)G(t, x)^{\mathrm{tr}}\nabla_x \log p(t, x) \bigg) p(t, x) \bigg) \\
        &amp; \qquad \qquad \qquad \qquad = \frac{\theta(t)}{2}\nabla_x^2 : \left( (G(t, x)G(t, x)^{\mathrm{tr}}) p(t, x) \right).
    \end{align*}\]</p><p>The associated <strong>splitted-up probability flow SDE</strong> reads</p><p class="math-container">\[    \begin{align*}
        \mathrm{d}Y_t = \bigg( &amp; f(t, Y_t) - \frac{1- \theta(t)}{2} \nabla_y \cdot ( G(t, Y_t)G(t, Y_t)^{\mathrm{tr}} ) \\
        &amp; \qquad \qquad - \frac{1 - \theta(t)}{2} G(t, Y_t)G(t, Y_t)^{\mathrm{tr}}\nabla_y \log p(t, Y_t)\bigg)\;\mathrm{d}t + \sqrt{\theta(t)} G(t, Y_t)\;\mathrm{d}W_t.
    \end{align*}\]</p><p>As mentioned before, all these formulations are theoretically equivalent. But their practical applications differ.</p><h3 id="For-an-SDE-with-scalar-time-dependent-diagonal-noise"><a class="docs-heading-anchor" href="#For-an-SDE-with-scalar-time-dependent-diagonal-noise">For an SDE with scalar time-dependent diagonal noise</a><a id="For-an-SDE-with-scalar-time-dependent-diagonal-noise-1"></a><a class="docs-heading-anchor-permalink" href="#For-an-SDE-with-scalar-time-dependent-diagonal-noise" title="Permalink"></a></h3><p>In the case that</p><p class="math-container">\[    G(t, x) = g(t)\mathbf{I},\]</p><p>the splitted-up probability flow SDE reduces to</p><p class="math-container">\[    \begin{align*}
        \mathrm{d}Y_t = \bigg( &amp; f(t, Y_t) - \frac{1 - \theta(t)}{2} g(t)^2 \nabla_y \log p(t, Y_t)\bigg)\;\mathrm{d}t + \sqrt{\theta(t)} g(t)\;\mathrm{d}W_t,
    \end{align*}\]</p><p>with the Fokker-Planck equation</p><p class="math-container">\[    \frac{\partial p}{\partial t} + \nabla_x \cdot \bigg( \bigg( f(t, x) - \frac{1-\theta(t)}{2} g(t)\nabla_x \log p(t, x) \bigg) p(t, x) \bigg) = \frac{\theta(t)g(t)^2}{2}\Delta_x p(t, x).\]</p><h3 id="Connection-with-the-Karras-et-al-probability-flow-SDE"><a class="docs-heading-anchor" href="#Connection-with-the-Karras-et-al-probability-flow-SDE">Connection with the Karras et al probability flow SDE</a><a id="Connection-with-the-Karras-et-al-probability-flow-SDE-1"></a><a class="docs-heading-anchor-permalink" href="#Connection-with-the-Karras-et-al-probability-flow-SDE" title="Permalink"></a></h3><p>If we set</p><p class="math-container">\[    f(t, x) = 0, \qquad g(t) = \sqrt{2\dot\sigma(t) \sigma(t)}\]</p><p>and choose</p><p class="math-container">\[    \theta(t) = \frac{2\beta(t)\sigma(t)^2}{g(t)^2},\]</p><p>we get</p><p class="math-container">\[    \begin{align*}
        - \frac{1 - \theta(t)}{2} g(t)^2 &amp; = \frac{1}{2}(\theta(t) - 1) g(t)^2 = \frac{1}{2}\left( \frac{2\beta(t)\sigma(t)^2}{g(t)^2} - 1\right)g(t)^2 = \frac{1}{2}\left(2\beta(t)\sigma(t)^2 - g(t)^2\right) \\
        &amp; = \frac{1}{2}\left( 2\beta(t)\sigma(t)^2 - 2\dot\sigma(t) \sigma(t)\right) = \beta(t)\sigma(t)^2 - \dot\sigma(t) \sigma(t)
    \end{align*}\]</p><p>and </p><p class="math-container">\[    \sqrt{\theta(t)} g(t) = \sqrt{2\beta(t)} \sigma(t)\]</p><p>Thus, we transform the splitted-up probability flow SDE into the probability flow SDE of <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/a98846e9d9cc01cfb87eb694d946ce6b-Abstract-Conference.html">Karras, Aittala, Aila, Laine (2022)</a>,</p><p class="math-container">\[    \begin{align*}
        \mathrm{d}Y_t = \left( -\dot\sigma(t)\sigma(t) + \beta(t)\sigma(t)^2 \right) \nabla_y \log p(t, Y_t)\;\mathrm{d}t + \sqrt{2\beta(t)} \sigma(t)\;\mathrm{d}W_t,
    \end{align*}\]</p><p>having the same distribution as the SDE</p><p class="math-container">\[    \mathrm{d}X_t = \sqrt{2\dot\sigma(t)\sigma(t)}\;\mathrm{d}W_t,\]</p><p>which is associated with the Fokker-Planck equation</p><p class="math-container">\[    \frac{\partial p}{\partial t}  = \dot\sigma(t)\sigma(t)\Delta_x p(t, x).\]</p><p>Notice that now, instead of the free parameter <span>$\theta(t)$</span>, we have the free parameter <span>$\beta(t)$</span> of <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/a98846e9d9cc01cfb87eb694d946ce6b-Abstract-Conference.html">Karras, Aittala, Aila, Laine (2022)</a>.</p><p>The motivation for writing it in this way, with <span>$\dot\sigma \sigma$</span> as the diffusion coefficient, is that the heat kernel becomes</p><p class="math-container">\[    G(\sigma(t)) = \frac{1}{(2\pi \sigma(t)^2)^{d/2}} e^{-\frac{1}{2}\frac{x^2}{\sigma(t)^2}},\]</p><p>which is the probability density of the Gaussian <span>$\mathcal{N}(0, \sigma(t)^2\mathbf{I}).$</span> The distribution <span>$p(t, x)$</span> is given by the convolution</p><p class="math-container">\[    p(t, \cdot) = G(\sigma(t)) \star p_0,\]</p><p>where <span>$p_0=p_0(x)$</span> is the initial probability density of the distribution we are attempting to model. This is just a reparametrization of the process directly in terms of a desired variance <span>$\sigma(t)^2.$</span></p><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><ol><li><a href="https://dl.acm.org/doi/10.5555/3045118.3045358">J. Sohl-Dickstein, E. A. Weiss, N. Maheswaranathan, S. Ganguli (2015), &quot;Deep unsupervised learning using nonequilibrium thermodynamics&quot;, ICML&#39;15: Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, 2256-2265</a></li><li><a href="https://dl.acm.org/doi/10.5555/3454287.3455354">Y. Song and S. Ermon (2019), &quot;Generative modeling by estimating gradients of the data distribution&quot;, NIPS&#39;19: Proceedings of the 33rd International Conference on Neural Information Processing Systems, no. 1067, 11918-11930</a></li><li><a href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html">J. Ho, A. Jain, P. Abbeel (2020), &quot;Denoising diffusion probabilistic models&quot;, in Advances in Neural Information Processing Systems 33, NeurIPS2020</a></li><li><a href="https://doi.org/10.3390/e22080802">D. Maoutsa, S. Reich, M. Opper (2020), &quot;Interacting particle solutions of Fokker-Planck equations through gradient-log-density estimation&quot;, Entropy, 22(8), 802, DOI: 10.3390/e22080802</a></li><li><a href="https://arxiv.org/abs/2011.13456">Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, B. Poole (2020), &quot;Score-based generative modeling through stochastic differential equations&quot;, arXiv:2011.13456</a></li><li><a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/a98846e9d9cc01cfb87eb694d946ce6b-Abstract-Conference.html">T. Karras, M. Aittala, T. Aila, S. Laine (2022), Elucidating the design space of diffusion-based generative models, Advances in Neural Information Processing Systems 35 (NeurIPS 2022)</a></li></ol></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../mdsm/">« Multiple denoising score matching</a><a class="docs-footer-nextpage" href="../reverse_flow/">Reverse probability flow »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.15.0 on <span class="colophon-date" title="Thursday 6 November 2025 15:39">Thursday 6 November 2025</span>. Using Julia version 1.12.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
