<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reverse probability flow · Random notes</title><meta name="title" content="Reverse probability flow · Random notes"/><meta property="og:title" content="Reverse probability flow · Random notes"/><meta property="twitter:title" content="Reverse probability flow · Random notes"/><meta name="description" content="Documentation for Random notes."/><meta property="og:description" content="Documentation for Random notes."/><meta property="twitter:description" content="Documentation for Random notes."/><meta property="og:url" content="https://github.com/rmsrosa/random_notes/generative/reverse_flow/"/><meta property="twitter:url" content="https://github.com/rmsrosa/random_notes/generative/reverse_flow/"/><link rel="canonical" href="https://github.com/rmsrosa/random_notes/generative/reverse_flow/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/style.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="Random notes logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Random notes</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Random Notes</a></li><li><span class="tocitem">Probability Essentials</span><ul><li><a class="tocitem" href="../../probability/kernel_density_estimation/">Kernel Density Estimation</a></li><li><a class="tocitem" href="../../probability/convergence_notions/">Convergence notions</a></li></ul></li><li><span class="tocitem">Discrete-time Markov chains</span><ul><li><a class="tocitem" href="../../markov_chains/mc_definitions/">Essential definitions</a></li><li><a class="tocitem" href="../../markov_chains/mc_invariance/">Invariant distributions</a></li><li><input class="collapse-toggle" id="menuitem-3-3" type="checkbox"/><label class="tocitem" for="menuitem-3-3"><span class="docs-label">Countable-space Markov chains</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../markov_chains/mc_countableX_recurrence/">Recurrence in the countable-space case</a></li><li><a class="tocitem" href="../../markov_chains/mc_countableX_connections/">Connected states, irreducibility and uniqueness of invariant measures</a></li><li><a class="tocitem" href="../../markov_chains/mc_countableX_convergencia/">Aperiodicidade e convergência para a distribuição estacionária</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-4" type="checkbox"/><label class="tocitem" for="menuitem-3-4"><span class="docs-label">Continuous-space Markov chains</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../markov_chains/mc_irreducibility_and_recurrence/">Irreducibility and recurrence in the continuous-space case</a></li></ul></li></ul></li><li><span class="tocitem">Sampling methods</span><ul><li><a class="tocitem" href="../../sampling/overview/">Overview</a></li><li><a class="tocitem" href="../../sampling/prng/">Random number generators</a></li><li><input class="collapse-toggle" id="menuitem-4-3" type="checkbox"/><label class="tocitem" for="menuitem-4-3"><span class="docs-label">Transform methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/invFtransform/">Probability integral transform</a></li><li><a class="tocitem" href="../../sampling/box_muller/">Box-Muller transform</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-4" type="checkbox"/><label class="tocitem" for="menuitem-4-4"><span class="docs-label">Accept-Reject methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/rejection_sampling/">Rejection sampling</a></li><li><a class="tocitem" href="../../sampling/empiricalsup_rejection/">Empirical supremum rejection sampling</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-5" type="checkbox"/><label class="tocitem" for="menuitem-4-5"><span class="docs-label">Markov Chain Monte Carlo (MCMC)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/mcmc/">Overview</a></li><li><a class="tocitem" href="../../sampling/metropolis/">Metropolis and Metropolis-Hastings</a></li><li><a class="tocitem" href="../../sampling/convergence_metropolis/">Convergence of Metropolis-Hastings</a></li><li><a class="tocitem" href="../../sampling/gibbs/">Gibbs sampling</a></li><li><a class="tocitem" href="../../sampling/hmc/">Hamiltonian Monte Carlo (HMC)</a></li></ul></li><li><a class="tocitem" href="../../sampling/langevin_sampling/">Langevin sampling</a></li></ul></li><li><span class="tocitem">Bayesian inference</span><ul><li><input class="collapse-toggle" id="menuitem-5-1" type="checkbox"/><label class="tocitem" for="menuitem-5-1"><span class="docs-label">Bayes Theory</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/bayes/">Bayes Theorem</a></li><li><a class="tocitem" href="../../bayesian/bayes_inference/">Bayesian inference</a></li><li><a class="tocitem" href="../../bayesian/bernstein_vonmises/">Bernstein–von Mises theorem</a></li></ul></li><li><a class="tocitem" href="../../bayesian/bayesian_probprog/">Bayesian probabilistic programming</a></li><li><input class="collapse-toggle" id="menuitem-5-3" type="checkbox"/><label class="tocitem" for="menuitem-5-3"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/find_pi/">Estimating π via frequentist and Bayesian methods</a></li><li><a class="tocitem" href="../../bayesian/linear_regression/">Many Ways to Linear Regression</a></li><li><a class="tocitem" href="../../bayesian/tilapia_alometry/">Alometry law for the Nile Tilapia</a></li><li><a class="tocitem" href="../../bayesian/mortality_tables/">Modeling mortality tables</a></li></ul></li></ul></li><li><span class="tocitem">Generative models</span><ul><li><input class="collapse-toggle" id="menuitem-6-1" type="checkbox" checked/><label class="tocitem" for="menuitem-6-1"><span class="docs-label">Score matching</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../overview/">Overview</a></li><li><a class="tocitem" href="../stein_score/">Stein score function</a></li><li><a class="tocitem" href="../score_matching_aapo/">Score matching of Aapo Hyvärinen</a></li><li><a class="tocitem" href="../score_matching_neural_network/">Score matching a neural network</a></li><li><a class="tocitem" href="../parzen_estimation_score_matching/">Score matching with Parzen estimation</a></li><li><a class="tocitem" href="../denoising_score_matching/">Denoising score matching of Pascal Vincent</a></li><li><a class="tocitem" href="../sliced_score_matching/">Sliced score matching</a></li><li><a class="tocitem" href="../1d_FD_score_matching/">1D finite-difference score matching</a></li><li><a class="tocitem" href="../2d_FD_score_matching/">2D finite-difference score matching</a></li><li><a class="tocitem" href="../ddpm/">Denoising diffusion probabilistic models</a></li><li><a class="tocitem" href="../mdsm/">Multiple denoising score matching</a></li><li><a class="tocitem" href="../probability_flow/">Probability flow</a></li><li class="is-active"><a class="tocitem" href>Reverse probability flow</a><ul class="internal"><li><a class="tocitem" href="#Aim"><span>Aim</span></a></li><li><a class="tocitem" href="#Reverse-ODE"><span>Reverse ODE</span></a></li><li><a class="tocitem" href="#Reverse-Itô-diffusion"><span>Reverse Itô diffusion</span></a></li><li><a class="tocitem" href="#Tracing-back-the-same-forward-paths-with-a-specific-Wiener-process"><span>Tracing back the same forward paths with a specific Wiener process</span></a></li><li><a class="tocitem" href="#A-simple-scalar-example"><span>A simple scalar example</span></a></li><li><a class="tocitem" href="#An-example-with-a-time-dependent-diffusion-coefficient"><span>An example with a time-dependent diffusion coefficient</span></a></li><li><a class="tocitem" href="#Numerics"><span>Numerics</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../score_based_sde/">Score-based SDE model</a></li></ul></li></ul></li><li><span class="tocitem">Sensitivity analysis</span><ul><li><a class="tocitem" href="../../sensitivity/overview/">Overview</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Generative models</a></li><li><a class="is-disabled">Score matching</a></li><li class="is-active"><a href>Reverse probability flow</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Reverse probability flow</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/rmsrosa/random_notes" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/rmsrosa/random_notes/blob/main/docs/src/generative/reverse_flow.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Reverse-probability-flow"><a class="docs-heading-anchor" href="#Reverse-probability-flow">Reverse probability flow</a><a id="Reverse-probability-flow-1"></a><a class="docs-heading-anchor-permalink" href="#Reverse-probability-flow" title="Permalink"></a></h1><h2 id="Aim"><a class="docs-heading-anchor" href="#Aim">Aim</a><a id="Aim-1"></a><a class="docs-heading-anchor-permalink" href="#Aim" title="Permalink"></a></h2><p>Review the reverse probability flow used for sampling, after the Stein score function has been trained, as developed in <a href="https://arxiv.org/abs/2011.13456">Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and Poole (2020)</a> and <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/a98846e9d9cc01cfb87eb694d946ce6b-Abstract-Conference.html">Karras, Aittala, Aila, and Laine (2022)</a>, based on the probability flow ODE developed in these articles and on the reverse time diffusion equation model previously worked out by <a href="https://doi.org/10.1016/0304-4149(82)90051-5">Anderson (1982)</a>.</p><h2 id="Reverse-ODE"><a class="docs-heading-anchor" href="#Reverse-ODE">Reverse ODE</a><a id="Reverse-ODE-1"></a><a class="docs-heading-anchor-permalink" href="#Reverse-ODE" title="Permalink"></a></h2><p>For an ODE of the form</p><p class="math-container">\[    \frac{\mathrm{d}x}{\mathrm{d}t} = f(t, x),\]</p><p>reverting time, on a time interval <span>$[0, T],$</span> is just a matter of decreasing <span>$t,$</span> from <span>$T$</span> to <span>$0.$</span> One way to think of it is via the integral formula</p><p class="math-container">\[    x(T) = x(t) + \int_t^T f(s, x(s))\;\mathrm{d}s,\]</p><p>so that</p><p class="math-container">\[    x(t) = x(T) - \int_t^T f(s, x(s))\;\mathrm{d}s.\]</p><p>Another way is to write <span>${\tilde x}(\tilde t\,) = x(T - \tilde t\,)$</span> and use the chain rule</p><p class="math-container">\[    \frac{\mathrm{d}{\tilde x}(\tilde t\,)}{\mathrm{d}\tilde t} = -\frac{\mathrm{d}x}{\mathrm{d}t}(T - \tilde t\,) = - f(T-\tilde t, x(T-\tilde t\,)) = -f(T-\tilde t, {\tilde x}(\tilde t\,)).\]</p><p>Integrating from <span>$0$</span> to <span>$T$</span> yields an integral relation equivalent to the previous one. In fact,</p><p class="math-container">\[    {\tilde x}(\tilde t\,) = {\tilde x}(0) - \int_0^{\tilde t} f(T-\tau, {\tilde x}(\tau)) \;\mathrm{d}\tau.\]</p><p>Going back to <span>$x(\cdot)$</span> and making the change of variables <span>$s = T - \tau,$</span> </p><p class="math-container">\[    x(T - \tilde t\,) = x(T) - \int_0^T f(T-\tau, x(T-\tau))\;\mathrm{d}\tau = x(T) + \int_T^{T-\tilde t} f(s, x(s))\;\mathrm{d}s.\]</p><p>Back to <span>$t = T - \tilde t$</span> yields</p><p class="math-container">\[    x(t) = x(T) - \int_t^T f(s, x(s))\;\mathrm{d}s.\]</p><p>The Euler method for the reverse flow is simply stepping backwards, from <span>$t$</span> to <span>$t - \Delta t,$</span> with the Taylor approximation reading</p><p class="math-container">\[    x(t_j) = x(t_{j+1}) - f(t_{j+1}, x(t_{j+1}))\Delta t,\]</p><p>with</p><p class="math-container">\[    t_j = j\Delta t,\]</p><p>for</p><p class="math-container">\[    \Delta t = T / n,\]</p><p>and <span>$n\in\mathbb{N}$</span> given.</p><p>If the initial condition is a random variable <span>$X_0,$</span> and the flow evolves to <span>$X_T,$</span> then the reverse flow evolves back to <span>$X_0.$</span> By approximating <span>$X_T \sim Y_T$</span> by another random variable <span>$Y_T,$</span> say a standard normal distribution as in the generative diffusion processes, then the reverse flow evolves backwards towards an approximation <span>$Y_0$</span> of the initial distribution <span>$X_0.$</span></p><p>We remark that this is a <em>pathwise reversion,</em> meaning that each forward path <span>$x(t)$</span> with initial condition <span>$x(0)$</span> is traced back by the reverse equation starting at the final point <span>$x(T).$</span> This is obtained without any knowledge of the prior solution, i.e. the backward integration only depends on the future of the solution. More precisely, we only need to know <span>$X_T$</span> and the function <span>$f(t, x)$</span> in order to solve (or numerically approximate) the solution <span>$X_t,$</span> for <span>$0 \leq t \leq T.$</span></p><p>This is in contrast with the result for SDEs, for which, without knowledge of a forward solution <span>$X_t(\omega),$</span> nor of the specific path <span>$W_t(\omega),$</span> of the driving Weiner process, we are not able to trace back exactly the same solution only from the knowledge of <span>$X_T(\omega).$</span> But the important fact is that we can still solve backwards the equation using a possibly different Wiener process and recover possibly different solutions of the forward equation, but which collectively give the same probability distribution. In order to trace back the exact forward paths, a specific Wiener process must be used.</p><h2 id="Reverse-Itô-diffusion"><a class="docs-heading-anchor" href="#Reverse-Itô-diffusion">Reverse Itô diffusion</a><a id="Reverse-Itô-diffusion-1"></a><a class="docs-heading-anchor-permalink" href="#Reverse-Itô-diffusion" title="Permalink"></a></h2><p>Consider now a forward evolution given by an Itô diffusion SDE</p><p class="math-container">\[    \mathrm{d}X_t = f(t, X_t)\;\mathrm{d}t + G(t, X_t)\;\mathrm{d}W_t,\]</p><p>where the drift factor is a vector-valued function <span>$f:I\times \mathbb{R}^d \rightarrow \mathbb{R}^d$</span>, the driving process <span>$W_t \in \mathbb{R}^k$</span> is a vector of independent Wiener processes, and the diffusion factor is a matrix-valued, time-dependent function <span>$G:I\times \mathbb{R}^d \rightarrow \mathbb{R}^{d\times k}.$</span></p><p>In the following proof, we cannot deduce that the reverse equation traces back exactly a given sample path <span>$X_t(\omega),$</span> as in the ODE case. Instead, we only obtain that the reverse SDE generates the same probability distribution as the forward SDE. We will recover the same path only with a specific Wiener process, as done further below.</p><p>Notice the reverse diffusion equation requires knowledge of the Stein score function, which fortunately is not a problem in the use case we have in mind, where the Stein score is properly modeled.</p><p>One way of obtaining the reverse SDE, as derived in <a href="https://doi.org/10.1016/0304-4149(82)90051-5">Anderson (1982)</a>, and seen in other works (e.g. <a href="https://doi.org/10.1214/aop/1176992362">Haussmann and Pardoux (1986)</a>), is by looking at the joint distribution <span>$p(t, x_t, s, x_s)$</span> at two different times <span>$t$</span> and <span>$s$</span> and by working with conditional distributions. We do it differently here, though: we exploit the connection between the SDE and the probability flow, introduced by <a href="https://doi.org/10.3390/e22080802">Maoutsa, Reich, Opper (2020)</a> and generalized by <a href="https://arxiv.org/abs/2011.13456">Song, Sohl-Dickstein, Kingma, Kumar, Ermon, Poole (2020)</a> and <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/a98846e9d9cc01cfb87eb694d946ce6b-Abstract-Conference.html">Karras, Aittala, Aila, Laine (2022)</a>.</p><p>For the stochastic differential equation above, the probability flow ODE obtained by <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/a98846e9d9cc01cfb87eb694d946ce6b-Abstract-Conference.html">Karras, Aittala, Aila, Laine (2022)</a> reads (except for the symbol <span>$\{Y_t\}_t$</span> instead of <span>$\{X_t\}_t$</span>) </p><p class="math-container">\[    \frac{\mathrm{d}Y_t}{\mathrm{d}t} = f(t, Y_t) - \frac{1}{2} \nabla_x \cdot ( G(t, Y_t)G(t, Y_t)^{\mathrm{tr}} ) - \frac{1}{2} G(t, Y_t)G(t, Y_t)^{\mathrm{tr}}\nabla_x \log p(t, Y_t).\]</p><p>Both <span>$\{X_t\}_t$</span> and <span>$\{Y_t\}_t$</span> have the same probability distribution <span>$p(t, \cdot).$</span></p><p>We now write the reverse ODE by making the change of variables <span>$\tilde Y_{\tilde t} = Y_{T - \tilde t},$</span> with the reverse time variable <span>$\tilde t = T - t.$</span> It is just an ODE (pathwise), so the reverse equation follows from a straightforward chain rule, upon the change <span>$\tilde t \mapsto T - \tilde t,$</span></p><p class="math-container">\[    \begin{align*}
        \frac{\mathrm{d}{\tilde Y}_{\tilde t}}{\mathrm{d}\tilde t} = \frac{\mathrm{d}}{\mathrm{d}\tilde t}Y_{T - \tilde t} = - \frac{\mathrm{d}Y_{T - \tilde t}}{\mathrm{d}\tilde t} &amp; = - f(T - \tilde t, Y_{T - \tilde t}) + \frac{1}{2} \nabla_y \cdot ( G(T - \tilde t, Y_{T - \tilde t})G(T - \tilde t, Y_{T - \tilde t})^{\mathrm{tr}} ) \\
        &amp; \qquad \qquad + \frac{1}{2} G(T - \tilde t, Y_{T - \tilde t})G(T - \tilde t, Y_{T - \tilde t})^{\mathrm{tr}}\nabla_y \log p(T - \tilde t, Y_{T - \tilde t}),
    \end{align*}\]</p><p>i.e.</p><p class="math-container">\[    \begin{align*}
        \frac{\mathrm{d}{\tilde Y}_{\tilde t}}{\mathrm{d}\tilde t} &amp; = - f(T - \tilde t, {\tilde Y}_{\tilde t}) + \frac{1}{2} \nabla_y \cdot ( G(T - \tilde t, {\tilde Y}_{\tilde t})G(T - \tilde t, {\tilde Y}_{\tilde t})^{\mathrm{tr}} ) \\
        &amp; \qquad \qquad + \frac{1}{2} G(T - \tilde t, {\tilde Y}_{\tilde t})G(T - \tilde t, {\tilde Y}_{\tilde t})^{\mathrm{tr}}\nabla_y \log p(T - \tilde t, {\tilde Y}_{\tilde t}).
    \end{align*}\]</p><p>The terms with <span>$GG^{\mathrm{tr}}$</span> don&#39;t come with the right sign (for the conversion from probability flow ODE to the associated SDE), so we just rewrite it as (like adding and subtracting the same terms)</p><p class="math-container">\[    \begin{align*}
        \frac{\mathrm{d}{\tilde Y}_{\tilde t}}{\mathrm{d}\tilde t} &amp; = - f(T - \tilde t, {\tilde Y}_{\tilde t}) + \nabla_y \cdot ( G(T - \tilde t, {\tilde Y}_{\tilde t})G(T - \tilde t, {\tilde Y}_{\tilde t})^{\mathrm{tr}} ) \\
        &amp; \qquad \qquad + G(T - \tilde t, {\tilde Y}_{\tilde t})G(T - \tilde t, {\tilde Y}_{\tilde t})^{\mathrm{tr}}\nabla_y \log p(T - \tilde t, {\tilde Y}_{\tilde t}) \\
        &amp; \qquad \qquad - \frac{1}{2} \nabla_y \cdot ( G(T - \tilde t, {\tilde Y}_{\tilde t})G(T - \tilde t, {\tilde Y}_{\tilde t})^{\mathrm{tr}} ) \\
        &amp; \qquad \qquad - \frac{1}{2} G(T - \tilde t, {\tilde Y}_{\tilde t})G(T - \tilde t, {\tilde Y}_{\tilde t})^{\mathrm{tr}}\nabla_y \log p(T - \tilde t, {\tilde Y}_{\tilde t}).
    \end{align*}\]</p><p>Now, with the proper sign, the last two terms on the right hand side become the diffusion term in the associated SDE for which this is the probability flow equation, namely</p><p class="math-container">\[    \begin{align*}
        \mathrm{d}{\tilde X}_{\tilde t} &amp; = \bigg( - f(T - \tilde t, {\tilde X}_{\tilde t}) + \nabla_x \cdot ( G(T - \tilde t, {\tilde X}_{\tilde t})G(T - \tilde t, {\tilde X}_{\tilde t})^{\mathrm{tr}} ) \\
        &amp; \qquad \qquad + G(T - \tilde t, {\tilde X}_{\tilde t})G(T - \tilde t, {\tilde X}_{\tilde t})^{\mathrm{tr}}\nabla_x \log p(T - \tilde t, {\tilde X}_{\tilde t}) \bigg) \;\mathrm{d}\tilde t\\
        &amp; \qquad \qquad \qquad + G(T - \tilde t, {\tilde X}_{\tilde t})\;\mathrm{d}{\tilde W}_{\tilde t},
    \end{align*}\]</p><p>where <span>$\{{\tilde W}_{\tilde t}\}_{\tilde t}$</span> is a (possibly different) Wiener process. In integral form, the equation for <span>${\tilde X}_{\tilde t},$</span> integrating from <span>$\tilde t = 0$</span> to <span>$\tilde t = T - t,$</span> reads</p><p class="math-container">\[    \begin{align*}
        {\tilde X}_{\tilde t} - {\tilde X}_0 &amp; = \int_0^{\tilde t} \bigg( - f(T - \tilde \tau, {\tilde X}_{\tilde \tau}) + \nabla_x \cdot ( G(T - \tilde \tau, {\tilde X}_{\tilde \tau})G(T - \tilde \tau, {\tilde X}_{\tilde \tau})^{\mathrm{tr}} ) \\
        &amp; \qquad \qquad + G(T - \tilde \tau, {\tilde X}_{\tilde \tau})G(T - \tilde \tau, {\tilde X}_{\tilde \tau})^{\mathrm{tr}}\nabla_x \log p(T - \tilde \tau, {\tilde X}_{\tilde \tau}) \bigg) \;\mathrm{d}\tilde \tau\\
        &amp; \qquad \qquad \qquad + \int_0^{\tilde t} G(T - \tilde \tau, {\tilde X}_{\tilde \tau})\;\mathrm{d}{\tilde W}_{\tilde \tau},
    \end{align*}\]</p><p>Back to the original (forward) time <span>$t = T - \tilde t,$</span> setting <span>${\hat X}_t = {\tilde X}_{T - t} = {\tilde X}_{\tilde t},$</span> and making the change of variable <span>$\tau = T - \tilde \tau$</span> in the integral term, this becomes</p><p class="math-container">\[    \begin{align*}
        {\hat X}_t - {\hat X}_T &amp; = \int_t^T \bigg( - f(\tau, {\hat X}_\tau) + \nabla_x \cdot ( G(\tau, {\hat X}_\tau)G(\tau, {\hat X}_\tau)^{\mathrm{tr}} ) \\
        &amp; \qquad \qquad + G(\tau, {\hat X}_\tau)G(\tau, {\hat X}_\tau)^{\mathrm{tr}}\nabla_x \log p(\tau, {\hat X}_\tau) \bigg) \;\mathrm{d}\tau\\
        &amp; \qquad \qquad \qquad - \int_t^{T} G(\tau, {\hat X}_\tau)\;\mathrm{d}{\tilde W}_{T-\tau},
    \end{align*}\]</p><p>which can be written as</p><p class="math-container">\[    \begin{align*}
        {\hat X}_T - {\hat X}_t &amp; = \int_{t}^{T} \bigg( f(\tau, {\hat X}_\tau) - \nabla_x \cdot ( G(\tau, {\hat X}_\tau)G(\tau, {\hat X}_\tau)^{\mathrm{tr}} ) \\
        &amp; \qquad \qquad - G(\tau, {\hat X}_\tau)G(\tau, {\hat X}_\tau)^{\mathrm{tr}}\nabla_x \log p(\tau, {\hat X}_\tau) \bigg) \;\mathrm{d} \tau\\
        &amp; \qquad \qquad \qquad + \int_{t}^T G(\tau, {\hat X}_\tau){\small \triangledown}\mathrm{d}{\hat W}_\tau,
    \end{align*}\]</p><p>with shorthand</p><p class="math-container">\[    \begin{align*}
        \mathrm{d}{\hat X}_t &amp; = \bigg( f(t, {\hat X}_t) - \nabla_x \cdot ( G(t, {\hat X}_t)G(t, {\hat X}_t)^{\mathrm{tr}} ) \\
        &amp; \qquad \qquad - G(t, {\hat X}_t)G(t, {\hat X}_t)^{\mathrm{tr}}\nabla_x \log p(t, {\hat X}_t) \bigg) \;\mathrm{d}t + G(\tau, {\hat X}_t){\small \triangledown}\mathrm{d}{\hat W}_t,
    \end{align*} \]</p><p>where</p><p class="math-container">\[    {\hat W}_t = {\tilde W}_{T - t},\]</p><p>with the understanding that <span>$\{{\hat W}_t\}_{0 \leq t \leq T}$</span> is a <em>backward Wiener process,</em> for which <span>${\hat W}_T = 0;$</span> the term <span>${\hat X}_t = {\tilde X}_{T - t}$</span> is independent of <em>previous</em> steps of the backward Wiener process, such as <span>${\hat W}_{t - \tau} - {\hat W}_t = {\tilde W}_{T - t + \tau} - {\tilde W}_{T - t},$</span> <span>$\tau &gt; 0;$</span> and the stochastic integral with <span>${\small \triangledown}\mathrm{d}{\hat W}_t$</span> is a <em>backward Itô integral,</em> given by</p><p class="math-container">\[    \int_t^T {\hat H}_\tau {\small \triangledown}\mathrm{d}{\hat W}_\tau = \lim \sum_{i=0}^{n-1} {\hat H}_{t_{i+1}} ( {\hat W}_{t_{i+1}} - {\hat W}_{t_{i}} ),\]</p><p>where <span>$t = \tau_0 &lt; \tau_1 &lt; \tau_n = T,$</span> and the limit is taken as <span>$\max_{i=0, n-1}|\tau_{i+1} - \tau_i| \rightarrow 0.$</span> This is essentially the Itô integral rephrased backwards, since the filtration is also reversed. Let us examine this more carefully.</p><p>We start with the Itô integral</p><p class="math-container">\[    \int_0^{T - t} {\tilde H}_{\tilde \tau}\;\mathrm{d}{\tilde W}_{\tilde \tau},\]</p><p>defined for any given (non-antecipative) process <span>$\{H_{\tilde t}\}_{\tilde t \geq 0},$</span> with respect to a (forward) Wiener process <span>$\{{\tilde W}_{\tilde t}\}_{\tilde t \geq 0}.$</span> This can be thought as the limit, as the mesh <span>$0 = \tilde \tau_0 &lt; \tilde \tau_1 &lt; \ldots &lt; \tilde \tau_n = T - \tilde t$</span> is refined, of the sums</p><p class="math-container">\[    \sum_{j=1}^n {\tilde H}_{\tilde \tau_{j-1}}({\tilde W}_{{\tilde \tau}_j} - {\tilde W}_{{\tilde \tau}_{j-1}}).\]</p><p>Now we define the points <span>$\tau_j = T - \tilde \tau_j,$</span> which form a mesh <span>$T = \tau_0 = T - {\tilde \tau}_0 &gt; \ldots T - {\tilde \tau}_n = T - t = \tau_n.$</span> The summation can be written as</p><p class="math-container">\[    \sum_{j=1}^n {\tilde H}_{\tilde \tau_{j-1}}({\tilde W}_{{\tilde \tau}_j} - {\tilde W}_{{\tilde \tau}_{j-1}}) = \sum_{j=1}^n {\tilde H}_{T - \tau_{j-1}} ( {\tilde W}_{T - \tau_j} - {\tilde W}_{T - \tau_{j-1}} ).\]</p><p>Defining <span>${\hat H}_t = {\tilde H}_{T - t}$</span> and <span>${\hat W}_t = {\tilde W}_{T - t},$</span> we write the above as</p><p class="math-container">\[    \sum_{j=1}^n {\hat H}_{\tau_{j-1}} ( {\hat W}_{\tau_j} - {\hat W}_{\tau_{j-1}} ).\]</p><p>But notice that, now, <span>$\tau_j &lt; \tau_{j-1}.$</span> In order to make this fact look more natural, we reindex the summation with <span>$i = N - j,$</span> and define the mesh with <span>${\hat \tau}_i = \tau_{N-i},$</span> so that</p><p class="math-container">\[    \begin{align*}
        \sum_{j=1}^n {\hat H}_{\tau_{j-1}} ( {\hat W}_{\tau_j} - {\hat W}_{\tau_{j-1}} ) &amp; = \sum_{i=0}^{n-1} {\hat H}_{\tau_{N-i-1}} ( {\hat W}_{\tau_{N-i}} - {\hat W}_{\tau_{N-i-1}} ) \\
        &amp; = \sum_{i=0}^{n-1} {\hat H}_{\tau_{N-(i+1)}} ( {\hat W}_{\tau_{N-i}} - {\hat W}_{\tau_{N-(i+1)}} ) \\
        &amp; = \sum_{i=0}^{n-1} {\hat H}_{{\hat \tau}_{i+1}} ( {\hat W}_{{\hat \tau}_{i}} - {\hat W}_{{\hat \tau}_{i+1}} ) \\
        &amp; = - \sum_{i=0}^{n-1} {\hat H}_{{\hat \tau}_{i+1}} ( {\hat W}_{{\hat \tau}_{i+1}} - {\hat W}_{{\hat \tau}_{i}} ).
    \end{align*}\]</p><p>The mesh runs from <span>${\hat \tau}_0 = \tau_N = T-t$</span> to <span>${\hat \tau}_N = \tau_0 = T.$</span> As the mesh is refined, this becomes the backward Itô integral</p><p class="math-container">\[    -\int_t^T {\hat H}_{\hat \tau}{\small \triangledown}\mathrm{d}{\hat W}_{\hat \tau}.\]</p><p>Thus, we have obtained the following identity between the forward and backward Itô integrals,</p><p class="math-container">\[    \int_0^{T - t} {\tilde H}_{\tilde \tau}\;\mathrm{d}{\tilde W}_{\tilde \tau} = -\int_t^T {\hat H}_{\hat \tau}{\small \triangledown}\mathrm{d}{\hat W}_{\hat \tau},\]</p><p>with the relevant changes of variables</p><p class="math-container">\[    {\hat H}_t = {\tilde H}_{T - t}, \qquad {\hat W}_t = {\tilde W}_{T - t}.\]</p><p>The process <span>${\tilde H}_{\tilde t}$</span> is independent of future increments of the Wiener process <span>$\{{\tilde W}_{\tilde t}\}_{\tilde t \geq 0}$</span> if, and only if, <span>${\hat H}_t$</span> is independent of previous increments of the backward Wiener process <span>$\{{\hat W}_t\}_{0\leq t \leq T}.$</span></p><h2 id="Tracing-back-the-same-forward-paths-with-a-specific-Wiener-process"><a class="docs-heading-anchor" href="#Tracing-back-the-same-forward-paths-with-a-specific-Wiener-process">Tracing back the same forward paths with a specific Wiener process</a><a id="Tracing-back-the-same-forward-paths-with-a-specific-Wiener-process-1"></a><a class="docs-heading-anchor-permalink" href="#Tracing-back-the-same-forward-paths-with-a-specific-Wiener-process" title="Permalink"></a></h2><p>Notice we wrote, above, <span>${\hat X}_t$</span> instead of <span>$X_t,$</span> because the paths might not be the same, although the distributions are. In order to trace back the same sample paths, one must use a specific Wiener process <span>$\{\bar W_t\}_{t\geq 0}$</span> defined as the weak solution (i.e. with the specific original Wiener process <span>$\{W_t\}_{t\geq 0}$</span> of the forward path)</p><p class="math-container">\[    \mathrm{d}\bar W_t = \mathrm{d}W_t + \frac{1}{p(t, X_t)}\nabla_x \cdot (p(t, X_t) G(t, X_t)) \;\mathrm{d}t,\]</p><p>i.e.</p><p class="math-container">\[    \bar W_t = W_t + \int_0^t \frac{1}{p(s, X_s)}\nabla_x \cdot (p(s, X_s) G(s, X_s)) \;\mathrm{d}s.\]</p><p>With this noise, if <span>$\{X_t\}_{t\geq 0}$</span> is the solution of the forward diffusion equation</p><p class="math-container">\[    \mathrm{d}X_t = f(t, X_t)\;\mathrm{d}t + G(t, X_t)\;\mathrm{d}W_t,\]</p><p>then it also solves the reverse equation</p><p class="math-container">\[    \mathrm{d}X_t = {\bar f}(t, X_t)\;\mathrm{d}t + G(t, X_t){\small \triangledown}\mathrm{d}{\bar W}_t,\]</p><p>where</p><p class="math-container">\[    \begin{align*}
        {\bar f}(t, x) &amp; = f(t, x) - \nabla_x \cdot ( G(t, x)G(t, x)^{\mathrm{tr}} ) - G(t, x)G(t, x)^{\mathrm{tr}}\nabla_x \log p(t, x) \\
        &amp; = f(t, x) - \frac{1}{p(t, x)}\nabla_x \cdot (G(t, x)G(t, x)^{\mathrm{tr}} p(t, x)).
    \end{align*}\]</p><p>The fact that <span>$\{\bar W_t\}_{t\geq 0}$</span> is actually a Wiener process is based on the characterization of Wiener processes as almost surely continuous martingales with <span>$W_0 = 0$</span> and with quadratic variation <span>$[W_t, W_t] = t,$</span> for all <span>$t\geq 0.$</span></p><p>The fact that it is a martingale follows from the fact that the integral term is itself a martingale. Indeed, this follows from</p><p class="math-container">\[    \begin{align*}
        \mathbb{E}\bigg[ \int_t^{t+\tau} &amp; \frac{1}{p(s, X_s)}\nabla_x \cdot (p(s, X_s) G(s, X_s)) \;\mathrm{d}s\bigg] = \int_\Omega \int_t^{t+\tau} \frac{1}{p(s, X_s(\omega))}\nabla_x \cdot (p(s, X_s(\omega)) G(s, X_s(\omega))) \;\mathrm{d}s\;\mathrm{d}\mathbb{P}(\omega) \\
        &amp; =  \int_t^{t+\tau} \int_{\mathbb{R}} \frac{1}{p(s, x)}\nabla_x \cdot (p(s, x) G(s, x)) p(s, x)\;\mathrm{d}x\;\mathrm{d}s \\
        &amp; = \int_t^{t+\tau} \int_{\mathbb{R}} \nabla_x \cdot (p(s, x) G(s, x)) \;\mathrm{d}x\;\mathrm{d}s \\
        &amp; = 0,
    \end{align*}\]</p><p>for arbitrary <span>$t, \tau \geq 0,$</span> provided we have sufficient decay of <span>$p(s, x) G(s, x),$</span> as <span>$x\rightarrow \pm\infty.$</span></p><p>Now, since the second term in the definition of <span>$\bar W_t$</span> is a Riemann integral, its quadratic variation is zero, and thus</p><p class="math-container">\[    [\bar W_t, \bar W_t]_t = [W_t, W_t]_t = t.\]</p><p>Hence, from the Lévy characterization, <span>$\{\bar W_t\}_{t\geq 0}$</span> is a Wiener process.</p><p>The fact that <span>$X_t$</span> is independent of <em>previous</em> steps of <span>$\{\bar W_t\}_{t \geq 0},$</span> say <span>$\bar W_{t_2} - \bar W_{t_1},$</span> <span>$0 \leq t_1 &lt; t_2 \leq t,$</span> follows from the facts that <span>$X_t$</span> is adapted to <span>$\{W_t\}_{t\geq 0}$</span> and that <span>$W_t$</span> itself is independent of previous steps of <span>$\{\bar W_t\}_{t \geq 0}.$</span> The proof of that is a bit involved, though, and can be found in <a href="https://doi.org/10.1016/0304-4149(82)90051-5">Anderson (1982)</a>. Here, we content ourselves in proving that in a specific simple case below.</p><h2 id="A-simple-scalar-example"><a class="docs-heading-anchor" href="#A-simple-scalar-example">A simple scalar example</a><a id="A-simple-scalar-example-1"></a><a class="docs-heading-anchor-permalink" href="#A-simple-scalar-example" title="Permalink"></a></h2><p>For illustrative purposes, consider the trivial diffusion equation</p><p class="math-container">\[    \mathrm{d}X_t = \sigma \;\mathrm{d}W_t,\]</p><p>with</p><p class="math-container">\[    X_0 = 0.\]</p><p>The solution is simply</p><p class="math-container">\[    X_t = \sigma W_t.\]</p><p>The marginal probability densities of this stochastic process are</p><p class="math-container">\[    p(t, x) = \frac{1}{\sqrt{2\pi \sigma^2 t}}e^{-\frac{1}{2}\frac{x^2}{\sigma^2t}}, \quad x\in\mathbb{R},\]</p><p>for <span>$t &gt; 0.$</span> In this case,</p><p class="math-container">\[    \frac{1}{p(s, x)}\nabla_x \cdot (p(s, x) G(s, x)) = \sigma \frac{1}{p(s, x)}\nabla_x \cdot (p(s, x)) = \sigma \nabla_x \log(p(s, x)),\]</p><p>with</p><p class="math-container">\[    \sigma\nabla_x \log(p(s, x)) = \sigma \nabla_x \left( -\frac{1}{2}\frac{x^2}{\sigma^2t} - \log(\sqrt{2\pi \sigma^2 t}) \right) = - \frac{x}{\sigma t}.\]</p><p>Thus, the reverse Wiener process takes the form</p><p class="math-container">\[    {\bar W}_t = W_t - \int_0^t \frac{X_s}{\sigma s} \;\mathrm{d}s = W_t - \int_0^t \frac{W_s}{s}\;\mathrm{d}s.\]</p><p>Write</p><p class="math-container">\[    X_T - X_t = \sigma W_T - \sigma W_t = \sigma {\bar W}_T - \sigma {\bar W}_t + \sigma\int_t^T \frac{W_s}{s}\;\mathrm{d}s = \sigma ({\bar W}_T - {\bar W}_t) + \int_t^T \frac{X_s}{s}\;\mathrm{d}s.\]</p><p>This becomes the reverse diffusion equation</p><p class="math-container">\[    \mathrm{d}X_t = \frac{X_t}{t}\;\mathrm{d}t + \sigma\;\mathrm{d}{\bar W}_t.\]</p><p>The diffusion term is a trivial constant term, but, nevertheless, we still have the remarkable property that <span>$X_t=\sigma W_t$</span> is independent of previous increments of <span>${\bar W}_t.$</span> Indeed, both are Gaussian processes with zero mean, so the covariance, with <span>$0 \leq t - \tau &lt; t,$</span> is given by</p><p class="math-container">\[    \begin{align*}
        \mathbb{E}[X_t ({\bar W}_t - {\bar W}_{t - \tau})] &amp; = \sigma\mathbb{E}\left[ W_t ({\bar W}_t - {\bar W}_{t - \tau})\right] \\
        &amp; = \sigma\mathbb{E}\left[ W_t \left(W_t - W_{t - \tau} - \int_{t-\tau}^t \frac{W_s}{s}\;\mathrm{d}s\right)\right] \\
        &amp; = \sigma\mathbb{E}\left[ W_t^2\right] - \sigma\mathbb{E}\left[W_t W_{t - \tau}\right] - \int_{t-\tau}^t \frac{\mathbb{E}[W_t W_s]}{s}\;\mathrm{d}s \\
        &amp; = \sigma t - \sigma \min\{t, t - \tau\} - \sigma \int_{t-\tau}^t \frac{\min\{t, s\}}{s}\;\mathrm{d}s \\
        &amp; = \sigma t - \sigma (t - \tau) - \sigma \int_{t-\tau}^t \;\mathrm{d}s \\
        &amp; = \sigma t - \sigma (t - \tau) - \sigma \tau \\
        &amp; = 0,
    \end{align*}\]</p><p>showing that they are uncorrelated. Similarly with</p><p class="math-container">\[    \mathbb{E}[X_{t_2} ({\bar W}_{t_1} - {\bar W}_{t_0})] = \sigma t_1 - \sigma t_0 - \sigma (t_1 - t_0) = 0,\]</p><p>for <span>$0 \leq t_0 &lt; t_1 \leq t_2.$</span></p><p>In order to see that <span>$\{\bar W_t\}_{t \geq 0}$</span> is, in fact, a Wiener process, we first notice that the formula in the definition implies that it is a Gaussian process with zero expectation at each time. Now we compute the covariance, at times <span>$t, s \geq 0,$</span> </p><p class="math-container">\[    \begin{align*}
        \mathbb{E}[\bar W_t \bar W_s] &amp; = \mathbb{E}\left[ \left(W_t - \int_0^t \frac{W_\tau}{\tau}\;\mathrm{d}\tau\right)\left(W_s - \int_0^s \frac{W_\xi}{\xi}\;\mathrm{d}\xi\right)\right] \\
        &amp; = \mathbb{E}\left[ W_t W_s - \int_0^s \frac{W_t W_\xi}{\xi}\;\mathrm{d}\xi - \int_0^t \frac{W_\tau W_s}{\tau}\;\mathrm{d}\tau + \int_0^t \int_0^s \frac{W_\tau W_\xi}{\tau\xi}\;\mathrm{d}\xi\;\mathrm{d}\tau\right] \\
        &amp; = \mathbb{E}[W_t W_s] - \int_0^s \frac{\mathbb{E}[W_t W_\xi]}{\xi}\;\mathrm{d}\xi - \int_0^t \frac{\mathbb{E}[W_\tau W_s]}{\tau}\;\mathrm{d}\tau + \int_0^t \int_0^s \frac{\mathbb{E}[W_\tau W_\xi]}{\tau\xi}\;\mathrm{d}\xi\;\mathrm{d}\tau \\
        &amp; = \min\{t, s\} - \int_0^s \frac{\min\{t, \xi\}}{\xi}\;\mathrm{d}\xi - \int_0^t \frac{\min\{\tau, s\}}{\tau}\;\mathrm{d}\tau + \int_0^t \int_0^s \frac{\min\{\tau, \xi\}}{\tau\xi}\;\mathrm{d}\xi\;\mathrm{d}\tau
    \end{align*}\]</p><p>Assuming <span>$0 \leq s \leq t,$</span> we obtain</p><p class="math-container">\[    \begin{align*}
        \mathbb{E}[\bar W_t \bar W_s] &amp; = s - \int_0^s \;\mathrm{d}\xi - \int_0^s \;\mathrm{d}\tau - \int_s^t \frac{s}{\tau}\;\mathrm{d}\tau + \int_0^s \int_0^\tau \frac{1}{\tau}\;\mathrm{d}\xi\;\mathrm{d}\tau + \int_0^s \int_\tau^s \frac{1}{\xi}\;\mathrm{d}\xi\;\mathrm{d}\tau + \int_s^t \int_0^s \frac{1}{\tau}\;\mathrm{d}\xi\;\mathrm{d}\tau\\
        &amp; = s - s - s - \int_s^t \frac{s}{\tau}\;\mathrm{d}\tau + \int_0^s \;\mathrm{d}\tau + \int_0^s \int_0^\xi \frac{1}{\xi}\;\mathrm{d}\tau\;\mathrm{d}\xi + \int_s^t \frac{s}{\tau}\;\mathrm{d}\tau \\ 
        &amp; = s - s - s + s + s \\
        &amp; = s.
    \end{align*}\]</p><p>For <span>$0 \leq t \leq s,$</span> we get</p><p class="math-container">\[    \mathbb{E}[\bar W_t \bar W_s] = t,\]</p><p>which means that</p><p class="math-container">\[    \mathbb{E}[\bar W_t \bar W_s] = \min\{t, s\}.\]</p><p>Thus, by the characterization of Wiener processes as the Gaussian processes with zero mean and correlation <span>$\min\{t, s\},$</span> we see that <span>$\{\bar W_t\}_{t \geq 0}$</span> is indeed a Wiener process.</p><p>As a final remark, notice that, if we revert time <span>$\tilde t = T - t$</span> in the equation</p><p class="math-container">\[    \mathrm{d}X_t = \frac{X_t}{t}\;\mathrm{d}t + \sigma\;\mathrm{d}{\bar W}_t,\]</p><p>we find</p><p class="math-container">\[    \mathrm{d}{\tilde X}_{\tilde t} = - \frac{{\tilde X}_{\tilde t}}{T - \tilde t}\;\mathrm{d}{\tilde t} + \sigma\;\mathrm{d}{\tilde W}_{\tilde t},\]</p><p>for <span>${\tilde X}_{\tilde t} = X_{T - \tilde t} = X_t$</span> and <span>${\tilde W}_{\tilde t} = {\bar W}_{T - \tilde t}.$</span> This is the Brownian bridge equation, except that we start at <span>${\tilde X}_0 = X_T$</span> and end up at <span>${\tilde X}_T = X_0.$</span></p><h2 id="An-example-with-a-time-dependent-diffusion-coefficient"><a class="docs-heading-anchor" href="#An-example-with-a-time-dependent-diffusion-coefficient">An example with a time-dependent diffusion coefficient</a><a id="An-example-with-a-time-dependent-diffusion-coefficient-1"></a><a class="docs-heading-anchor-permalink" href="#An-example-with-a-time-dependent-diffusion-coefficient" title="Permalink"></a></h2><p>Let us consider now a different example, with a time-varying diffusion coefficient. We start with <span>$X_0 = 0$</span> and consider the SDE with <span>$f=0$</span> and <span>$g=g(t) = \sqrt{2\sigma(t)\sigma&#39;(t)},$</span> for a given <span>$\sigma=\sigma(t),$</span></p><p class="math-container">\[\begin{cases}
    \mathrm{d}X_t = \sqrt{2\sigma(t)\sigma&#39;(t)}\;\mathrm{d}W_t, \\
    X_t\bigg|_{t=0} = 0.
\end{cases}\]</p><p>The solution is a time-changed Brownian motion,</p><p class="math-container">\[    X_t = \int_0^t \sqrt{2\sigma(s)\sigma&#39;(s)} \;\mathrm{d}W_s = W_{\sigma(t)^2}.\]</p><p>The probability density function for the process is</p><p class="math-container">\[    p(t, x) = G(\sigma(t)) = \frac{1}{\sqrt{2\pi\sigma(t)^2}} e^{-\frac{1}{2}\frac{x^2}{\sigma(t)^2}},\]</p><p>where <span>$G=G(\sigma)$</span> is the probability density function of the normal distribution <span>$\mathcal{N}(0, \sigma^2).$</span></p><p>Since</p><p class="math-container">\[\ln p(t, x) = -\frac{1}{2}\frac{x^2}{\sigma(t)^2} - \ln(\sqrt{2\pi\sigma(t)^2}),\]</p><p>the Stein score function of the process <span>$\{X_t\}_{t\geq 0}$</span> is</p><p class="math-container">\[    \nabla_x \ln p(t, x) = -\frac{x}{\sigma(t)^2}.\]</p><p>Hence, the reverse equation</p><p class="math-container">\[    \mathrm{d}{X}_t = -g(t)^2\nabla_x \ln p(t, {X}_t) \;\mathrm{d}t + g(t){\small \triangledown}\mathrm{d}{\hat W}_t,\]</p><p>becomes</p><p class="math-container">\[    \mathrm{d}{X}_t = \frac{g(t)^2}{\sigma(t)^2} X_t\;\mathrm{d}t + g(t){\small \triangledown}\mathrm{d}{\hat W}_t,\]</p><p>or, in terms of <span>$\sigma$</span> and <span>$\sigma&#39;,$</span></p><p class="math-container">\[    \mathrm{d}{X}_t = 2\frac{\sigma&#39;(t)}{\sigma(t)} X_t\;\mathrm{d}t + \sqrt{2\sigma(t)\sigma&#39;(t)}\mathrm{d}{\bar W}_t,\]</p><p>where</p><p class="math-container">\[    {\bar W}_t = W_t - \int_0^t \frac{g(s)}{\sigma(s)^2} X_s \;\mathrm{d}s = W_t - \int_0^t \sqrt{\frac{2\sigma&#39;(s)}{\sigma(s)}} X_s \;\mathrm{d}s.\]</p><p>This is iterated recursively backwards in time, with</p><p class="math-container">\[X_{t_j} - X_{t_{j-1}} = \int_{t_{j-1}}^{t_j} 2\frac{\sigma&#39;(s)}{\sigma(s)} X_s \;\mathrm{d}s + \int_{t_{j-1}}^{t_j} g(s){\small \triangledown}\mathrm{d}{\hat W}_s.\]</p><p>which we approximate with</p><p class="math-container">\[X_{t_j} - X_{t_{j-1}} \approx 2\frac{\sigma&#39;(t_j)}{\sigma(t_j)} X_{t_j} (t_j - t_{j-1}) + g(t_j) ({\hat W}_{t_j} - {\hat W}_{t_{j-1}}).\]</p><p>We implement this example below, in order to recover, backwards, the specific paths of the forward diffusion.</p><h2 id="Numerics"><a class="docs-heading-anchor" href="#Numerics">Numerics</a><a id="Numerics-1"></a><a class="docs-heading-anchor-permalink" href="#Numerics" title="Permalink"></a></h2><pre><code class="language-julia hljs">using StatsPlots
using Random
using Distributions
using Markdown</code></pre><pre><code class="language-julia hljs">rng = Xoshiro(12345)</code></pre><pre><code class="language-julia hljs">trange = range(0.0, 1.0, step=0.01)
numsamples = 1024

sigma(t) = t
sigmaprime(t) = 1
g(t) = sqrt(2 * sigma(t) * sigmaprime(t))

x0 = 0.0

Xt = zeros(size(trange, 1), numsamples)
dt = Float64(trange.step)
dWt = sqrt(dt) .* randn(length(trange), numsamples)
Wt = zero(Xt)
@assert axes(Xt, 1) == axes(trange, 1)
@inbounds for m in axes(Xt, 2)
    n1 = first(eachindex(axes(Xt, 1), axes(trange, 1)))
    Xt[n1, m] = x0
    Wt[n1, m] = 0.0
    @inbounds for n in Iterators.drop(eachindex(axes(trange,1), axes(Xt, 1)), 1)
        Xt[n, m] = Xt[n1, m] + g(trange[n1]) * dWt[n1, m]
        Wt[n, m] = Wt[n1, m] + dWt[n1, m]
        n1 = n
    end
end</code></pre><pre><code class="language-julia hljs">histogram(title=&quot;histogram of Xt&quot;, titlefont=10, Xt[end, :], bins=40)</code></pre><img src="8be886e9.svg" alt="Example block output"/><pre><code class="language-julia hljs">histogram(title=&quot;histogram of Wt&quot;, titlefont=10, Wt[end, :], bins=40)</code></pre><img src="64c46b3a.svg" alt="Example block output"/><pre><code class="language-julia hljs">plot(title=&quot;Sample paths of Xt&quot;, titlefont=10)
plot!(trange, Xt[:, 1:200], color=1, alpha=0.2, legend=false)
plot!(trange, Xt[:, 1:5], color=2, linewidth=1.5, legend=false)</code></pre><img src="b5ffb21e.svg" alt="Example block output"/><pre><code class="language-julia hljs">barWt = zero(Xt)
Vt = zero(Xt)
for m in axes(barWt, 2)
    n1 = first(eachindex(axes(barWt, 1), axes(trange, 1)))
    barWt[n1, m] = 0.0
    Vt[n1, m] = 0.0
    @inbounds for n in Iterators.drop(eachindex(axes(trange,1), axes(barWt, 1), axes(Xt, 1), axes(Vt, 1)), 1)
        Vt[n, m] = Vt[n1, m] - g(trange[n]) / sigma(trange[n])^2 * Xt[n1, m] * dt
        barWt[n, m] = Wt[n, m] + Vt[n, m]
        n1 = n
    end
end</code></pre><pre><code class="language-julia hljs">histogram(title=&quot;histogram of barWt&quot;, titlefont=10, barWt[end, :], bins=40)</code></pre><img src="4480e0b8.svg" alt="Example block output"/><pre><code class="language-julia hljs">plot(title=&quot;Sample paths Wt&quot;, titlefont=10, ylims=(-4, 4))
plot!(trange, Wt[:, 1:200], color=1, alpha=0.2, label=false)
plot!(trange, Wt[:, 1:5], color=2, linewidth=1.5, label=false)
plot!(trange, mean(barWt, dims=2), color=3, label=&quot;mean&quot;)
plot!(trange, sqrt.(mean(Wt .^2, dims=2)), color=4, label=&quot;std. dev.&quot;)
plot!(trange, -sqrt.(mean(Wt .^2, dims=2)), color=4, label=false)
plot!(trange, t -&gt; sqrt(t), color=5, label=&quot;t ↦ √t&quot;)
plot!(trange, t -&gt; -sqrt(t), color=5, label=false)</code></pre><img src="7b800e04.svg" alt="Example block output"/><pre><code class="language-julia hljs">plot(title=&quot;Sample paths barWt&quot;, titlefont=10, ylims=(-4, 4))
plot!(trange, barWt[:, 1:200], color=1, alpha=0.2, label=false)
plot!(trange, barWt[:, 1:5], color=2, linewidth=1.5, label=false)
plot!(trange, mean(barWt, dims=2), color=3, label=&quot;mean&quot;)
plot!(trange, sqrt.(mean(barWt .^2, dims=2)), color=4, label=&quot;std. dev.&quot;)
plot!(trange, -sqrt.(mean(barWt .^2, dims=2)), color=4, label=false)
plot!(trange, t -&gt; sqrt(t), color=5, label=&quot;t ↦ √t&quot;)
plot!(trange, t -&gt; -sqrt(t), color=5, label=false)</code></pre><img src="ae584cc4.svg" alt="Example block output"/><p>Oddly enough, the distribution of <span>$\bar W_t$</span> is a little bit squeezed, but the reverse flow is working nonetheless. But something is amiss.</p><pre><code class="language-julia hljs">Xtback = zeros(size(trange, 1), numsamples)

dt = Float64(trange.step)
@assert axes(Xtback, 1) == axes(trange, 1)
for m in axes(Xtback, 2)
    n1 = last(eachindex(axes(Xtback, 1), axes(trange, 1)))
    Xtback[n1, m] = Xt[end, m]
    for n in Iterators.drop(Iterators.reverse(eachindex(axes(trange,1), axes(Xtback, 1), axes(barWt, 1))), 1)
        Xtback[n, m] = Xtback[n1, m] - 2 * sigmaprime(trange[n1]) / sigma(trange[n1]) * Xtback[n1, m] * dt - g(trange[n1]) * (barWt[n1, m] - barWt[n, m])
        n1 = n
    end
end</code></pre><pre><code class="language-julia hljs">plot(title=&quot;Sample paths reverse Xt&quot;, titlefont=10)
plot!(trange, Xtback[:, 1:200], color=1, alpha=0.2, legend=false)
plot!(trange, Xtback[:, 1:5], color=2, linewidth=1.5, legend=false)</code></pre><img src="71bd96c9.svg" alt="Example block output"/><pre><code class="language-julia hljs">plot(title=&quot;Some sample paths of both Xt (blue) and reverse Xt (red)&quot;, titlefont=10, legend=false)
plot!(trange, Xt[:, 1:5], color=1, label=&quot;forward&quot;)
plot!(trange, Xtback[:, 1:5], color=2, linewidth=1.5, label=&quot;reverse&quot;)</code></pre><img src="a64afdb0.svg" alt="Example block output"/><pre><code class="language-julia hljs">nothing</code></pre><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><ol><li><a href="https://doi.org/10.1016/0304-4149(82)90051-5">B. D. O. Anderson (1982). Reverse-time diffusion equation models, Stochastic Process. Appl., vol. 12, no. 3, 313–326, DOI: 10.1016/0304-4149(82)90051-5</a></li><li><a href="https://doi.org/10.1214/aop/1176992362">U. G. Haussmann, E. Pardoux (1986). Time reversal of diffusions, Ann. Probab. 14, no. 4, 1188-1205</a></li><li><a href="https://doi.org/10.3390/e22080802">D. Maoutsa, S. Reich, M. Opper (2020), &quot;Interacting particle solutions of Fokker-Planck equations through gradient-log-density estimation&quot;, Entropy, 22(8), 802, DOI: 10.3390/e22080802</a></li><li><a href="https://arxiv.org/abs/2011.13456">Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, B. Poole (2020), &quot;Score-based generative modeling through stochastic differential equations&quot;, arXiv:2011.13456</a></li><li><a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/a98846e9d9cc01cfb87eb694d946ce6b-Abstract-Conference.html">T. Karras, M. Aittala, T. Aila, S. Laine (2022), Elucidating the design space of diffusion-based generative models, Advances in Neural Information Processing Systems 35 (NeurIPS 2022)</a></li></ol></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../probability_flow/">« Probability flow</a><a class="docs-footer-nextpage" href="../score_based_sde/">Score-based SDE model »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.15.0 on <span class="colophon-date" title="Thursday 6 November 2025 15:39">Thursday 6 November 2025</span>. Using Julia version 1.12.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
