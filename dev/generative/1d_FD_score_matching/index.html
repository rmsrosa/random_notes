<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>1D finite-difference score matching · Random notes</title><meta name="title" content="1D finite-difference score matching · Random notes"/><meta property="og:title" content="1D finite-difference score matching · Random notes"/><meta property="twitter:title" content="1D finite-difference score matching · Random notes"/><meta name="description" content="Documentation for Random notes."/><meta property="og:description" content="Documentation for Random notes."/><meta property="twitter:description" content="Documentation for Random notes."/><meta property="og:url" content="https://github.com/rmsrosa/random_notes/generative/1d_FD_score_matching/"/><meta property="twitter:url" content="https://github.com/rmsrosa/random_notes/generative/1d_FD_score_matching/"/><link rel="canonical" href="https://github.com/rmsrosa/random_notes/generative/1d_FD_score_matching/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/style.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="Random notes logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Random notes</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Random Notes</a></li><li><span class="tocitem">Probability Essentials</span><ul><li><a class="tocitem" href="../../probability/kernel_density_estimation/">Kernel Density Estimation</a></li><li><a class="tocitem" href="../../probability/convergence_notions/">Convergence notions</a></li></ul></li><li><span class="tocitem">Discrete-time Markov chains</span><ul><li><a class="tocitem" href="../../markov_chains/mc_definitions/">Essential definitions</a></li><li><a class="tocitem" href="../../markov_chains/mc_invariance/">Invariant distributions</a></li><li><input class="collapse-toggle" id="menuitem-3-3" type="checkbox"/><label class="tocitem" for="menuitem-3-3"><span class="docs-label">Countable-space Markov chains</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../markov_chains/mc_countableX_recurrence/">Recurrence in the countable-space case</a></li><li><a class="tocitem" href="../../markov_chains/mc_countableX_connections/">Connected states, irreducibility and uniqueness of invariant measures</a></li><li><a class="tocitem" href="../../markov_chains/mc_countableX_convergencia/">Aperiodicidade e convergência para a distribuição estacionária</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-4" type="checkbox"/><label class="tocitem" for="menuitem-3-4"><span class="docs-label">Continuous-space Markov chains</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../markov_chains/mc_irreducibility_and_recurrence/">Irreducibility and recurrence in the continuous-space case</a></li></ul></li></ul></li><li><span class="tocitem">Sampling methods</span><ul><li><a class="tocitem" href="../../sampling/overview/">Overview</a></li><li><a class="tocitem" href="../../sampling/prng/">Random number generators</a></li><li><input class="collapse-toggle" id="menuitem-4-3" type="checkbox"/><label class="tocitem" for="menuitem-4-3"><span class="docs-label">Transform methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/invFtransform/">Probability integral transform</a></li><li><a class="tocitem" href="../../sampling/box_muller/">Box-Muller transform</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-4" type="checkbox"/><label class="tocitem" for="menuitem-4-4"><span class="docs-label">Accept-Reject methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/rejection_sampling/">Rejection sampling</a></li><li><a class="tocitem" href="../../sampling/empiricalsup_rejection/">Empirical supremum rejection sampling</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-5" type="checkbox"/><label class="tocitem" for="menuitem-4-5"><span class="docs-label">Markov Chain Monte Carlo (MCMC)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/mcmc/">Overview</a></li><li><a class="tocitem" href="../../sampling/metropolis/">Metropolis and Metropolis-Hastings</a></li><li><a class="tocitem" href="../../sampling/convergence_metropolis/">Convergence of Metropolis-Hastings</a></li><li><a class="tocitem" href="../../sampling/gibbs/">Gibbs sampling</a></li><li><a class="tocitem" href="../../sampling/hmc/">Hamiltonian Monte Carlo (HMC)</a></li></ul></li><li><a class="tocitem" href="../../sampling/langevin_sampling/">Langevin sampling</a></li></ul></li><li><span class="tocitem">Bayesian inference</span><ul><li><input class="collapse-toggle" id="menuitem-5-1" type="checkbox"/><label class="tocitem" for="menuitem-5-1"><span class="docs-label">Bayes Theory</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/bayes/">Bayes Theorem</a></li><li><a class="tocitem" href="../../bayesian/bayes_inference/">Bayesian inference</a></li><li><a class="tocitem" href="../../bayesian/bernstein_vonmises/">Bernstein–von Mises theorem</a></li></ul></li><li><a class="tocitem" href="../../bayesian/bayesian_probprog/">Bayesian probabilistic programming</a></li><li><input class="collapse-toggle" id="menuitem-5-3" type="checkbox"/><label class="tocitem" for="menuitem-5-3"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/find_pi/">Estimating π via frequentist and Bayesian methods</a></li><li><a class="tocitem" href="../../bayesian/linear_regression/">Many Ways to Linear Regression</a></li><li><a class="tocitem" href="../../bayesian/tilapia_alometry/">Alometry law for the Nile Tilapia</a></li><li><a class="tocitem" href="../../bayesian/mortality_tables/">Modeling mortality tables</a></li></ul></li></ul></li><li><span class="tocitem">Generative models</span><ul><li><input class="collapse-toggle" id="menuitem-6-1" type="checkbox" checked/><label class="tocitem" for="menuitem-6-1"><span class="docs-label">Score matching</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../overview/">Overview</a></li><li><a class="tocitem" href="../stein_score/">Stein score function</a></li><li><a class="tocitem" href="../score_matching_aapo/">Score matching of Aapo Hyvärinen</a></li><li><a class="tocitem" href="../score_matching_neural_network/">Score matching a neural network</a></li><li><a class="tocitem" href="../parzen_estimation_score_matching/">Score matching with Parzen estimation</a></li><li><a class="tocitem" href="../denoising_score_matching/">Denoising score matching of Pascal Vincent</a></li><li><a class="tocitem" href="../sliced_score_matching/">Sliced score matching</a></li><li class="is-active"><a class="tocitem" href>1D finite-difference score matching</a><ul class="internal"><li><a class="tocitem" href="#Introduction"><span>Introduction</span></a></li><li><a class="tocitem" href="#The-finite-difference-implicit-score-matching-method"><span>The finite-difference implicit score matching method</span></a></li><li><a class="tocitem" href="#Numerical-example"><span>Numerical example</span></a></li><li><a class="tocitem" href="#Code-introspection"><span>Code introspection</span></a></li><li><a class="tocitem" href="#Data"><span>Data</span></a></li><li><a class="tocitem" href="#The-neural-network-model"><span>The neural network model</span></a></li><li><a class="tocitem" href="#Optimization-setup"><span>Optimization setup</span></a></li><li><a class="tocitem" href="#Training"><span>Training</span></a></li><li><a class="tocitem" href="#The-need-for-enough-sample-points"><span>The need for enough sample points</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../2d_FD_score_matching/">2D finite-difference score matching</a></li><li><a class="tocitem" href="../ddpm/">Denoising diffusion probabilistic models</a></li><li><a class="tocitem" href="../mdsm/">Multiple denoising score matching</a></li><li><a class="tocitem" href="../probability_flow/">Probability flow</a></li><li><a class="tocitem" href="../reverse_flow/">Reverse probability flow</a></li><li><a class="tocitem" href="../score_based_sde/">Score-based SDE model</a></li></ul></li></ul></li><li><span class="tocitem">Sensitivity analysis</span><ul><li><a class="tocitem" href="../../sensitivity/overview/">Overview</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Generative models</a></li><li><a class="is-disabled">Score matching</a></li><li class="is-active"><a href>1D finite-difference score matching</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>1D finite-difference score matching</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/rmsrosa/random_notes" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/rmsrosa/random_notes/blob/main/docs/src/generative/1d_FD_score_matching.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Finite-difference-score-matching-of-a-one-dimensional-Gaussian-mixture-model"><a class="docs-heading-anchor" href="#Finite-difference-score-matching-of-a-one-dimensional-Gaussian-mixture-model">Finite-difference score-matching of a one-dimensional Gaussian mixture model</a><a id="Finite-difference-score-matching-of-a-one-dimensional-Gaussian-mixture-model-1"></a><a class="docs-heading-anchor-permalink" href="#Finite-difference-score-matching-of-a-one-dimensional-Gaussian-mixture-model" title="Permalink"></a></h1><h2 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h2><h3 id="Aim"><a class="docs-heading-anchor" href="#Aim">Aim</a><a id="Aim-1"></a><a class="docs-heading-anchor-permalink" href="#Aim" title="Permalink"></a></h3><p>The aim, this time, is to fit a neural network via <strong>finite-difference score matching</strong>, following the pioneering work of <a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Aapo Hyvärinen (2005)</a> about score-matching, combined with the work of <a href="https://openreview.net/forum?id=LVRoKppWczk">Pang, Xu, Li, Song, Ermon, and Zhu (2020)</a>, which uses finite differences to efficiently approximate the gradient in the loss function proposed by <a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Aapo Hyvärinen (2005)</a>, and the idea of 1. <a href="https://dl.acm.org/doi/10.5555/3454287.3455354">Song and Ermon (2019)</a> of modeling directly the score function instead of the pdf or an energy potential of the pdf.</p><h3 id="Background"><a class="docs-heading-anchor" href="#Background">Background</a><a id="Background-1"></a><a class="docs-heading-anchor-permalink" href="#Background" title="Permalink"></a></h3><p>Generative score-matching diffusion methods use Langevin dynamics to draw samples from a modeled score function. It rests on the idea of <a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Aapo Hyvärinen (2005)</a> that one can directly fit the score function, from the sample data, using a suitable loss function (associated with the Fisher divergence) not depending on the unknown score function of the random variable. This is obtained by a simple integration by parts on the expected square distance between the model score function and the actual score function. The integration by parts separates the dependence on the actual score function from the parameters of the model, so the fitting process (minimization over the parameters of the model) does not depend on the unknown score function.</p><p>The obtained loss function, however, depends on the gradient of the model, which is computationally expensive. <a href="https://openreview.net/forum?id=LVRoKppWczk">Pang, Xu, Li, Song, Ermon, and Zhu (2020)</a> proposed to use finite differences to approximate the derivative of the model to significantly reduce the computational cost of training the model.</p><p>The differentiation for the optimization is with respect to the parameters, while the differentiation of the modeled score function is on the variate, but still this is a great computational challenge and not all AD are fit for that. For this reason, we resort to centered finite differences to approximate the derivative of the modeled score function.</p><p>For a python version of a similar pedagogical example, see <a href="https://ericmjl.github.io/score-models/">Eric J. Ma (2021)</a>. There, they use AD on top of AD, via the <a href="https://github.com/google/jax">google/jax</a> library, which apparently handles this double-AD not so badly.</p><h3 id="Take-away"><a class="docs-heading-anchor" href="#Take-away">Take away</a><a id="Take-away-1"></a><a class="docs-heading-anchor-permalink" href="#Take-away" title="Permalink"></a></h3><p>We&#39;ll see that, in this simple example at least, we don&#39;t need a large or deep neural network. It is much more important to have enough sample points to capture the transition region in the mixture of Gaussians.</p><h2 id="The-finite-difference-implicit-score-matching-method"><a class="docs-heading-anchor" href="#The-finite-difference-implicit-score-matching-method">The finite-difference implicit score matching method</a><a id="The-finite-difference-implicit-score-matching-method-1"></a><a class="docs-heading-anchor-permalink" href="#The-finite-difference-implicit-score-matching-method" title="Permalink"></a></h2><p>The score-matching method from <a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Aapo Hyvärinen (2005)</a> rests on the following ideas:</p><p><strong>1.</strong> Fit the model by minimizing the expected square distance between the model score function <span>$\psi(x; {\boldsymbol{\theta}})$</span> and the score function <span>$\psi_X(x)$</span> of the random variable <span>$X$</span>, via the <strong>explicit score matching (ESM)</strong> objective</p><p class="math-container">\[    J_{\mathrm{ESM}}({\boldsymbol{\theta}}) = \frac{1}{2}\int_{\mathbb{R}^d} p_{\mathbf{X}}(\mathbf{x}) \left\|\boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}}) - \boldsymbol{\psi}_{\mathbf{X}}(\mathbf{x})\right\|^2\;\mathrm{d}\mathbf{x}.\]</p><p><strong>2.</strong> Use integration by parts in the expectation to write that</p><p class="math-container">\[    J_{\mathrm{ESM}}({\boldsymbol{\theta}}) = J_{\mathrm{ISM}}({\boldsymbol{\theta}}) + C,\]</p><p>where <span>$C$</span> is constant with respect to the parameters, so we only need to minimize the <strong>implicit score matching (ISM)</strong> objective <span>${\tilde J}_{\mathrm{ISM}}$</span>, given by</p><p class="math-container">\[    J_{\mathrm{ISM}}({\boldsymbol{\theta}}) = \int_{\mathbb{R}} p_{\mathbf{X}}(\mathbf{x}) \left( \frac{1}{2}\left\|\boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}})\right\|^2 + \boldsymbol{\nabla}_{\mathbf{x}} \cdot \boldsymbol{\psi}(\mathbf{x}; {\boldsymbol{\theta}}) \right)\;\mathrm{d}\mathbf{x},\]</p><p>which does not involve the unknown score function of <span>${\mathbf{X}}$</span>. It does, however, involve the gradient of the modeled score function, which is expensive to compute.</p><p><strong>3.</strong> In practice, the implicit score-matching loss function, which depends on the unknown <span>$p_\mathbf{X}(\mathbf{x})$</span>, is estimated via the empirical distribution, obtained from the sample data <span>$(\mathbf{x}_n)_n$</span>. Thus, we minimize the <strong>empirical implicit score matching</strong> objective</p><p class="math-container">\[    {\tilde J}_{\mathrm{ISM}{\tilde p}_0} =  \frac{1}{N}\sum_{n=1}^N \left( \frac{1}{2}\|\boldsymbol{\psi}(\mathbf{x}_n; {\boldsymbol{\theta}})\|^2 + \boldsymbol{\nabla}_{\mathbf{x}} \cdot \boldsymbol{\psi}(\mathbf{x}_n; {\boldsymbol{\theta}}) \right).\]</p><p>where the <em>empirical distribution</em> is given by <span>${\tilde p}_0 = (1/N)\sum_{n=1}^N \delta_{\mathbf{x}_n}.$</span></p><p>On top of that, we add one more step.</p><p><strong>4.</strong> As mentioned before, computing a derivative to form the loss function becomes expensive when combined with the usual optimization methods to fit a neural network, as they require the gradient of the loss function itself, i.e. the optimization process involves the gradient of the gradient of something. Because of that, one alternative is to approximate the derivative of the model score function by centered finite differences, i.e.</p><p class="math-container">\[    \frac{\partial}{\partial x} \psi(x_n; {\boldsymbol{\theta}}) \approx \frac{\psi(x_n + \delta; {\boldsymbol{\theta}}) - \psi(x_n - \delta; {\boldsymbol{\theta}})}{2\delta},\]</p><p>for a suitably small <span>$\delta &gt; 0$</span>.</p><p>In this case, since we need compute <span>$\psi(x_n + \delta; {\boldsymbol{\theta}})$</span> and <span>$\psi(x_n - \delta; {\boldsymbol{\theta}})$</span>, we avoid computing also <span>$\psi(x_n; {\boldsymbol{\theta}})$</span> and approximate it with the average</p><p class="math-container">\[    \psi(x_n; {\boldsymbol{\theta}}) \approx \frac{\psi(x_n + \delta; {\boldsymbol{\theta}}) + \psi(x_n - \delta; {\boldsymbol{\theta}})}{2}.\]</p><p>Hence, we approximate the implicit score matching <span>${\tilde J}_{\mathrm{ISM}{\tilde p}_0}$</span> by the <strong>finite-difference (implicit) score matching</strong></p><p class="math-container">\[    {\tilde J}_{\mathrm{FDSM}}({\boldsymbol{\theta}}) = \int_{\mathbb{R}} p_X(x) \Bigg( \frac{1}{2}\left(\frac{\psi(x + \delta; {\boldsymbol{\theta}}) + \psi(x - \delta; {\boldsymbol{\theta}})}{2}\right)^2 + \frac{\psi(x + \delta; {\boldsymbol{\theta}}) - \psi(x - \delta; {\boldsymbol{\theta}})}{2\delta} \Bigg)\;\mathrm{d}x,\]</p><p>And the empirical implicit score matching <span>${\tilde J}_{\mathrm{ISM}{\tilde p}_0}$</span> is approximated by</p><p class="math-container">\[    {\tilde J}_{\mathrm{FDSM}{\tilde p}_0} =  \frac{1}{N}\sum_{n=1}^N \Bigg( \frac{1}{2}\left(\frac{\psi(x + \delta; {\boldsymbol{\theta}}) + \psi(x - \delta; {\boldsymbol{\theta}})}{2}\right)^2 + \frac{\psi(x + \delta; {\boldsymbol{\theta}}) - \psi(x - \delta; {\boldsymbol{\theta}})}{2\delta} \Bigg).\]</p><h2 id="Numerical-example"><a class="docs-heading-anchor" href="#Numerical-example">Numerical example</a><a id="Numerical-example-1"></a><a class="docs-heading-anchor-permalink" href="#Numerical-example" title="Permalink"></a></h2><p>We illustrate the above method by fitting a neural network to a univariate Gaussian mixture distribution.</p><p>We played with different target distributions and settled here with a bimodal distribution used in <a href="https://ericmjl.github.io/score-models/">Eric J. Ma (2021)</a>.</p><h3 id="Julia-language-setup"><a class="docs-heading-anchor" href="#Julia-language-setup">Julia language setup</a><a id="Julia-language-setup-1"></a><a class="docs-heading-anchor-permalink" href="#Julia-language-setup" title="Permalink"></a></h3><p>We use the <a href="https://julialang.org">Julia programming language</a> with suitable packages.</p><h4 id="Packages"><a class="docs-heading-anchor" href="#Packages">Packages</a><a id="Packages-1"></a><a class="docs-heading-anchor-permalink" href="#Packages" title="Permalink"></a></h4><pre><code class="language-julia hljs">using StatsPlots
using Random
using Distributions
using Lux # artificial neural networks explicitly parametrized
using Optimisers
using Zygote # automatic differentiation</code></pre><h4 id="Reproducibility"><a class="docs-heading-anchor" href="#Reproducibility">Reproducibility</a><a id="Reproducibility-1"></a><a class="docs-heading-anchor-permalink" href="#Reproducibility" title="Permalink"></a></h4><p>We set the random seed for reproducibility purposes.</p><pre><code class="language-julia hljs">rng = Xoshiro(12345)</code></pre><h2 id="Code-introspection"><a class="docs-heading-anchor" href="#Code-introspection">Code introspection</a><a id="Code-introspection-1"></a><a class="docs-heading-anchor-permalink" href="#Code-introspection" title="Permalink"></a></h2><p>We do not attempt to overly optimize the code here since this is a simple one-dimensional problem. Nevertheless, it is always healthy to check the type stability of the critical parts (like the loss functions) with <code>@code_warntype</code>. One should also check for any unusual time and allocation with <code>BenchmarkTools.@btime</code> or <code>BenchmarkTools.@benchmark</code>. We performed these analysis and everything seems good. We found it unnecessary to clutter the notebook with their outputs here, though.</p><h2 id="Data"><a class="docs-heading-anchor" href="#Data">Data</a><a id="Data-1"></a><a class="docs-heading-anchor-permalink" href="#Data" title="Permalink"></a></h2><p>We build the target model and draw samples from it. We need enough sample points to capture the transition region in the mixture of Gaussians.</p><pre><code class="language-julia hljs">xrange = range(-10, 10, 200)
dx = Float64(xrange.step)
x = permutedims(collect(xrange))

target_prob = MixtureModel([Normal(-3, 1), Normal(3, 1)], [0.1, 0.9])

target_pdf = pdf.(target_prob, x)
target_score = gradlogpdf.(target_prob, x)

y = target_score # just to simplify the notation
sample_points = permutedims(rand(rng, target_prob, 1024))
data = (x, y, target_pdf, sample_points)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([-10.0 -9.899497487437186 … 9.899497487437186 10.0], [7.0 6.899497487437186 … -6.899497487437186 -7.0], [9.134720408364594e-13 1.8366893783972853e-12 … 1.6530204405575567e-11 8.221248367528135e-12], [2.303077959422043 2.8428423932782843 … 3.1410080972036334 2.488464630750972])</code></pre><p>Notice the data <code>x</code> and <code>sample_points</code> are defined as row vectors so we can apply the model in batch to all of their values at once. The values <code>y</code> are also row vectors for easy comparison with the predicted values. When, plotting, though, we need to revert them to vectors.</p><p>Visualizing the sample data drawn from the distribution and the PDF.</p><pre><code class="language-julia hljs">plot(title=&quot;PDF and histogram of sample data from the distribution&quot;, titlefont=10)
histogram!(sample_points&#39;, normalize=:pdf, nbins=80, label=&quot;sample histogram&quot;)
plot!(x&#39;, target_pdf&#39;, linewidth=4, label=&quot;pdf&quot;)
scatter!(sample_points&#39;, s -&gt; pdf(target_prob, s), linewidth=4, label=&quot;sample&quot;)</code></pre><img src="495cd663.svg" alt="Example block output"/><p>Visualizing the score function.</p><pre><code class="language-julia hljs">plot(title=&quot;The score function and the sample&quot;, titlefont=10)

plot!(x&#39;, target_score&#39;, label=&quot;score function&quot;, markersize=2)
scatter!(sample_points&#39;, s -&gt; gradlogpdf(target_prob, s), label=&quot;data&quot;, markersize=2)</code></pre><img src="c874d03f.svg" alt="Example block output"/><h2 id="The-neural-network-model"><a class="docs-heading-anchor" href="#The-neural-network-model">The neural network model</a><a id="The-neural-network-model-1"></a><a class="docs-heading-anchor-permalink" href="#The-neural-network-model" title="Permalink"></a></h2><p>The neural network we consider is a simple feed-forward neural network made of a single hidden layer, obtained as a chain of a couple of dense layers. This is implemented with the <a href="https://github.com/LuxDL/Lux.jl">LuxDL/Lux.jl</a> package.</p><p>We will see, again, that we don&#39;t need a big neural network in this simple example. We go as low as it works.</p><pre><code class="language-julia hljs">model = Chain(Dense(1 =&gt; 8, relu), Dense(8 =&gt; 1))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
    layer_1 = Dense(1 =&gt; 8, relu),      <span class="sgr90"># 16 parameters</span>
    layer_2 = Dense(8 =&gt; 1),            <span class="sgr90"># 9 parameters</span>
) <span class="sgr90">        # Total: </span>25 parameters,
<span class="sgr90">          #        plus </span>0 states.</code></pre><p>The <a href="https://github.com/LuxDL/Lux.jl">LuxDL/Lux.jl</a> package uses explicit parameters, that are initialized (or obtained) with the <code>Lux.setup</code> function, giving us the <em>parameters</em> and the <em>state</em> of the model.</p><pre><code class="language-julia hljs">ps, st = Lux.setup(rng, model) # initialize and get the parameters and states of the model</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">((layer_1 = (weight = Float32[-0.003556765; -1.8715183; … ; 0.66702616; -0.9373461;;], bias = Float32[0.18317068, 0.5787344, -0.18110967, 0.9307035, -0.43067825, -0.46645045, -0.8246051, -0.9340805]), layer_2 = (weight = Float32[0.27326134 -0.2086962 … 0.42855448 0.5658726], bias = Float32[0.09530755])), (layer_1 = NamedTuple(), layer_2 = NamedTuple()))</code></pre><h3 id="Explicit-score-matching-loss-function-J_{\\mathrm{ESM}}({\\boldsymbol{\\theta}})"><a class="docs-heading-anchor" href="#Explicit-score-matching-loss-function-J_{\\mathrm{ESM}}({\\boldsymbol{\\theta}})">Explicit score matching loss function <span>$J_{\mathrm{ESM}}({\boldsymbol{\theta}})$</span></a><a id="Explicit-score-matching-loss-function-J_{\\mathrm{ESM}}({\\boldsymbol{\\theta}})-1"></a><a class="docs-heading-anchor-permalink" href="#Explicit-score-matching-loss-function-J_{\\mathrm{ESM}}({\\boldsymbol{\\theta}})" title="Permalink"></a></h3><p>For educational purposes, since we have the pdf and the score function, one of the ways we may train the model is directly with <span>$J_{\mathrm{ESM}}({\boldsymbol{\theta}})$</span>. This is also useful to make sure that our network is able to model the desired score function.</p><p>Here is how we implement it.</p><pre><code class="language-julia hljs">function loss_function_esm(model, ps, st, data)
    x, y, target_pdf, sample_points = data
    y_pred, st = Lux.apply(model, x, ps, st)
    loss = mean(target_pdf .* (y_pred .- y) .^2)
    return loss, st, ()
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">loss_function_esm (generic function with 1 method)</code></pre><h3 id="Plain-square-error-loss-function"><a class="docs-heading-anchor" href="#Plain-square-error-loss-function">Plain square error loss function</a><a id="Plain-square-error-loss-function-1"></a><a class="docs-heading-anchor-permalink" href="#Plain-square-error-loss-function" title="Permalink"></a></h3><p>Still for educational purposes, we modify <span>$J_{\mathrm{ESM}}({\boldsymbol{\theta}})$</span> for training, without weighting on the distribution of the random variable itself, as in <span>$J_{\mathrm{ESM}}({\boldsymbol{\theta}})$</span>. This has the benefit of giving more weight to the transition region. Here is how we implement it.</p><pre><code class="language-julia hljs">function loss_function_esm_plain(model, ps, st, data)
    x, y, target_pdf, sample_points = data
    y_pred, st = Lux.apply(model, x, ps, st)
    loss = mean(abs2, y_pred .- y)
    return loss, st, ()
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">loss_function_esm_plain (generic function with 1 method)</code></pre><h3 id="Finite-difference-score-matching-{\\tilde-J}_{\\mathrm{FDSM}}"><a class="docs-heading-anchor" href="#Finite-difference-score-matching-{\\tilde-J}_{\\mathrm{FDSM}}">Finite-difference score matching <span>${\tilde J}_{\mathrm{FDSM}}$</span></a><a id="Finite-difference-score-matching-{\\tilde-J}_{\\mathrm{FDSM}}-1"></a><a class="docs-heading-anchor-permalink" href="#Finite-difference-score-matching-{\\tilde-J}_{\\mathrm{FDSM}}" title="Permalink"></a></h3><p>Again, for educational purposes, we may implement <span>${\tilde J}_{\mathrm{FDSM}}({\boldsymbol{\theta}})$</span>, as follows.</p><pre><code class="language-julia hljs">function loss_function_FDSM(model, ps, st, data)
    x, y, target_pdf, sample_points = data
    xmin, xmax = extrema(x)
    delta = (xmax - xmin) / 2length(x)
    y_pred_fwd, = Lux.apply(model, x .+ delta, ps, st)
    y_pred_bwd, = Lux.apply(model, x .- delta, ps, st)
    y_pred = ( y_pred_bwd .+ y_pred_fwd ) ./ 2
    dy_pred = (y_pred_fwd .- y_pred_bwd ) ./ 2delta
    loss = mean(target_pdf .* (dy_pred + y_pred .^ 2 / 2))
    return loss, st, ()
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">loss_function_FDSM (generic function with 1 method)</code></pre><h3 id="Empirical-finite-difference-score-matching-loss-function-{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}"><a class="docs-heading-anchor" href="#Empirical-finite-difference-score-matching-loss-function-{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}">Empirical finite-difference score matching loss function <span>${\tilde J}_{\mathrm{FDSM}{\tilde p}_0}$</span></a><a id="Empirical-finite-difference-score-matching-loss-function-{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}-1"></a><a class="docs-heading-anchor-permalink" href="#Empirical-finite-difference-score-matching-loss-function-{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}" title="Permalink"></a></h3><p>In practice we would use the sample data, not the supposedly unknown score function and PDF themselves. Here would be one implementation using finite differences, along with Monte-Carlo.</p><pre><code class="language-julia hljs">function loss_function_FDSM_over_sample(model, ps, st, data)
    x, y, target_pdf, sample_points = data
    xmin, xmax = extrema(sample_points)
    delta = (xmax - xmin) / 2length(sample_points)
    y_pred_fwd, = Lux.apply(model, sample_points .+ delta, ps, st)
    y_pred_bwd, = Lux.apply(model, sample_points .- delta, ps, st)
    y_pred = ( y_pred_bwd .+ y_pred_fwd ) ./ 2
    dy_pred = (y_pred_fwd .- y_pred_bwd ) ./ 2delta
    loss = mean(dy_pred + y_pred .^ 2 / 2)
    return loss, st, ()
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">loss_function_FDSM_over_sample (generic function with 1 method)</code></pre><h2 id="Optimization-setup"><a class="docs-heading-anchor" href="#Optimization-setup">Optimization setup</a><a id="Optimization-setup-1"></a><a class="docs-heading-anchor-permalink" href="#Optimization-setup" title="Permalink"></a></h2><h3 id="Optimization-method"><a class="docs-heading-anchor" href="#Optimization-method">Optimization method</a><a id="Optimization-method-1"></a><a class="docs-heading-anchor-permalink" href="#Optimization-method" title="Permalink"></a></h3><p>We use the classical Adam optimiser (see <a href="https://www.semanticscholar.org/paper/Adam%3A-A-Method-for-Stochastic-Optimization-Kingma-Ba/a6cb366736791bcccc5c8639de5a8f9636bf87e8">Kingma and Ba (2015)</a>), which is a stochastic gradient-based optimization method.</p><pre><code class="language-julia hljs">opt = Adam(0.03)

tstate_org = Lux.Training.TrainState(model, ps, st, opt)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">TrainState
    model: Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.relu), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(1 =&gt; 8, relu), layer_2 = Dense(8 =&gt; 1)), nothing)
    # of parameters: 25
    # of states: 0
    optimizer: Optimisers.Adam(eta=0.03, beta=(0.9, 0.999), epsilon=1.0e-8)
    step: 0</code></pre><h3 id="Automatic-differentiation-in-the-optimization"><a class="docs-heading-anchor" href="#Automatic-differentiation-in-the-optimization">Automatic differentiation in the optimization</a><a id="Automatic-differentiation-in-the-optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Automatic-differentiation-in-the-optimization" title="Permalink"></a></h3><p>As mentioned, we setup differentiation in <a href="https://github.com/LuxDL/Lux.jl">LuxDL/Lux.jl</a> with the <a href="https://github.com/FluxML/Zygote.jl">FluxML/Zygote.jl</a> library, which is currently the only one implemented (there are pre-defined methods for <code>AutoForwardDiff()</code>, <code>AutoReverseDiff()</code>, <code>AutoFiniteDifferences()</code>, etc., but not implemented yet).</p><pre><code class="language-julia hljs">vjp_rule = Lux.Training.AutoZygote()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">ADTypes.AutoZygote()</code></pre><h3 id="Processor"><a class="docs-heading-anchor" href="#Processor">Processor</a><a id="Processor-1"></a><a class="docs-heading-anchor-permalink" href="#Processor" title="Permalink"></a></h3><p>We use the CPU instead of the GPU.</p><pre><code class="language-julia hljs">dev_cpu = cpu_device()
## dev_gpu = gpu_device()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(::MLDataDevices.CPUDevice) (generic function with 1 method)</code></pre><h3 id="Check-differentiation"><a class="docs-heading-anchor" href="#Check-differentiation">Check differentiation</a><a id="Check-differentiation-1"></a><a class="docs-heading-anchor-permalink" href="#Check-differentiation" title="Permalink"></a></h3><p>Check if Zygote via Lux is working fine to differentiate the loss functions for training.</p><pre><code class="language-julia hljs">Lux.Training.compute_gradients(vjp_rule, loss_function_esm, data, tstate_org)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">((layer_1 = (weight = Float32[-0.050562456; -0.0022956962; … ; -0.08161155; 0.006370581;;], bias = Float32[-0.023082083, 0.00034197676, -0.0100809485, -0.020624125, 0.029695434, 0.0067583956, -0.03278248, -0.00071924547]), layer_2 = (weight = Float32[-0.014814105 -0.021535374 … -0.063946225 -0.00936537], bias = Float32[-0.08446889])), 0.05891430321572992, (), Lux.Training.TrainState{Nothing, Nothing, Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.relu), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, @NamedTuple{layer_1::@NamedTuple{weight::Matrix{Float32}, bias::Vector{Float32}}, layer_2::@NamedTuple{weight::Matrix{Float32}, bias::Vector{Float32}}}, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}}, Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, @NamedTuple{layer_1::@NamedTuple{weight::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}}, layer_2::@NamedTuple{weight::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}}}}(nothing, nothing, Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.relu), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(1 =&gt; 8, relu), layer_2 = Dense(8 =&gt; 1)), nothing), (layer_1 = (weight = Float32[-0.003556765; -1.8715183; … ; 0.66702616; -0.9373461;;], bias = Float32[0.18317068, 0.5787344, -0.18110967, 0.9307035, -0.43067825, -0.46645045, -0.8246051, -0.9340805]), layer_2 = (weight = Float32[0.27326134 -0.2086962 … 0.42855448 0.5658726], bias = Float32[0.09530755])), (layer_1 = NamedTuple(), layer_2 = NamedTuple()), Optimisers.Adam(eta=0.03, beta=(0.9, 0.999), epsilon=1.0e-8), (layer_1 = (weight = <span class="sgr32">Leaf(Adam(eta=0.03, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0; 0.0; … ; 0.0; 0.0;;], Float32[0.0; 0.0; … ; 0.0; 0.0;;], (0.9, 0.999))<span class="sgr32">)</span>, bias = <span class="sgr32">Leaf(Adam(eta=0.03, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))<span class="sgr32">)</span>), layer_2 = (weight = <span class="sgr32">Leaf(Adam(eta=0.03, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0], (0.9, 0.999))<span class="sgr32">)</span>, bias = <span class="sgr32">Leaf(Adam(eta=0.03, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0], Float32[0.0], (0.9, 0.999))<span class="sgr32">)</span>)), 0))</code></pre><pre><code class="language-julia hljs">Lux.Training.compute_gradients(vjp_rule, loss_function_esm_plain, data, tstate_org)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">((layer_1 = (weight = Float32[6.3705935; -3.428723; … ; 3.0130339; 9.327687;;], bias = Float32[-0.45501328, 0.43466827, 0.045152973, 0.08273475, -0.13300705, -0.0490374, 0.26342168, -1.1487464]), layer_2 = (weight = Float32[-0.38792095 -31.953028 … 4.1827903 -13.554729], bias = Float32[-1.6651213])), 6.2528048955176025, (), Lux.Training.TrainState{Nothing, Nothing, Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.relu), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, @NamedTuple{layer_1::@NamedTuple{weight::Matrix{Float32}, bias::Vector{Float32}}, layer_2::@NamedTuple{weight::Matrix{Float32}, bias::Vector{Float32}}}, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}}, Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, @NamedTuple{layer_1::@NamedTuple{weight::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}}, layer_2::@NamedTuple{weight::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}}}}(nothing, nothing, Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.relu), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(1 =&gt; 8, relu), layer_2 = Dense(8 =&gt; 1)), nothing), (layer_1 = (weight = Float32[-0.003556765; -1.8715183; … ; 0.66702616; -0.9373461;;], bias = Float32[0.18317068, 0.5787344, -0.18110967, 0.9307035, -0.43067825, -0.46645045, -0.8246051, -0.9340805]), layer_2 = (weight = Float32[0.27326134 -0.2086962 … 0.42855448 0.5658726], bias = Float32[0.09530755])), (layer_1 = NamedTuple(), layer_2 = NamedTuple()), Optimisers.Adam(eta=0.03, beta=(0.9, 0.999), epsilon=1.0e-8), (layer_1 = (weight = <span class="sgr32">Leaf(Adam(eta=0.03, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0; 0.0; … ; 0.0; 0.0;;], Float32[0.0; 0.0; … ; 0.0; 0.0;;], (0.9, 0.999))<span class="sgr32">)</span>, bias = <span class="sgr32">Leaf(Adam(eta=0.03, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))<span class="sgr32">)</span>), layer_2 = (weight = <span class="sgr32">Leaf(Adam(eta=0.03, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0], (0.9, 0.999))<span class="sgr32">)</span>, bias = <span class="sgr32">Leaf(Adam(eta=0.03, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0], Float32[0.0], (0.9, 0.999))<span class="sgr32">)</span>)), 0))</code></pre><pre><code class="language-julia hljs">Lux.Training.compute_gradients(vjp_rule, loss_function_FDSM, data, tstate_org)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">((layer_1 = (weight = Float32[-0.025280774; -0.0011436436; … ; -0.040590584; 0.0031747743;;], bias = Float32[-0.011540726, 0.00018402562, -0.005043756, -0.010314912, 0.0148344785, 0.0033988245, -0.01621516, -0.00034894608]), layer_2 = (weight = Float32[-0.007406853 -0.010766149 … -0.031977117 -0.0046828836], bias = Float32[-0.042233348])), 0.00510874380657287, (), Lux.Training.TrainState{Nothing, Nothing, Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.relu), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, @NamedTuple{layer_1::@NamedTuple{weight::Matrix{Float32}, bias::Vector{Float32}}, layer_2::@NamedTuple{weight::Matrix{Float32}, bias::Vector{Float32}}}, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}}, Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, @NamedTuple{layer_1::@NamedTuple{weight::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}}, layer_2::@NamedTuple{weight::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}}}}(nothing, nothing, Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.relu), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(1 =&gt; 8, relu), layer_2 = Dense(8 =&gt; 1)), nothing), (layer_1 = (weight = Float32[-0.003556765; -1.8715183; … ; 0.66702616; -0.9373461;;], bias = Float32[0.18317068, 0.5787344, -0.18110967, 0.9307035, -0.43067825, -0.46645045, -0.8246051, -0.9340805]), layer_2 = (weight = Float32[0.27326134 -0.2086962 … 0.42855448 0.5658726], bias = Float32[0.09530755])), (layer_1 = NamedTuple(), layer_2 = NamedTuple()), Optimisers.Adam(eta=0.03, beta=(0.9, 0.999), epsilon=1.0e-8), (layer_1 = (weight = <span class="sgr32">Leaf(Adam(eta=0.03, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0; 0.0; … ; 0.0; 0.0;;], Float32[0.0; 0.0; … ; 0.0; 0.0;;], (0.9, 0.999))<span class="sgr32">)</span>, bias = <span class="sgr32">Leaf(Adam(eta=0.03, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))<span class="sgr32">)</span>), layer_2 = (weight = <span class="sgr32">Leaf(Adam(eta=0.03, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0], (0.9, 0.999))<span class="sgr32">)</span>, bias = <span class="sgr32">Leaf(Adam(eta=0.03, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0], Float32[0.0], (0.9, 0.999))<span class="sgr32">)</span>)), 0))</code></pre><pre><code class="language-julia hljs">Lux.Training.compute_gradients(vjp_rule, loss_function_FDSM_over_sample, data, tstate_org)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">((layer_1 = (weight = Float32[-0.527771; -0.021055222; … ; -0.8834152; 0.052393913;;], bias = Float32[-0.23545456, 0.0011318922, -0.10370636, -0.21179199, 0.30560684, 0.06583691, -0.3659439, -0.0032877922]), layer_2 = (weight = Float32[-0.15095806 -0.19195557 … -0.6708679 -0.08135986], bias = Float32[-0.86164093])), 0.11494219313979222, (), Lux.Training.TrainState{Nothing, Nothing, Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.relu), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}, @NamedTuple{layer_1::@NamedTuple{weight::Matrix{Float32}, bias::Vector{Float32}}, layer_2::@NamedTuple{weight::Matrix{Float32}, bias::Vector{Float32}}}, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}}, Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, @NamedTuple{layer_1::@NamedTuple{weight::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}}, layer_2::@NamedTuple{weight::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float32, Float32}}}, bias::Optimisers.Leaf{Optimisers.Adam{Float64, Tuple{Float64, Float64}, Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float32, Float32}}}}}}(nothing, nothing, Lux.Chain{@NamedTuple{layer_1::Lux.Dense{typeof(NNlib.relu), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Lux.Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(1 =&gt; 8, relu), layer_2 = Dense(8 =&gt; 1)), nothing), (layer_1 = (weight = Float32[-0.003556765; -1.8715183; … ; 0.66702616; -0.9373461;;], bias = Float32[0.18317068, 0.5787344, -0.18110967, 0.9307035, -0.43067825, -0.46645045, -0.8246051, -0.9340805]), layer_2 = (weight = Float32[0.27326134 -0.2086962 … 0.42855448 0.5658726], bias = Float32[0.09530755])), (layer_1 = NamedTuple(), layer_2 = NamedTuple()), Optimisers.Adam(eta=0.03, beta=(0.9, 0.999), epsilon=1.0e-8), (layer_1 = (weight = <span class="sgr32">Leaf(Adam(eta=0.03, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0; 0.0; … ; 0.0; 0.0;;], Float32[0.0; 0.0; … ; 0.0; 0.0;;], (0.9, 0.999))<span class="sgr32">)</span>, bias = <span class="sgr32">Leaf(Adam(eta=0.03, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))<span class="sgr32">)</span>), layer_2 = (weight = <span class="sgr32">Leaf(Adam(eta=0.03, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0], (0.9, 0.999))<span class="sgr32">)</span>, bias = <span class="sgr32">Leaf(Adam(eta=0.03, beta=(0.9, 0.999), epsilon=1.0e-8), </span>(Float32[0.0], Float32[0.0], (0.9, 0.999))<span class="sgr32">)</span>)), 0))</code></pre><h3 id="Training-loop"><a class="docs-heading-anchor" href="#Training-loop">Training loop</a><a id="Training-loop-1"></a><a class="docs-heading-anchor-permalink" href="#Training-loop" title="Permalink"></a></h3><p>Here is the typical main training loop suggest in the <a href="https://github.com/LuxDL/Lux.jl">LuxDL/Lux.jl</a> tutorials, but sligthly modified to save the history of losses per iteration.</p><pre><code class="language-julia hljs">function train(tstate, vjp, data, loss_function, epochs, numshowepochs=20, numsavestates=0)
    losses = zeros(epochs)
    tstates = [(0, tstate)]
    for epoch in 1:epochs
        grads, loss, stats, tstate = Lux.Training.compute_gradients(vjp,
            loss_function, data, tstate)
        if ( epochs ≥ numshowepochs &gt; 0 ) &amp;&amp; rem(epoch, div(epochs, numshowepochs)) == 0
            println(&quot;Epoch: $(epoch) || Loss: $(loss)&quot;)
        end
        if ( epochs ≥ numsavestates &gt; 0 ) &amp;&amp; rem(epoch, div(epochs, numsavestates)) == 0
            push!(tstates, (epoch, tstate))
        end
        losses[epoch] = loss
        tstate = Lux.Training.apply_gradients(tstate, grads)
    end
    return tstate, losses, tstates
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">train (generic function with 3 methods)</code></pre><h2 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h2><h3 id="Training-with-J_{\\mathrm{ESM}}({\\boldsymbol{\\theta}})"><a class="docs-heading-anchor" href="#Training-with-J_{\\mathrm{ESM}}({\\boldsymbol{\\theta}})">Training with <span>$J_{\mathrm{ESM}}({\boldsymbol{\theta}})$</span></a><a id="Training-with-J_{\\mathrm{ESM}}({\\boldsymbol{\\theta}})-1"></a><a class="docs-heading-anchor-permalink" href="#Training-with-J_{\\mathrm{ESM}}({\\boldsymbol{\\theta}})" title="Permalink"></a></h3><p>Now we attempt to train the model, starting with <span>$J_{\mathrm{ESM}}({\boldsymbol{\theta}})$</span>.</p><pre><code class="language-julia hljs">@time tstate, losses, tstates = train(tstate_org, vjp_rule, data, loss_function_esm, 500, 20, 125)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌ Warning: Mixed-Precision `matmul_cpu_fallback!` detected and Octavian.jl cannot be used for this set of inputs (C [Matrix{Float64}]: A [Matrix{Float32}] x B [Matrix{Float64}]). Falling back to generic implementation. This may be slow.
└ @ LuxLib.Impl ~/.julia/packages/LuxLib/1B1qw/src/impl/matmul.jl:190
Epoch: 25 || Loss: 0.011484647476584184
Epoch: 50 || Loss: 0.0010892008543654456
Epoch: 75 || Loss: 0.0005139587301049121
Epoch: 100 || Loss: 0.0003048078681800926
Epoch: 125 || Loss: 0.00022010774832755456
Epoch: 150 || Loss: 0.00016877845539167637
Epoch: 175 || Loss: 0.00013389781166276758
Epoch: 200 || Loss: 0.00010710671033748542
Epoch: 225 || Loss: 8.837107175303617e-5
Epoch: 250 || Loss: 7.325462539288798e-5
Epoch: 275 || Loss: 6.075454121212702e-5
Epoch: 300 || Loss: 6.519539827046248e-5
Epoch: 325 || Loss: 5.080965209395447e-5
Epoch: 350 || Loss: 4.499543672433463e-5
Epoch: 375 || Loss: 4.107296801645077e-5
Epoch: 400 || Loss: 3.753088421206525e-5
Epoch: 425 || Loss: 3.427673849087842e-5
Epoch: 450 || Loss: 3.128626629962772e-5
Epoch: 475 || Loss: 2.856143813023042e-5
Epoch: 500 || Loss: 2.6069196946524126e-5
  1.805148 seconds (5.39 M allocations: 309.042 MiB, 5.31% gc time, 95.74% compilation time)</code></pre><p>Testing out the trained model.</p><pre><code class="language-julia hljs">y_pred = Lux.apply(tstate.model, dev_cpu(x), tstate.parameters, tstate.states)[1]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×200 Matrix{Float64}:
 6.99703  6.89657  6.7961  6.69564  6.59517  …  -6.80168  -6.90223  -7.00277</code></pre><p>Visualizing the result.</p><pre><code class="language-julia hljs">plot(title=&quot;Fitting&quot;, titlefont=10)

plot!(x&#39;, y&#39;, linewidth=4, label=&quot;score function&quot;)

scatter!(sample_points&#39;, s -&gt; gradlogpdf(target_prob, s), label=&quot;data&quot;, markersize=2)

plot!(x&#39;, y_pred&#39;, linewidth=2, label=&quot;predicted MLP&quot;)</code></pre><img src="6f3719aa.svg" alt="Example block output"/><p>Just for the fun of it, let us see an animation of the optimization process.</p><img src="dd52b93b.gif" alt="Example block output"/><p>We also visualize the evolution of the losses.</p><pre><code class="language-julia hljs">plot(losses, title=&quot;Evolution of the loss&quot;, titlefont=10, xlabel=&quot;iteration&quot;, ylabel=&quot;error&quot;, legend=false)</code></pre><img src="15f4b258.svg" alt="Example block output"/><p>Recovering the PDF of the distribution from the trained score function.</p><pre><code class="language-julia hljs">paux = exp.(accumulate(+, y_pred) .* dx)
pdf_pred = paux ./ sum(paux) ./ dx
plot(title=&quot;Original PDF and PDF from predicted score function&quot;, titlefont=10)
plot!(x&#39;, target_pdf&#39;, label=&quot;original&quot;)
plot!(x&#39;, pdf_pred&#39;, label=&quot;recoverd&quot;)</code></pre><img src="fc3be3c2.svg" alt="Example block output"/><h3 id="Training-with-plain-square-error-loss"><a class="docs-heading-anchor" href="#Training-with-plain-square-error-loss">Training with plain square error loss</a><a id="Training-with-plain-square-error-loss-1"></a><a class="docs-heading-anchor-permalink" href="#Training-with-plain-square-error-loss" title="Permalink"></a></h3><p>Now we attempt to train it with the plain square error loss function. We do not reuse the state from the previous optimization. We start over at the initial state, for the sake of comparison of the different loss functions.</p><pre><code class="language-julia hljs">@time tstate, losses, = train(tstate_org, vjp_rule, data, loss_function_esm_plain, 500)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌ Warning: Mixed-Precision `matmul_cpu_fallback!` detected and Octavian.jl cannot be used for this set of inputs (C [Matrix{Float64}]: A [Matrix{Float32}] x B [Matrix{Float64}]). Falling back to generic implementation. This may be slow.
└ @ LuxLib.Impl ~/.julia/packages/LuxLib/1B1qw/src/impl/matmul.jl:190
Epoch: 25 || Loss: 1.1143287173808216
Epoch: 50 || Loss: 0.6205280419218188
Epoch: 75 || Loss: 0.32544911126408366
Epoch: 100 || Loss: 0.1710433824918761
Epoch: 125 || Loss: 0.11396855990773959
Epoch: 150 || Loss: 0.08977222267758472
Epoch: 175 || Loss: 0.07414964239800347
Epoch: 200 || Loss: 0.06208636363763847
Epoch: 225 || Loss: 0.052722900480855595
Epoch: 250 || Loss: 0.04509877044198144
Epoch: 275 || Loss: 0.03915099306002895
Epoch: 300 || Loss: 0.034118094277221955
Epoch: 325 || Loss: 0.03472436665426661
Epoch: 350 || Loss: 0.02919968691985968
Epoch: 375 || Loss: 0.02425868074860777
Epoch: 400 || Loss: 0.02138890586961945
Epoch: 425 || Loss: 0.019181660521026083
Epoch: 450 || Loss: 0.017489529179635404
Epoch: 475 || Loss: 0.016001324149431055
Epoch: 500 || Loss: 0.01425827694051816
  0.034670 seconds (169.65 k allocations: 35.912 MiB, 12.88% compilation time)</code></pre><p>Testing out the trained model.</p><pre><code class="language-julia hljs">y_pred = Lux.apply(tstate.model, dev_cpu(x), tstate.parameters, tstate.states)[1]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×200 Matrix{Float64}:
 7.00132  6.90073  6.80014  6.69956  6.59897  …  -6.78975  -6.89043  -6.99112</code></pre><p>Visualizing the result.</p><pre><code class="language-julia hljs">plot(title=&quot;Fitting&quot;, titlefont=10)

plot!(x&#39;, y&#39;, linewidth=4, label=&quot;score function&quot;)

scatter!(sample_points&#39;, s -&gt; gradlogpdf(target_prob, s), label=&quot;data&quot;, markersize=2)

plot!(x&#39;, y_pred&#39;, linewidth=2, label=&quot;predicted MLP&quot;)</code></pre><img src="8e8ff57a.svg" alt="Example block output"/><p>And evolution of the losses.</p><pre><code class="language-julia hljs">plot(losses, title=&quot;Evolution of the loss&quot;, titlefont=10, xlabel=&quot;iteration&quot;, ylabel=&quot;error&quot;, legend=false)</code></pre><img src="7bddd5f7.svg" alt="Example block output"/><p>Recovering the PDF of the distribution from the trained score function.</p><pre><code class="language-julia hljs">paux = exp.(accumulate(+, y_pred) * dx)
pdf_pred = paux ./ sum(paux) ./ dx
plot(title=&quot;Original PDF and PDF from predicted score function&quot;, titlefont=10)
plot!(x&#39;, target_pdf&#39;, label=&quot;original&quot;)
plot!(x&#39;, pdf_pred&#39;, label=&quot;recoverd&quot;)</code></pre><img src="56609be1.svg" alt="Example block output"/><p>That is an almost perfect matching.</p><h3 id="Training-with-{\\tilde-J}_{\\mathrm{FDSM}}({\\boldsymbol{\\theta}})"><a class="docs-heading-anchor" href="#Training-with-{\\tilde-J}_{\\mathrm{FDSM}}({\\boldsymbol{\\theta}})">Training with <span>${\tilde J}_{\mathrm{FDSM}}({\boldsymbol{\theta}})$</span></a><a id="Training-with-{\\tilde-J}_{\\mathrm{FDSM}}({\\boldsymbol{\\theta}})-1"></a><a class="docs-heading-anchor-permalink" href="#Training-with-{\\tilde-J}_{\\mathrm{FDSM}}({\\boldsymbol{\\theta}})" title="Permalink"></a></h3><p>Now we attempt to train it with <span>${\tilde J}_{\mathrm{FDSM}}$</span>. Again we start over with the untrained state of the model.</p><pre><code class="language-julia hljs">@time tstate, losses, = train(tstate_org, vjp_rule, data, loss_function_FDSM, 500)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌ Warning: Mixed-Precision `matmul_cpu_fallback!` detected and Octavian.jl cannot be used for this set of inputs (C [Matrix{Float64}]: A [Matrix{Float32}] x B [Matrix{Float64}]). Falling back to generic implementation. This may be slow.
└ @ LuxLib.Impl ~/.julia/packages/LuxLib/1B1qw/src/impl/matmul.jl:190
Epoch: 25 || Loss: -0.018614018488740797
Epoch: 50 || Loss: -0.023813179554128247
Epoch: 75 || Loss: -0.024092143080255847
Epoch: 100 || Loss: -0.02419494535460953
Epoch: 125 || Loss: -0.024240916453391156
Epoch: 150 || Loss: -0.024267346116293057
Epoch: 175 || Loss: -0.024284249143792003
Epoch: 200 || Loss: -0.024297095733649313
Epoch: 225 || Loss: -0.024308211705183807
Epoch: 250 || Loss: -0.024315596351141243
Epoch: 275 || Loss: -0.024321496737331944
Epoch: 300 || Loss: -0.024321372316381637
Epoch: 325 || Loss: -0.024327671166778794
Epoch: 350 || Loss: -0.024331920194604548
Epoch: 375 || Loss: -0.024334697882427526
Epoch: 400 || Loss: -0.024336853420801087
Epoch: 425 || Loss: -0.02433463366479651
Epoch: 450 || Loss: -0.02433717576079263
Epoch: 475 || Loss: -0.024338191812142456
Epoch: 500 || Loss: -0.0243391619743789
  0.048848 seconds (217.54 k allocations: 74.835 MiB)</code></pre><p>We may try a little longer from this state on.</p><pre><code class="language-julia hljs">@time tstate, losses_more, = train(tstate, vjp_rule, data, loss_function_FDSM, 500)
append!(losses, losses_more)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌ Warning: Mixed-Precision `matmul_cpu_fallback!` detected and Octavian.jl cannot be used for this set of inputs (C [Matrix{Float64}]: A [Matrix{Float32}] x B [Matrix{Float64}]). Falling back to generic implementation. This may be slow.
└ @ LuxLib.Impl ~/.julia/packages/LuxLib/1B1qw/src/impl/matmul.jl:190
Epoch: 25 || Loss: -0.024336056076074294
Epoch: 50 || Loss: -0.0243410742556927
Epoch: 75 || Loss: -0.024342017805526382
Epoch: 100 || Loss: -0.024330182528856367
Epoch: 125 || Loss: -0.024340313035580204
Epoch: 150 || Loss: -0.024344204557304433
Epoch: 175 || Loss: -0.02434481425285506
Epoch: 200 || Loss: -0.024345444444349242
Epoch: 225 || Loss: -0.024344854500703582
Epoch: 250 || Loss: -0.024346313966187947
Epoch: 275 || Loss: -0.024346212833022917
Epoch: 300 || Loss: -0.024347147021435093
Epoch: 325 || Loss: -0.024347559436250027
Epoch: 350 || Loss: -0.024094172899427172
Epoch: 375 || Loss: -0.02434726337346666
Epoch: 400 || Loss: -0.02434762170922898
Epoch: 425 || Loss: -0.024348712909202163
Epoch: 450 || Loss: -0.02434899638532974
Epoch: 475 || Loss: -0.02434919378039356
Epoch: 500 || Loss: -0.024237451317891203
  0.067250 seconds (218.51 k allocations: 74.849 MiB)</code></pre><p>Testing out the trained model.</p><pre><code class="language-julia hljs">y_pred = Lux.apply(tstate.model, dev_cpu(x), tstate.parameters, tstate.states)[1]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×200 Matrix{Float64}:
 7.28941  7.18565  7.08189  6.97813  6.87437  …  -7.00168  -7.10417  -7.20665</code></pre><p>Visualizing the result.</p><pre><code class="language-julia hljs">plot(title=&quot;Fitting&quot;, titlefont=10)

plot!(x&#39;, y&#39;, linewidth=4, label=&quot;score function&quot;)

scatter!(sample_points&#39;, s -&gt; gradlogpdf(target_prob, s), label=&quot;data&quot;, markersize=2)

plot!(x&#39;, y_pred&#39;, linewidth=2, label=&quot;predicted MLP&quot;)</code></pre><img src="bbd7816e.svg" alt="Example block output"/><p>And evolution of the losses.</p><pre><code class="language-julia hljs">plot(losses, title=&quot;Evolution of the loss&quot;, titlefont=10, xlabel=&quot;iteration&quot;, ylabel=&quot;error&quot;, legend=false)</code></pre><img src="078579a4.svg" alt="Example block output"/><p>Recovering the PDF of the distribution from the trained score function.</p><pre><code class="language-julia hljs">paux = exp.(accumulate(+, y_pred) * dx)
pdf_pred = paux ./ sum(paux) ./ dx
plot(title=&quot;Original PDF and PDF from predicted score function&quot;, titlefont=10)
plot!(x&#39;, target_pdf&#39;, label=&quot;original&quot;)
plot!(x&#39;, pdf_pred&#39;, label=&quot;recoverd&quot;)</code></pre><img src="8bf08bd8.svg" alt="Example block output"/><h3 id="Training-with-{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}({\\boldsymbol{\\theta}})"><a class="docs-heading-anchor" href="#Training-with-{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}({\\boldsymbol{\\theta}})">Training with <span>${\tilde J}_{\mathrm{FDSM}{\tilde p}_0}({\boldsymbol{\theta}})$</span></a><a id="Training-with-{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}({\\boldsymbol{\\theta}})-1"></a><a class="docs-heading-anchor-permalink" href="#Training-with-{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}({\\boldsymbol{\\theta}})" title="Permalink"></a></h3><p>Finally we attemp to train with the sample data. This is the real thing, without anything from the supposedly unknown target distribution other than the sample data.</p><pre><code class="language-julia hljs">@time tstate, losses, tstates = train(tstate_org, vjp_rule, data, loss_function_FDSM_over_sample, 500, 20, 125)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌ Warning: Mixed-Precision `matmul_cpu_fallback!` detected and Octavian.jl cannot be used for this set of inputs (C [Matrix{Float64}]: A [Matrix{Float32}] x B [Matrix{Float64}]). Falling back to generic implementation. This may be slow.
└ @ LuxLib.Impl ~/.julia/packages/LuxLib/1B1qw/src/impl/matmul.jl:190
Epoch: 25 || Loss: -0.35961054035246726
Epoch: 50 || Loss: -0.4551765519086538
Epoch: 75 || Loss: -0.4637960763447525
Epoch: 100 || Loss: -0.46541639141792585
Epoch: 125 || Loss: -0.46440229493835966
Epoch: 150 || Loss: -0.46505629593826653
Epoch: 175 || Loss: -0.4646927348307835
Epoch: 200 || Loss: -0.46542044207357663
Epoch: 225 || Loss: -0.465324725184812
Epoch: 250 || Loss: -0.4656559359113997
Epoch: 275 || Loss: -0.4649428554451408
Epoch: 300 || Loss: -0.46457362593439777
Epoch: 325 || Loss: -0.46505688461762273
Epoch: 350 || Loss: -0.46533166373245266
Epoch: 375 || Loss: -0.46659489714388475
Epoch: 400 || Loss: -0.46651713064562467
Epoch: 425 || Loss: -0.46655590358253773
Epoch: 450 || Loss: -0.4663559473146156
Epoch: 475 || Loss: -0.4668261558201798
Epoch: 500 || Loss: -0.4670447795697106
  0.178944 seconds (226.64 k allocations: 298.805 MiB, 22.00% gc time)</code></pre><p>Testing out the trained model.</p><pre><code class="language-julia hljs">y_pred = Lux.apply(tstate.model, dev_cpu(x), tstate.parameters, tstate.states)[1]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×200 Matrix{Float64}:
 5.90841  5.82217  5.73592  5.64967  5.56343  …  -6.47269  -6.55982  -6.64694</code></pre><p>Visualizing the result.</p><pre><code class="language-julia hljs">plot(title=&quot;Fitting&quot;, titlefont=10)

plot!(x&#39;, y&#39;, linewidth=4, label=&quot;score function&quot;)

scatter!(sample_points&#39;, s -&gt; gradlogpdf(target_prob, s), label=&quot;data&quot;, markersize=2)

plot!(x&#39;, y_pred&#39;, linewidth=2, label=&quot;predicted MLP&quot;)</code></pre><img src="5272dd39.svg" alt="Example block output"/><p>Let us see an animation of the optimization process in this case, as well, since it is the one of interest.</p><img src="cf2e62b7.gif" alt="Example block output"/><p>Here is the evolution of the losses.</p><pre><code class="language-julia hljs">plot(losses, title=&quot;Evolution of the loss&quot;, titlefont=10, xlabel=&quot;iteration&quot;, ylabel=&quot;error&quot;, legend=false)</code></pre><img src="529938b3.svg" alt="Example block output"/><p>Recovering the PDF of the distribution from the trained score function.</p><pre><code class="language-julia hljs">paux = exp.(accumulate(+, y_pred) * dx)
pdf_pred = paux ./ sum(paux) ./ dx
plot(title=&quot;Original PDF and PDF from predicted score function&quot;, titlefont=10)
plot!(x&#39;, target_pdf&#39;, label=&quot;original&quot;)
plot!(x&#39;, pdf_pred&#39;, label=&quot;recoverd&quot;)</code></pre><img src="be0e2125.svg" alt="Example block output"/><p>And the evolution of the PDF.</p><img src="ccfafdb7.gif" alt="Example block output"/><h3 id="Pre-training-{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}({\\boldsymbol{\\theta}})-with-J_{\\mathrm{ESM}}({\\boldsymbol{\\theta}})"><a class="docs-heading-anchor" href="#Pre-training-{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}({\\boldsymbol{\\theta}})-with-J_{\\mathrm{ESM}}({\\boldsymbol{\\theta}})">Pre-training <span>${\tilde J}_{\mathrm{FDSM}{\tilde p}_0}{\tilde J}_{\mathrm{FDSM}{\tilde p}_0}({\boldsymbol{\theta}})$</span> with <span>$J_{\mathrm{ESM}}({\boldsymbol{\theta}})$</span></a><a id="Pre-training-{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}({\\boldsymbol{\\theta}})-with-J_{\\mathrm{ESM}}({\\boldsymbol{\\theta}})-1"></a><a class="docs-heading-anchor-permalink" href="#Pre-training-{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}{\\tilde-J}_{\\mathrm{FDSM}{\\tilde-p}_0}({\\boldsymbol{\\theta}})-with-J_{\\mathrm{ESM}}({\\boldsymbol{\\theta}})" title="Permalink"></a></h3><p>Let us now pre-train the model with the <span>$J_{\mathrm{ESM}}({\boldsymbol{\theta}})$</span> and see if <span>${\tilde J}_{\mathrm{FDSM}{\tilde p}_0}({\boldsymbol{\theta}})$</span> improves.</p><pre><code class="language-julia hljs">tstate, = train(tstate_org, vjp_rule, data, loss_function_esm, 500)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌ Warning: Mixed-Precision `matmul_cpu_fallback!` detected and Octavian.jl cannot be used for this set of inputs (C [Matrix{Float64}]: A [Matrix{Float32}] x B [Matrix{Float64}]). Falling back to generic implementation. This may be slow.
└ @ LuxLib.Impl ~/.julia/packages/LuxLib/1B1qw/src/impl/matmul.jl:190
Epoch: 25 || Loss: 0.011484647476584184
Epoch: 50 || Loss: 0.0010892008543654456
Epoch: 75 || Loss: 0.0005139587301049121
Epoch: 100 || Loss: 0.0003048078681800926
Epoch: 125 || Loss: 0.00022010774832755456
Epoch: 150 || Loss: 0.00016877845539167637
Epoch: 175 || Loss: 0.00013389781166276758
Epoch: 200 || Loss: 0.00010710671033748542
Epoch: 225 || Loss: 8.837107175303617e-5
Epoch: 250 || Loss: 7.325462539288798e-5
Epoch: 275 || Loss: 6.075454121212702e-5
Epoch: 300 || Loss: 6.519539827046248e-5
Epoch: 325 || Loss: 5.080965209395447e-5
Epoch: 350 || Loss: 4.499543672433463e-5
Epoch: 375 || Loss: 4.107296801645077e-5
Epoch: 400 || Loss: 3.753088421206525e-5
Epoch: 425 || Loss: 3.427673849087842e-5
Epoch: 450 || Loss: 3.128626629962772e-5
Epoch: 475 || Loss: 2.856143813023042e-5
Epoch: 500 || Loss: 2.6069196946524126e-5</code></pre><pre><code class="language-julia hljs">tstate, losses, = train(tstate, vjp_rule, data, loss_function_FDSM_over_sample, 500)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌ Warning: Mixed-Precision `matmul_cpu_fallback!` detected and Octavian.jl cannot be used for this set of inputs (C [Matrix{Float64}]: A [Matrix{Float32}] x B [Matrix{Float64}]). Falling back to generic implementation. This may be slow.
└ @ LuxLib.Impl ~/.julia/packages/LuxLib/1B1qw/src/impl/matmul.jl:190
Epoch: 25 || Loss: -0.40840829540697654
Epoch: 50 || Loss: -0.45839653803847796
Epoch: 75 || Loss: -0.4652597211560801
Epoch: 100 || Loss: -0.46585038494415626
Epoch: 125 || Loss: -0.4663084024154143
Epoch: 150 || Loss: -0.4667541288434689
Epoch: 175 || Loss: -0.4671830176420521
Epoch: 200 || Loss: -0.4678322578316065
Epoch: 225 || Loss: -0.46800295087892546
Epoch: 250 || Loss: -0.4680941506706185
Epoch: 275 || Loss: -0.46801694166125085
Epoch: 300 || Loss: -0.46835968628380975
Epoch: 325 || Loss: -0.46852757620996616
Epoch: 350 || Loss: -0.46849941847366045
Epoch: 375 || Loss: -0.4684963914745495
Epoch: 400 || Loss: -0.46819020963134167
Epoch: 425 || Loss: -0.46870317306619164
Epoch: 450 || Loss: -0.4683444546608004
Epoch: 475 || Loss: -0.46870841003097646
Epoch: 500 || Loss: -0.46857195722115036</code></pre><p>Testing out the trained model.</p><pre><code class="language-julia hljs">y_pred = Lux.apply(tstate.model, dev_cpu(x), tstate.parameters, tstate.states)[1]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×200 Matrix{Float64}:
 6.08112  5.99339  5.90565  5.81792  5.73018  …  -6.63836  -6.73676  -6.83517</code></pre><p>Visualizing the result.</p><pre><code class="language-julia hljs">plot(title=&quot;Fitting&quot;, titlefont=10)

plot!(x&#39;, y&#39;, linewidth=4, label=&quot;score function&quot;)

scatter!(sample_points&#39;, s -&gt; gradlogpdf(target_prob, s), label=&quot;data&quot;, markersize=2)

plot!(x&#39;, y_pred&#39;, linewidth=2, label=&quot;predicted MLP&quot;)</code></pre><img src="0cd463ec.svg" alt="Example block output"/><p>Recovering the PDF of the distribution from the trained score function.</p><pre><code class="language-julia hljs">paux = exp.(accumulate(+, y_pred) * dx)
pdf_pred = paux ./ sum(paux) ./ dx
plot(title=&quot;Original PDF and PDF from predicted score function&quot;, titlefont=10)
plot!(x&#39;, target_pdf&#39;, label=&quot;original&quot;)
plot!(x&#39;, pdf_pred&#39;, label=&quot;recoverd&quot;)</code></pre><img src="a49c18fd.svg" alt="Example block output"/><p>And evolution of the losses.</p><pre><code class="language-julia hljs">plot(losses, title=&quot;Evolution of the loss&quot;, titlefont=10, xlabel=&quot;iteration&quot;, ylabel=&quot;error&quot;, legend=false)</code></pre><img src="1b2aac7f.svg" alt="Example block output"/><h2 id="The-need-for-enough-sample-points"><a class="docs-heading-anchor" href="#The-need-for-enough-sample-points">The need for enough sample points</a><a id="The-need-for-enough-sample-points-1"></a><a class="docs-heading-anchor-permalink" href="#The-need-for-enough-sample-points" title="Permalink"></a></h2><p>One interesting thing is that enough sample points in the low-probability transition region is required for a proper fit, as the following example with few samples illustrates.</p><pre><code class="language-julia hljs">y = target_score # just to simplify the notation
sample_points = permutedims(rand(rng, target_prob, 128))
data = (x, y, target_pdf, sample_points)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([-10.0 -9.899497487437186 … 9.899497487437186 10.0], [7.0 6.899497487437186 … -6.899497487437186 -7.0], [9.134720408364594e-13 1.8366893783972853e-12 … 1.6530204405575567e-11 8.221248367528135e-12], [2.468184259729197 3.042984658516062 … 5.725081068647343 -5.180972754540539])</code></pre><pre><code class="language-julia hljs">tstate, losses, = train(tstate_org, vjp_rule, data, loss_function_FDSM_over_sample, 500)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌ Warning: Mixed-Precision `matmul_cpu_fallback!` detected and Octavian.jl cannot be used for this set of inputs (C [Matrix{Float64}]: A [Matrix{Float32}] x B [Matrix{Float64}]). Falling back to generic implementation. This may be slow.
└ @ LuxLib.Impl ~/.julia/packages/LuxLib/1B1qw/src/impl/matmul.jl:190
Epoch: 25 || Loss: -0.3838705443928865
Epoch: 50 || Loss: -0.48887835459085494
Epoch: 75 || Loss: -0.4967438326023362
Epoch: 100 || Loss: -0.4981079964280764
Epoch: 125 || Loss: -0.4993105142721118
Epoch: 150 || Loss: -0.5003832554392386
Epoch: 175 || Loss: -0.5012980845567676
Epoch: 200 || Loss: -0.5020789173624373
Epoch: 225 || Loss: -0.5027828483131915
Epoch: 250 || Loss: -0.5035585908567416
Epoch: 275 || Loss: -0.5044853614854157
Epoch: 300 || Loss: -0.5058446337324545
Epoch: 325 || Loss: -0.5077990566217403
Epoch: 350 || Loss: -0.5104631091100137
Epoch: 375 || Loss: -0.5198160534910374
Epoch: 400 || Loss: -0.5317397867431113
Epoch: 425 || Loss: -0.5392019630318063
Epoch: 450 || Loss: -0.5562426060026169
Epoch: 475 || Loss: -0.564099250178824
Epoch: 500 || Loss: -0.575856080338598</code></pre><p>Testing out the trained model.</p><pre><code class="language-julia hljs">y_pred = Lux.apply(tstate.model, dev_cpu(x), tstate.parameters, tstate.states)[1]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×200 Matrix{Float64}:
 7.31557  7.20412  7.09267  6.98121  6.86976  …  -4.59634  -4.63756  -4.67879</code></pre><p>Visualizing the result.</p><pre><code class="language-julia hljs">plot(title=&quot;Fitting&quot;, titlefont=10)

plot!(x&#39;, y&#39;, linewidth=4, label=&quot;score function&quot;)

scatter!(sample_points&#39;, s -&gt; gradlogpdf(target_prob, s)&#39;, label=&quot;data&quot;, markersize=2)

plot!(x&#39;, y_pred&#39;, linewidth=2, label=&quot;predicted MLP&quot;)</code></pre><img src="c0f061fc.svg" alt="Example block output"/><p>Recovering the PDF of the distribution from the trained score function.</p><pre><code class="language-julia hljs">paux = exp.(accumulate(+, y_pred) * dx)
pdf_pred = paux ./ sum(paux) ./ dx
plot(title=&quot;Original PDF and PDF from predicted score function&quot;, titlefont=10)
plot!(x&#39;, target_pdf&#39;, label=&quot;original&quot;)
plot!(x&#39;, pdf_pred&#39;, label=&quot;recoverd&quot;)</code></pre><img src="60c1a3d1.svg" alt="Example block output"/><p>And evolution of the losses.</p><pre><code class="language-julia hljs">plot(losses, title=&quot;Evolution of the loss&quot;, titlefont=10, xlabel=&quot;iteration&quot;, ylabel=&quot;error&quot;, legend=false)</code></pre><img src="33a176dc.svg" alt="Example block output"/><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><ol><li><a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Aapo Hyvärinen (2005), &quot;Estimation of non-normalized statistical models by score matching&quot;, Journal of Machine Learning Research 6, 695-709</a></li><li><a href="https://openreview.net/forum?id=LVRoKppWczk">T. Pang, K. Xu, C. Li, Y. Song, S. Ermon, J. Zhu (2020), Efficient Learning of Generative Models via Finite-Difference Score Matching, NeurIPS</a> - see also the <a href="https://arxiv.org/abs/2007.03317">arxiv version</a></li><li><a href="https://ericmjl.github.io/score-models/">Eric J. Ma, A Pedagogical Introduction to Score Models, webpage, April 21, 2021</a> - with the associated <a href="https://github.com/ericmjl/score-models/blob/main/score_models/losses/diffusion.py#L7">github repo</a></li><li><a href="https://www.semanticscholar.org/paper/Adam%3A-A-Method-for-Stochastic-Optimization-Kingma-Ba/a6cb366736791bcccc5c8639de5a8f9636bf87e8">D. P. Kingma, J. Ba (2015), Adam: A Method for Stochastic Optimization, In International Conference on Learning Representations (ICLR)</a> – see also the <a href="https://arxiv.org/abs/1412.6980">arxiv version</a></li><li><a href="https://dl.acm.org/doi/10.5555/3454287.3455354">Y. Song and S. Ermon (2019), &quot;Generative modeling by estimating gradients of the data distribution&quot;, NIPS&#39;19: Proceedings of the 33rd International Conference on Neural Information Processing Systems, no. 1067, 11918-11930</a></li></ol></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../sliced_score_matching/">« Sliced score matching</a><a class="docs-footer-nextpage" href="../2d_FD_score_matching/">2D finite-difference score matching »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.13.0 on <span class="colophon-date" title="Thursday 26 June 2025 15:35">Thursday 26 June 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
