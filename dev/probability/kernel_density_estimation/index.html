<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Kernel Density Estimation · Random notes</title><meta name="title" content="Kernel Density Estimation · Random notes"/><meta property="og:title" content="Kernel Density Estimation · Random notes"/><meta property="twitter:title" content="Kernel Density Estimation · Random notes"/><meta name="description" content="Documentation for Random notes."/><meta property="og:description" content="Documentation for Random notes."/><meta property="twitter:description" content="Documentation for Random notes."/><meta property="og:url" content="https://github.com/rmsrosa/random_notes/probability/kernel_density_estimation/"/><meta property="twitter:url" content="https://github.com/rmsrosa/random_notes/probability/kernel_density_estimation/"/><link rel="canonical" href="https://github.com/rmsrosa/random_notes/probability/kernel_density_estimation/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/style.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="Random notes logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Random notes</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Random Notes</a></li><li><span class="tocitem">Probability Essentials</span><ul><li class="is-active"><a class="tocitem" href>Kernel Density Estimation</a><ul class="internal"><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../convergence_notions/">Convergence notions</a></li></ul></li><li><span class="tocitem">Discrete-time Markov chains</span><ul><li><a class="tocitem" href="../../markov_chains/mc_definitions/">Essential definitions</a></li><li><a class="tocitem" href="../../markov_chains/mc_invariance/">Invariant distributions</a></li><li><input class="collapse-toggle" id="menuitem-3-3" type="checkbox"/><label class="tocitem" for="menuitem-3-3"><span class="docs-label">Countable-space Markov chains</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../markov_chains/mc_countableX_recurrence/">Recurrence in the countable-space case</a></li><li><a class="tocitem" href="../../markov_chains/mc_countableX_connections/">Connected states, irreducibility and uniqueness of invariant measures</a></li><li><a class="tocitem" href="../../markov_chains/mc_countableX_convergencia/">Aperiodicidade e convergência para a distribuição estacionária</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-4" type="checkbox"/><label class="tocitem" for="menuitem-3-4"><span class="docs-label">Continuous-space Markov chains</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../markov_chains/mc_irreducibility_and_recurrence/">Irreducibility and recurrence in the continuous-space case</a></li></ul></li></ul></li><li><span class="tocitem">Sampling methods</span><ul><li><a class="tocitem" href="../../sampling/overview/">Overview</a></li><li><a class="tocitem" href="../../sampling/prng/">Random number generators</a></li><li><input class="collapse-toggle" id="menuitem-4-3" type="checkbox"/><label class="tocitem" for="menuitem-4-3"><span class="docs-label">Transform methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/invFtransform/">Probability integral transform</a></li><li><a class="tocitem" href="../../sampling/box_muller/">Box-Muller transform</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-4" type="checkbox"/><label class="tocitem" for="menuitem-4-4"><span class="docs-label">Accept-Reject methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/rejection_sampling/">Rejection sampling</a></li><li><a class="tocitem" href="../../sampling/empiricalsup_rejection/">Empirical supremum rejection sampling</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-5" type="checkbox"/><label class="tocitem" for="menuitem-4-5"><span class="docs-label">Markov Chain Monte Carlo (MCMC)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/mcmc/">Overview</a></li><li><a class="tocitem" href="../../sampling/metropolis/">Metropolis and Metropolis-Hastings</a></li><li><a class="tocitem" href="../../sampling/convergence_metropolis/">Convergence of Metropolis-Hastings</a></li><li><a class="tocitem" href="../../sampling/gibbs/">Gibbs sampling</a></li><li><a class="tocitem" href="../../sampling/hmc/">Hamiltonian Monte Carlo (HMC)</a></li></ul></li><li><a class="tocitem" href="../../sampling/langevin_sampling/">Langevin sampling</a></li></ul></li><li><span class="tocitem">Bayesian inference</span><ul><li><input class="collapse-toggle" id="menuitem-5-1" type="checkbox"/><label class="tocitem" for="menuitem-5-1"><span class="docs-label">Bayes Theory</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/bayes/">Bayes Theorem</a></li><li><a class="tocitem" href="../../bayesian/bayes_inference/">Bayesian inference</a></li><li><a class="tocitem" href="../../bayesian/bernstein_vonmises/">Bernstein–von Mises theorem</a></li></ul></li><li><a class="tocitem" href="../../bayesian/bayesian_probprog/">Bayesian probabilistic programming</a></li><li><input class="collapse-toggle" id="menuitem-5-3" type="checkbox"/><label class="tocitem" for="menuitem-5-3"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/find_pi/">Estimating π via frequentist and Bayesian methods</a></li><li><a class="tocitem" href="../../bayesian/linear_regression/">Many Ways to Linear Regression</a></li><li><a class="tocitem" href="../../bayesian/tilapia_alometry/">Alometry law for the Nile Tilapia</a></li><li><a class="tocitem" href="../../bayesian/mortality_tables/">Modeling mortality tables</a></li></ul></li></ul></li><li><span class="tocitem">Generative models</span><ul><li><input class="collapse-toggle" id="menuitem-6-1" type="checkbox"/><label class="tocitem" for="menuitem-6-1"><span class="docs-label">Score matching</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../generative/overview/">Overview</a></li><li><a class="tocitem" href="../../generative/stein_score/">Stein score function</a></li><li><a class="tocitem" href="../../generative/score_matching_aapo/">Score matching of Aapo Hyvärinen</a></li><li><a class="tocitem" href="../../generative/score_matching_neural_network/">Score matching a neural network</a></li><li><a class="tocitem" href="../../generative/parzen_estimation_score_matching/">Score matching with Parzen estimation</a></li><li><a class="tocitem" href="../../generative/denoising_score_matching/">Denoising score matching of Pascal Vincent</a></li><li><a class="tocitem" href="../../generative/sliced_score_matching/">Sliced score matching</a></li><li><a class="tocitem" href="../../generative/1d_FD_score_matching/">1D finite-difference score matching</a></li><li><a class="tocitem" href="../../generative/2d_FD_score_matching/">2D finite-difference score matching</a></li><li><a class="tocitem" href="../../generative/ddpm/">Denoising diffusion probabilistic models</a></li><li><a class="tocitem" href="../../generative/mdsm/">Multiple denoising score matching</a></li><li><a class="tocitem" href="../../generative/probability_flow/">Probability flow</a></li><li><a class="tocitem" href="../../generative/reverse_flow/">Reverse probability flow</a></li><li><a class="tocitem" href="../../generative/score_based_sde/">Score-based SDE model</a></li></ul></li></ul></li><li><span class="tocitem">Sensitivity analysis</span><ul><li><a class="tocitem" href="../../sensitivity/overview/">Overview</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Probability Essentials</a></li><li class="is-active"><a href>Kernel Density Estimation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Kernel Density Estimation</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/rmsrosa/random_notes" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/rmsrosa/random_notes/blob/main/docs/src/probability/kernel_density_estimation.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Kernel-density-estimation"><a class="docs-heading-anchor" href="#Kernel-density-estimation">Kernel density estimation</a><a id="Kernel-density-estimation-1"></a><a class="docs-heading-anchor-permalink" href="#Kernel-density-estimation" title="Permalink"></a></h1><p>We consider a univariate real-valued random variable <span>$X$</span>, for simplicity, but the same idea applies to multivariate random variables.</p><p>Let us say we have a sample <span>$\{x_n\}_{n=1}^N$</span> of <span>$X$</span>, where <span>$N\in\mathbb{N}$</span>.</p><img src="1dded306.svg" alt="Example block output"/><p>Which statistical informations one can draw from it?</p><p>Certainly we can compute the <em>sample</em> mean, and the <em>sample</em> standard deviation, directly from the data, and so on. They give us some reasonable estimates on the true values, depending on the number of sample points and on the independence of the sample.</p><img src="738451f3.svg" alt="Example block output"/><p>We can also draw its histogram, to have a more visual information of the underlying distribution.</p><img src="13b84d85.svg" alt="Example block output"/><p>That histogram resembles a PDF. It is not a PDF in the sense that its <em>mass</em> is not one, but it can be normalized to resemble a PDF.</p><p>In view of that, one natural question is <em>how well can we approximate the PDF from the data?</em></p><p>There are <em>parametric</em> ways to do that, which means we can assume a <em>parametrized model</em>, say a Beta distribution <span>$B(\alpha, \beta)$</span> with shape parameters <span>$\alpha$</span> and <span>$\beta$</span>, and <em>fit</em> the model to the data, say using maximum likelyhood estimation, and use the pdf of the fitted model. That&#39;s all good. Depending on the random variable, though, your model can become quite complex.</p><p>There are also <em>nonparametric</em> ways of obtaining an approximate PDF for the distribution. One popular choice is the <em>kernel density estimation,</em> also known as <em>Parzen window estimation</em>, developed by <a href="http://projecteuclid.org/euclid.aoms/1177728190">Murray Rosenblatt (1956)</a>, <a href="https://www.jstor.org/stable/2983894">Peter Whittle (1958)</a>, and <a href="ttp://projecteuclid.org/euclid.aoms/1177704472">Emanuel Parzen (1962)</a>.</p><p>One way we can view the kernel density estimation is as a spin-off of the histogram. The PDF is likely to be larger where there are more sample points nearby. The closer they are to a point, the higher the chances around that. We can measure this with a <em>kernel</em> density around each sample point, like a region of influence. One can use different types of kernels, but a common one is a Gaussian kernel.</p><p>In the case of a histogram, if the interval <span>$I_j$</span> represents a bin, then the corresponding height <span>$h_j$</span> of the histogram on this bin is the sample count within the bin, which can be written as</p><p class="math-container">\[    h_j = \sum_{n=1}^N \chi_{I_j}(x_n).\]</p><p>We can normalize this with</p><p class="math-container">\[    p_j = \frac{1}{N|I_j|}\sum_{n=1}^N \chi_{I_j}(x_n),\]</p><p>where <span>$|I_j|$</span> is the width (or length) of the interval <span>$I_j$</span>. </p><p>In this case, if we set <span>$\hat p_{\mathcal{I}}(x) = p_j$</span> for <span>$x\in I_i$</span>, then</p><p class="math-container">\[    \int_{\mathbb{R}} \hat p_{\mathcal{I}}(x) \;\mathrm{d}x = \sum_{j=1}^M p_j |I_j|,\]</p><p>where <span>$M$</span> denotes the total number of bins, containing all the sample points and the partition <span>$\mathcal{I} = \{I_j\}_{j=1}^M$</span> is the collection of bins. Then,</p><p class="math-container">\[    \int_{\mathbb{R}} \hat p_{\mathcal{I}}(x) \;\mathrm{d}x = \sum_{j=1}^M \frac{1}{N|I_j|}\sum_{n=1}^N \chi_{I_j}(x_n)|I_j| = \frac{1}{N} \sum_{j=1}^M \sum_{n=1}^N \chi_{I_j}(x_n).\]</p><p>Switching the order of summation, we obtain</p><p class="math-container">\[    \int_{\mathbb{R}} \hat p_{\mathcal{I}}(x) \;\mathrm{d}x = \frac{1}{N} \sum_{n=1}^N \sum_{j=1}^M \chi_{I_j}(x_n).\]</p><p>Since each sample point is in one and only one bin, we have that</p><p class="math-container">\[    \sum_{j=1}^M \chi_{I_j}(x_n) = 1.\]</p><p>Thus,</p><p class="math-container">\[    \int_{\mathbb{R}} \hat p_{\mathcal{I}}(x) \;\mathrm{d}x = \frac{1}{N} \sum_{n=1}^N 1  = \frac{N}{N} = 1,\]</p><p>showing that <span>$\hat p_{\mathcal{I}}(\cdot)$</span> is normalized to have total mass <span>$1$</span>. So, this is a genuine PDF of some distribution that somehow approximates the true distribution. But it is not smooth.</p><img src="7a0cf8ac.svg" alt="Example block output"/><p>The kernel window estimation can be seen as a variation of this, which regularizes the PDF, provided the kernel is smooth. In this estimation, instead of summing up characteristic functions of the bins, we sum up the kernel around each sample point:</p><p class="math-container">\[    \hat p_h(x) = \frac{1}{h N}\sum_{n=1}^N K\left(\frac{x - x_n}{h}\right),\]</p><p>where <span>$h$</span> is a scale parameter that plays the role of the width of the bin, for a <em>nondimensional</em> kernel.</p><p>If the kernel has mass <span>$1$</span>, so does <span>$\hat p_h(x)$</span>. Indeed, using the change of variables <span>$y = (x - x_n) / h$</span>,</p><p class="math-container">\[    \begin{align*}
        \int_{\mathbb{R}} \hat p_h(x) \;\mathrm{d}x &amp; = \frac{1}{h N}\sum_{n=1}^N \int_{\mathbb{R}} K\left(\frac{x - x_n}{h}\right) \;\mathrm{d}x \\
        &amp; = \frac{1}{h N}\sum_{n=1}^N \int_{\mathbb{R}} K(y) h \;\mathrm{d}y \\
        &amp; = \frac{1}{N} \sum_{n=1}^N \int_{\mathbb{R}} K(y) \;\mathrm{d}y \\
        &amp; = \frac{1}{N} \sum_{n=1}^N 1 = \frac{N}{N} = 1.
    \end{align*}\]</p><p>If the kernel is flat, say the characteristic function of the interval <span>$[-1/2, 1/2)$</span>, </p><p class="math-container">\[    K(x) = \chi_{[-1/2, 1/2)}(x),\]</p><p>then the kernel window estimation <span>$\hat p_h(x)$</span> is constant by parts, resembling a histogram, but not quite like a histogram since the characteristic function is not attached to bins, but are centered on each sample point.</p><img src="71f3e02d.svg" alt="Example block output"/><p>When the kernel is smooth, so is <span>$\hat p_h(x)$</span>. In fact, <span>$\hat p_h(x)$</span> is as regular as the kernel. One popular choice is the Gaussian kernel</p><p class="math-container">\[    K(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}.\]</p><p>This yields the estimation</p><p class="math-container">\[    \hat p_h(x) = \frac{1}{h N}\sum_{n=1}^N K\left(\frac{x - x_n}{h}\right) = \frac{1}{N}\sum_{n=1}^N \frac{1}{\sqrt{2\pi} h} e^{-\frac{1}{2}\left(\frac{x - x_n}{h}\right)^2}.\]</p><img src="a8cea0d4.svg" alt="Example block output"/><p>The sample in this example was drawn from a mixture model combining a Beta distribution, a Gamma distribution, and a normal distribution. Here is the actual PDF compared with Gaussian kernel estimation with a specific value of <span>$h$</span>.</p><img src="143a4db1.svg" alt="Example block output"/><p>The choice of a suitable value for <span>$h$</span> is a delicate problem, though, as one can see from the estimations above, which is akin to the problem of choosing how many bins to view the histogram. And how can we be sure that this is really a good approximation for some &quot;reasonable&quot; choices of <span>$h$</span>?</p><p>Indeed, these are fundamental questions, and the works of Rosenblatt, Whittle, and Parzen are deeper than simply proposing the estimation <span>$\bar p_h$</span> for some kernel function and some value <span>$h$</span>. They also discuss further conditions on the kernel such that the estimate is not biased, and discuss asymptotic properties of the estimation, as the number of sample points grows to infinity. One of the results is that the choice of <span>$h$</span> should depend on <span>$n$</span> and decay to zero as <span>$n$</span> increases. They are worth reading, but we will not dwelve into further details at this moment.</p><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><ol><li><a href="http://projecteuclid.org/euclid.aoms/1177728190">M. Rosenblatt (1956), Remarks on Some Nonparametric Estimates of a Density Function. The Annals of Mathematical Statistics 27, no. 3, 832–837, doi:10.1214/aoms/1177728190</a></li><li><a href="https://www.jstor.org/stable/2983894">P. Whittle (1958), On the Smoothing of Probability Density Functions, Journal of the Royal Statistical Society. Series B (Methodological), Vol. 20, No. 2, pp. 334-343</a></li><li><a href="http://projecteuclid.org/euclid.aoms/1177704472">E. Parzen (1962), On Estimation of a Probability Density Function and Mode. The Annals of Mathematical Statistics 33, no. 3, 1065–1076, doi:10.1214/aoms/1177704472</a></li></ol></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« Random Notes</a><a class="docs-footer-nextpage" href="../convergence_notions/">Convergence notions »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.13.0 on <span class="colophon-date" title="Thursday 26 June 2025 15:34">Thursday 26 June 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
