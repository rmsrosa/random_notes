<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Recurrence in the countable-space case · Random notes</title><meta name="title" content="Recurrence in the countable-space case · Random notes"/><meta property="og:title" content="Recurrence in the countable-space case · Random notes"/><meta property="twitter:title" content="Recurrence in the countable-space case · Random notes"/><meta name="description" content="Documentation for Random notes."/><meta property="og:description" content="Documentation for Random notes."/><meta property="twitter:description" content="Documentation for Random notes."/><meta property="og:url" content="https://github.com/rmsrosa/random_notes/markov_chains/mc_countableX_recurrence/"/><meta property="twitter:url" content="https://github.com/rmsrosa/random_notes/markov_chains/mc_countableX_recurrence/"/><link rel="canonical" href="https://github.com/rmsrosa/random_notes/markov_chains/mc_countableX_recurrence/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/style.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="Random notes logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Random notes</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Random Notes</a></li><li><span class="tocitem">Probability Essentials</span><ul><li><a class="tocitem" href="../../probability/kernel_density_estimation/">Kernel Density Estimation</a></li><li><a class="tocitem" href="../../probability/convergence_notions/">Convergence notions</a></li></ul></li><li><span class="tocitem">Discrete-time Markov chains</span><ul><li><a class="tocitem" href="../mc_definitions/">Essential definitions</a></li><li><a class="tocitem" href="../mc_invariance/">Invariant distributions</a></li><li><input class="collapse-toggle" id="menuitem-3-3" type="checkbox" checked/><label class="tocitem" for="menuitem-3-3"><span class="docs-label">Countable-space Markov chains</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>Recurrence in the countable-space case</a><ul class="internal"><li><a class="tocitem" href="#Setting"><span>Setting</span></a></li><li><a class="tocitem" href="#Definitions"><span>Definitions</span></a></li><li><a class="tocitem" href="#Existence-of-invariant-distribution"><span>Existence of invariant distribution</span></a></li></ul></li><li><a class="tocitem" href="../mc_countableX_connections/">Connected states, irreducibility and uniqueness of invariant measures</a></li><li><a class="tocitem" href="../mc_countableX_convergencia/">Aperiodicidade e convergência para a distribuição estacionária</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-4" type="checkbox"/><label class="tocitem" for="menuitem-3-4"><span class="docs-label">Continuous-space Markov chains</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../mc_irreducibility_and_recurrence/">Irreducibility and recurrence in the continuous-space case</a></li></ul></li></ul></li><li><span class="tocitem">Sampling methods</span><ul><li><a class="tocitem" href="../../sampling/overview/">Overview</a></li><li><a class="tocitem" href="../../sampling/prng/">Random number generators</a></li><li><input class="collapse-toggle" id="menuitem-4-3" type="checkbox"/><label class="tocitem" for="menuitem-4-3"><span class="docs-label">Transform methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/invFtransform/">Probability integral transform</a></li><li><a class="tocitem" href="../../sampling/box_muller/">Box-Muller transform</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-4" type="checkbox"/><label class="tocitem" for="menuitem-4-4"><span class="docs-label">Accept-Reject methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/rejection_sampling/">Rejection sampling</a></li><li><a class="tocitem" href="../../sampling/empiricalsup_rejection/">Empirical supremum rejection sampling</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-5" type="checkbox"/><label class="tocitem" for="menuitem-4-5"><span class="docs-label">Markov Chain Monte Carlo (MCMC)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/mcmc/">Overview</a></li><li><a class="tocitem" href="../../sampling/metropolis/">Metropolis and Metropolis-Hastings</a></li><li><a class="tocitem" href="../../sampling/convergence_metropolis/">Convergence of Metropolis-Hastings</a></li><li><a class="tocitem" href="../../sampling/gibbs/">Gibbs sampling</a></li><li><a class="tocitem" href="../../sampling/hmc/">Hamiltonian Monte Carlo (HMC)</a></li></ul></li><li><a class="tocitem" href="../../sampling/langevin_sampling/">Langevin sampling</a></li></ul></li><li><span class="tocitem">Bayesian inference</span><ul><li><input class="collapse-toggle" id="menuitem-5-1" type="checkbox"/><label class="tocitem" for="menuitem-5-1"><span class="docs-label">Bayes Theory</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/bayes/">Bayes Theorem</a></li><li><a class="tocitem" href="../../bayesian/bayes_inference/">Bayesian inference</a></li><li><a class="tocitem" href="../../bayesian/bernstein_vonmises/">Bernstein–von Mises theorem</a></li></ul></li><li><a class="tocitem" href="../../bayesian/bayesian_probprog/">Bayesian probabilistic programming</a></li><li><input class="collapse-toggle" id="menuitem-5-3" type="checkbox"/><label class="tocitem" for="menuitem-5-3"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/find_pi/">Estimating π via frequentist and Bayesian methods</a></li><li><a class="tocitem" href="../../bayesian/linear_regression/">Many Ways to Linear Regression</a></li><li><a class="tocitem" href="../../bayesian/tilapia_alometry/">Alometry law for the Nile Tilapia</a></li><li><a class="tocitem" href="../../bayesian/mortality_tables/">Modeling mortality tables</a></li></ul></li></ul></li><li><span class="tocitem">Generative models</span><ul><li><input class="collapse-toggle" id="menuitem-6-1" type="checkbox"/><label class="tocitem" for="menuitem-6-1"><span class="docs-label">Score matching</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../generative/overview/">Overview</a></li><li><a class="tocitem" href="../../generative/stein_score/">Stein score function</a></li><li><a class="tocitem" href="../../generative/score_matching_aapo/">Score matching of Aapo Hyvärinen</a></li><li><a class="tocitem" href="../../generative/score_matching_neural_network/">Score matching a neural network</a></li><li><a class="tocitem" href="../../generative/parzen_estimation_score_matching/">Score matching with Parzen estimation</a></li><li><a class="tocitem" href="../../generative/denoising_score_matching/">Denoising score matching of Pascal Vincent</a></li><li><a class="tocitem" href="../../generative/sliced_score_matching/">Sliced score matching</a></li><li><a class="tocitem" href="../../generative/1d_FD_score_matching/">1D finite-difference score matching</a></li><li><a class="tocitem" href="../../generative/2d_FD_score_matching/">2D finite-difference score matching</a></li><li><a class="tocitem" href="../../generative/ddpm/">Denoising diffusion probabilistic models</a></li><li><a class="tocitem" href="../../generative/mdsm/">Multiple denoising score matching</a></li><li><a class="tocitem" href="../../generative/probability_flow/">Probability flow</a></li><li><a class="tocitem" href="../../generative/reverse_flow/">Reverse probability flow</a></li><li><a class="tocitem" href="../../generative/score_based_sde/">Score-based SDE model</a></li></ul></li></ul></li><li><span class="tocitem">Sensitivity analysis</span><ul><li><a class="tocitem" href="../../sensitivity/overview/">Overview</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Discrete-time Markov chains</a></li><li><a class="is-disabled">Countable-space Markov chains</a></li><li class="is-active"><a href>Recurrence in the countable-space case</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Recurrence in the countable-space case</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/rmsrosa/random_notes" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/rmsrosa/random_notes/blob/main/docs/src/markov_chains/mc_countableX_recurrence.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Recurrence-in-the-countable-space-case"><a class="docs-heading-anchor" href="#Recurrence-in-the-countable-space-case">Recurrence in the countable-space case</a><a id="Recurrence-in-the-countable-space-case-1"></a><a class="docs-heading-anchor-permalink" href="#Recurrence-in-the-countable-space-case" title="Permalink"></a></h1><p>Recurrence is a fundamental concept related to the existence of invariant measures. We explore such concept here, in the context of a countable state space.</p><h2 id="Setting"><a class="docs-heading-anchor" href="#Setting">Setting</a><a id="Setting-1"></a><a class="docs-heading-anchor-permalink" href="#Setting" title="Permalink"></a></h2><p>Here, we assume that <span>$(X_n)_n$</span> is a time-homogeneous, discrete-time Markov chain with a countable state space. More precisely, we assume the indices are <span>$n=0, 1, 2, \ldots,$</span> and that the space <span>$\mathcal{X}$</span> is finite or countably infinite. The sample space is the probability space <span>$(\Omega, \mathcal{F}, \mathbb{P}),$</span> where <span>$\mathcal{F}$</span> is the <span>$\sigma$</span>-algebra on the set <span>$\Omega$</span> and <span>$\mathbb{P}$</span> is the probability distribution. The one-step transition distribution is denoted by <span>$K(x, y) = \mathbb{P}(X_{n+1} = y | X_n = x),$</span> and is independent of <span>$n=0, 1, \ldots,$</span> thanks to the time-homogeneous assumption. Similary, the <span>$n$</span>-step transition distribution is denoted <span>$K_n(x, y) = \mathbb{P}(X_{k+n} = y | X_k = x),$</span> for <span>$n=1, 2, \ldots,$</span> independently of <span>$k=0, 1, \ldots.$</span></p><h2 id="Definitions"><a class="docs-heading-anchor" href="#Definitions">Definitions</a><a id="Definitions-1"></a><a class="docs-heading-anchor-permalink" href="#Definitions" title="Permalink"></a></h2><p>We start with some fundamental definitions.</p><h3 id="Return-time"><a class="docs-heading-anchor" href="#Return-time">Return time</a><a id="Return-time-1"></a><a class="docs-heading-anchor-permalink" href="#Return-time" title="Permalink"></a></h3><div class="admonition is-info" id="Definition-(return-time)-290f27b5cef1b13d"><header class="admonition-header">Definition (return time)<a class="admonition-anchor" href="#Definition-(return-time)-290f27b5cef1b13d" title="Permalink"></a></header><div class="admonition-body"><p>Given a point <span>$x\in\mathcal{X},$</span> we define the <strong>return time</strong> to <span>$x$</span> by</p><p class="math-container">\[    \tau_x = \inf\left\{n\in\mathbb{N}\cup\{+\infty\}; \; n = \infty \textrm{ or } X_n = x\right\}.\]</p></div></div><p>By definition, <span>$\tau_x$</span> is a random variable with values in <span>$\mathbb{N}.$</span> We call it <em>return time,</em> but, in the definition itself, we do not condition it on <span>$X_0 = x,$</span> so it is not always a &quot;return&quot; time, <em>per se;</em> the quantity</p><p class="math-container">\[    \mathbb{P}(\tau_x &lt; \infty)\]</p><p>is just the <em>probability of reaching <span>$x$</span> in a finite number of steps.</em> It is more like a &quot;first arrival&quot; time. Only when conditioned to <span>$X_0 = x$</span> that</p><p class="math-container">\[    \mathbb{P}(\tau_x &lt; \infty | X_0 = x)\]</p><p>is indeed the <em>probability of returning to <span>$x$</span> in a finite number of steps.</em> Meanwhile, for <span>$y \neq x,$</span></p><p class="math-container">\[    \mathbb{P}(\tau_y &lt; \infty | X_0 = x)\]</p><p>is the <em>probability of reaching <span>$y$</span> from <span>$x$</span> in a finite number of steps.</em> </p><h3 id="Number-of-passages"><a class="docs-heading-anchor" href="#Number-of-passages">Number of passages</a><a id="Number-of-passages-1"></a><a class="docs-heading-anchor-permalink" href="#Number-of-passages" title="Permalink"></a></h3><p>Another useful quantity is the random variable denoting the <em>number of passages</em> through a given state <span>$x\in\mathcal{X},$</span></p><div class="admonition is-info" id="Definition-(number-of-passages)-f98fd0edbbac47f6"><header class="admonition-header">Definition (number of passages)<a class="admonition-anchor" href="#Definition-(number-of-passages)-f98fd0edbbac47f6" title="Permalink"></a></header><div class="admonition-body"><p>The <strong>number of passages</strong> through a given state <span>$x\in\mathcal{X}$</span> is defined by</p><p class="math-container">\[    \eta_x = \#\{n\in\mathbb{N}; \;X_n = x\} = \sum_{n=1}^\infty \mathbb{1}_{\{X_n = x\}}.\]</p></div></div><p>By definition, <span>$\eta_x$</span> is a random variable with nonnegative integer values. Notice we did not include the starting time <span>$n=0$</span> in the definition. Some authors do, while others don&#39;t (e.g. <a href="https://doi.org/10.1007/978-1-4757-4145-2">Robert and Casella (2004)</a> don&#39;t, while <a href="https://doi.org/10.1201/9781315273600">Lawler (2006)</a> does).</p><h3 id="Relations-between-return-time-and-number-of-visits"><a class="docs-heading-anchor" href="#Relations-between-return-time-and-number-of-visits">Relations between return time and number of visits</a><a id="Relations-between-return-time-and-number-of-visits-1"></a><a class="docs-heading-anchor-permalink" href="#Relations-between-return-time-and-number-of-visits" title="Permalink"></a></h3><p>There are some important relations between return time and number of visits. Indeed, the first return time is finite if, and only if, the state is visited at least once. This is valid for each sample point. We can express this as</p><p class="math-container">\[    \tau_x(\omega) &lt; \infty \quad \Longleftrightarrow \quad \eta_x(\omega) \geq 1, \quad \forall \omega \in \Omega.\]</p><p>As a consequence,</p><p class="math-container">\[    \mathbb{P}(\tau_x &lt; \infty | X_0 = x) = \mathbb{P}(\eta_x \geq 1 | X_0 = x).\]</p><p>The complement of that is</p><p class="math-container">\[    \mathbb{P}(\tau_x = \infty | X_0 = x) = \mathbb{P}(\eta_x = 0 | X_0 = x).\]</p><p>More generally, the chances of having multiple visits is a power of the return time. This follows from the Markov property of the chain, since once back to <span>$x$</span> for the <span>$m-1$</span> time, the chances of coming back again is the same as coming back for the first time. And the chances of not returning after the <span>$m-1$</span> visit is the same as the chances of never arriving any time.</p><div class="admonition is-info" id="Proposition-(return-time-and-number-of-visits)-8d94fb38dec2e74f"><header class="admonition-header">Proposition (return time and number of visits)<a class="admonition-anchor" href="#Proposition-(return-time-and-number-of-visits)-8d94fb38dec2e74f" title="Permalink"></a></header><div class="admonition-body"><p>Let <span>$x\in\mathcal{X}$</span> and set</p><p class="math-container">\[    q = \mathbb{P}(\tau_x &lt; \infty | X_0 = x).\]</p><p>Then,</p><p class="math-container">\[    \mathbb{P}(\eta_x \geq m | X_0 = x) = q^m,\]</p><p>and</p><p class="math-container">\[    \mathbb{P}(\eta_x = m | X_0 = x) = q^m(1 - q),\]</p><p>for any <span>$m = 0, 1, 2, \ldots.$</span></p></div></div><p><strong>Proof.</strong> By definition, we have <span>$\eta_x \geq 0,$</span> thus the equality <span>$\mathbb{P}(\eta_x \geq m | X_0 = x) = q^m$</span> for <span>$m = 0$</span> is trivial. Now, for <span>$m\in\mathcal{N},$</span> we have</p><p class="math-container">\[    \begin{align*}
        \mathbb{P}(\eta_x \geq m | X_0 = x) &amp; = \mathbb{P}\bigg( \exist n_1, \ldots, n_m\in \mathbb{N}, X_i = x, n_{m-1} &lt; i \leq n_m \Leftrightarrow i = n_m \\
        &amp; \hspace{1in} \bigg| X_j = x, 0 \leq j \leq n_{m-1} \Leftrightarrow j = 0, n_1, \ldots, n_{m-1}\bigg) \\
        &amp; \quad \times \mathbb{P}\bigg( \exist n_1, \ldots, n_{m-1} \in \mathbb{N}, X_i = x, n_{m-2} &lt; i \leq n_{m-1} \Leftrightarrow i = n_{m-1} \\
        &amp; \hspace{1in} \bigg| X_j = x, 0 \leq j \leq n_{m-2} \Leftrightarrow j = 0, n_1, \ldots, n_{m-2}\bigg) \\
        &amp; \quad \times \cdots \\
        &amp; \quad \times \mathbb{P}\bigg( \exist n_1 \in \mathbb{N}, X_i = x, 0 &lt; i \leq n_1 \Leftrightarrow i = n_1 \bigg| X_0 = x \bigg)
    \end{align*}\]</p><p>By the Markov property of the chain, only the most recent conditioned state is important, so that</p><p class="math-container">\[    \begin{align*}
        \mathbb{P}(\eta_x \geq m | X_0 = x) &amp; = \mathbb{P}\left( \exist n_1, \ldots, n_m\in \mathbb{N}, X_i = x, 1\leq i \leq n_m \Leftrightarrow i = n_1, \ldots,n_m | X_0 = x\right) \\
        &amp; = \mathbb{P}\bigg( \exist n_{m-1}, n_m\in \mathbb{N}, X_i = x, n_{m-1} &lt; i \leq n_m \Leftrightarrow i = n_m \bigg| X_{n_{m-1}} = x\bigg) \\
        &amp; \;\; \times \mathbb{P}\bigg( \exist n_{m-2}, n_{m-1} \in \mathbb{N}, X_i = x, n_{m-2} &lt; i \leq n_{m-1} \Leftrightarrow i = n_{m-1} \bigg| X_{n_{m-2}} = x \bigg) \\
        &amp; \;\; \times \cdots \\
        &amp; \;\; \times \mathbb{P}\bigg( \exist n_1 \in \mathbb{N}, X_i = x, 0 &lt; i \leq n_1 \Leftrightarrow i = n_1 \bigg| X_0 = x \bigg)
    \end{align*}\]</p><p>By the time-homogeneous property, we can shift each probability by <span>$-n_{k-1}$</span> to see that only the time differences <span>$d_k = n_{k} - n_{k-1}$</span> matters, for <span>$k=1, \ldots, m,$</span> with all the events conditioned at the initial time <span>$n_0 = 0,$</span>, i.e.</p><p class="math-container">\[    \begin{align*}
        \mathbb{P}(\eta_x \geq m | X_0 = x) &amp; = \mathbb{P}\bigg( \exist d_m \in \mathbb{N}, X_i = x, 0 &lt; i \leq d_m \Leftrightarrow i = d_m \bigg| X_0 = x\bigg) \\
        &amp; \;\; \times \mathbb{P}\bigg( \exist d_{m-1} \in \mathbb{N}, X_i = x, 0 &lt; i \leq d_{m-1} \Leftrightarrow i = d_{m-1} \bigg| X_0 = x \bigg) \\
        &amp; \;\; \times \cdots \\
        &amp; \;\; \times \mathbb{P}\bigg( \exist d_1 \in \mathbb{N}, X_i = x, 0 &lt; i \leq d_1 \Leftrightarrow i = d_1 \bigg| X_0 = x \bigg)
    \end{align*}\]</p><p>The difference is just a matter of notation, for which we can denote them all by <span>$d,$</span> and write</p><p class="math-container">\[    \mathbb{P}(\eta_x \geq m | X_0 = x) = \mathbb{P}\bigg( \exist d \in \mathbb{N}, X_i = x, 0 &lt; i \leq d \Leftrightarrow i = d \bigg| X_0 = x\bigg)^m.\]</p><p>The existence of one such <span>$d$</span> is equivalent to <span>$\eta_x \geq 1,$</span> which is equivalent to <span>$\tau_x &lt; \infty,$</span> se we can write</p><p class="math-container">\[    \mathbb{P}(\eta_x \geq m | X_0 = x) = \mathbb{P}(\tau_x &lt; \infty | X_0 = x)^m = q^m,\]</p><p>for all <span>$m\in\mathbb{N},$</span> which completes the proof of the first statement.</p><p>Now, for any <span>$m=0, 1, 2, \ldots,$</span> the events <span>$\tau_x = m$</span> and <span>$\tau_x \geq m+1$</span> are independent, so that</p><p class="math-container">\[    \begin{align*}
        \mathbb{P}(\eta_x = m | X_0 = x) &amp; = \mathbb{P}(\eta_x \geq m | X_0 = x) - \mathbb{P}(\eta_x \geq m + 1 | X_0 = x) \\
        &amp; = q^m - q^{m+1} \\
        &amp; = q^m (1 - q),
    \end{align*}\]</p><p>which completes the proof. □  </p><h3 id="Recurrence-and-Transience"><a class="docs-heading-anchor" href="#Recurrence-and-Transience">Recurrence and Transience</a><a id="Recurrence-and-Transience-1"></a><a class="docs-heading-anchor-permalink" href="#Recurrence-and-Transience" title="Permalink"></a></h3><p>As the name says it, recurrence refers to a property that occurs repeatedly in time. In the case of a state <span>$x\in\mathcal{X},$</span> we are interested in knowning how often the state is observed, i.e. how often <span>$X_n = x,$</span> with respect to <span>$n\in\mathbb{N}.$</span></p><p>The idea is that a state that is not visited after some instant in time is, in some sense, <em>transient,</em> while others that get visited infinitely often in time are <em>recurrent.</em> But since this is a stochastic process, we must quantify <em>how</em> often this happens in a probabilistic way, i.e. with respect to the underlying probability measure. This can be measured by the probability of a state to be visited infinitely often in the chain, i.e.</p><p class="math-container">\[    \mathbb{P}(X_n = x \textrm{ infinitely often} | X_0 = x).\]</p><p>If the probability is one, we will almost surely observe this state infinitely many times. If the probability is zero, we will almost surely observe this state at most a finite number of times. What if the probability is in between zero and one? Could it be in a superpositioned state, i.e. in some instances it is visited infinitely often while in other instances it is visited only finitely-many times? Fortunately, this cannot happen. The probability is either zero or one, and we can definitely characterize it as recurrent or transient. This is a manifestation of the Kolmogorov zero-one law for the tail event </p><p class="math-container">\[    \{X_n = \textrm{i.o}\} = \bigcap_{n\in\mathbb{N}}\bigcup_{m\geq n}\{X_m = x\}.\]</p><div class="admonition is-info" id="Proposition-(zero-one-law-for-infinitely-many-visits)-c6a919d597eb9a28"><header class="admonition-header">Proposition (zero-one law for infinitely-many visits)<a class="admonition-anchor" href="#Proposition-(zero-one-law-for-infinitely-many-visits)-c6a919d597eb9a28" title="Permalink"></a></header><div class="admonition-body"><p>Consider a state <span>$x\in\mathcal{X}.$</span> Then either</p><p class="math-container">\[    \mathbb{P}(X_n = x \textrm{ infinitely often} | X_0 = x) = 1\]</p><p>or</p><p class="math-container">\[    \mathbb{P}(X_n = x \textrm{ infinitely often} | X_0 = x) = 0\]</p></div></div><p><strong>Proof.</strong> The idea is to use the Markov property of the chain, that if the probability of being visited once after the initial time is <span>$q,$</span> then, the probability of having <span>$m$</span> visits is <span>$q^m,$</span> to deduce that, at the limit <span>$m\rightarrow \infty,$</span> it is either zero or one, depending on whether <span>$0 \leq q &lt; 1$</span> or <span>$q = 1.$</span> </p><p>More precisely, we know that</p><p class="math-container">\[    \mathbb{P}(X_n = x \textrm{ infinitely often} | X_0 = x) = \mathbb{P}(\eta_x = \infty | X_0 = x).\]</p><p>This means</p><p class="math-container">\[    \mathbb{P}(X_n = x \textrm{ infinitely often} | X_0 = x) = \mathbb{P}(\eta_x \geq m, \;\forall m\in\mathbb{N} | X_0 = x).\]</p><p>But</p><p class="math-container">\[    \{\eta_x \geq m, \;\forall m\in\mathbb{N}, X_0 = x \} = \bigcap_{m\in\mathbb{N}}\{\eta_x \geq m, X_0 = x\},\]</p><p>with the intersection being of non-increasing sets, with respect to increasing <span>$m\in\mathbb{N}.$</span> Thus, by the continuity of probability measures,</p><p class="math-container">\[    \mathbb{P}(\eta_x \geq m, \;\forall m\in\mathbb{N}, X_0 = x) = \lim_{m\rightarrow} \mathbb{P}(\eta_x \geq m, X_0 = x).\]</p><p>Similarly,</p><p class="math-container">\[    \mathbb{P}(\eta_x \geq m, \;\forall m\in\mathbb{N} | X_0 = x) = \lim_{m\rightarrow} \mathbb{P}(\eta_x \geq m | X_0 = x).\]</p><p>We have already seen that</p><p class="math-container">\[    \mathbb{P}(\eta_x \geq m | X_0 = x) = q^m,\]</p><p>where</p><p class="math-container">\[    q = \mathbb{P}(\tau_x &lt; \infty | X_0 = x).\]</p><p>Thus,</p><p class="math-container">\[    \mathbb{P}(\eta_x \geq m, \;\forall m\in\mathbb{N} | X_0 = x) = \lim_{m\rightarrow} q^m.\]</p><p>Clearly,</p><p class="math-container">\[    \lim_{m\rightarrow} q^m = 0, \quad \textrm{if } 0 &lt; q \leq 1,\]</p><p>and</p><p class="math-container">\[    \lim_{m\rightarrow} q^m = 1, \quad \textrm{if } q = 1.\]</p><p>Since <span>$q$</span> is a probability, which can only assume values in the range <span>$0\leq q \leq 1,$</span> the only possible limits are 0 and 1, proving the result. □</p><p>With this in mind, we have the following definitions.</p><div class="admonition is-info" id="Definition-(recurrent-and-transient-states)-8149fa40fb31576"><header class="admonition-header">Definition (recurrent and transient states)<a class="admonition-anchor" href="#Definition-(recurrent-and-transient-states)-8149fa40fb31576" title="Permalink"></a></header><div class="admonition-body"><p>A state <span>$x$</span> is called <strong>recurrent</strong> when</p><p class="math-container">\[    \mathbb{P}(X_n = x \textrm{ infinitely often} | X_0 = x) = 1,\]</p><p>and is called <strong>transient</strong> when</p><p class="math-container">\[    \mathbb{P}(X_n = x \textrm{ infinitely often} | X_0 = x) = 0,\]</p><p>with no intermediate values being possible.</p></div></div><p>Equivalent definitions of recurrence can be made with the notions of number of passages and return time. In that regard, we have the following result, which we borrow, essentially, from <a href="https://doi.org/10.1201/9781315273600">Lawler (2006)</a>, except that we do not include the time <span>$n=0$</span> in the number of passages, so the formula is slightly different.</p><div class="admonition is-info" id="Theorem-(characterizations-of-recurrent-and-transient-states)-9ed03893685be28b"><header class="admonition-header">Theorem (characterizations of recurrent and transient states)<a class="admonition-anchor" href="#Theorem-(characterizations-of-recurrent-and-transient-states)-9ed03893685be28b" title="Permalink"></a></header><div class="admonition-body"><p>For any state <span>$x\in\mathcal{X},$</span> we have</p><p class="math-container">\[    x \textrm{ is recurrent } \quad \Longleftrightarrow \quad \mathbb{P}(\tau_x &lt; \infty | X_0 = x) = 1 \quad \Longleftrightarrow \quad \mathbb{E}[\eta_x | X_0 = x] = \infty,\]</p><p>and</p><p class="math-container">\[    x \textrm{ is transient } \quad \Longleftrightarrow \quad \mathbb{P}(\tau_x &lt; \infty | X_0 = x) &lt; 1 \quad \Longleftrightarrow \quad \mathbb{E}[\eta_x | X_0 = x] &lt; \infty.\]</p><p>Moreover, we have the relation</p><p class="math-container">\[    \mathbb{E}[\eta_x | X_0 = x] = \frac{\mathbb{P}(\tau_x &lt; \infty | X_0 = x)}{1 - \mathbb{P}(\tau_x &lt; \infty | X_0 = x)},\]</p><p>with the understanding that the left hand side is infinite when the probability in the right hand side is 1.</p></div></div><p><strong>Proof.</strong> We have that <span>$\tau_x = \infty$</span> iff <span>$X_n$</span> never returns to <span>$x.$</span> If <span>$\tau_x &lt; \infty,$</span> then it returns to <span>$x$</span> in finite time at least once. If <span>$\tau_x &lt; \infty$</span> with probability one, then, with probability one, it will return again and again to <span>$x,$</span> still with probability one, since the countable intersection of sets of full measure still has full measure. Thus,</p><p class="math-container">\[    \mathbb{P}(X_n = x \textrm{ infinitely often} | X_0 = x) = 1 \quad \Longleftrightarrow \quad \mathbb{P}(\tau_x &lt; \infty | X_0 = x) = 1.\]</p><p>This proves that <span>$x$</span> is recurrent if, and only if, <span>$\mathbb{P}(\tau_x &lt; \infty | X_0 = x) = 1,$</span> which is the first part of the equivalence in the recurrence case. The complement of that is precisely that <span>$x$</span> is transient if, and only if, <span>$\mathbb{P}(\tau_x &lt; \infty | X_0 = x) &lt; 1.$</span></p><p>For the remaining equivalences, let us suppose, first, that <span>$x$</span> is transient. Then, as we have seen,</p><p class="math-container">\[    q = \mathbb{P}(\tau_x &lt; \infty | X_0 = x) &lt; 1.\]</p><p>We compute the expectation of the number of passages by</p><p class="math-container">\[    \begin{align*}
        \mathbb{E}[\eta_x | X_0 = x] &amp; = \mathbb{E}\left[ \sum_{n=1}^\infty \mathbb{1}_{X_n = x} \bigg| X_0 = x\right] = \sum_{n=1}^\infty \mathbb{E}\left[\mathbb{1}_{X_n = x} \bigg| X_0 = x\right] \\
        &amp; = \sum_{n=1}^\infty \mathbb{P}(X_n = x | X_0 = x) = \sum_{n=1}^\infty p_n(x, x).
    \end{align*}\]</p><p>We can also compute this in a different way. Since <span>$\eta_x$</span> is always an integer, its expectation is given by</p><p class="math-container">\[    \mathbb{E}[\eta_x | X_0 = x] = \sum_{m=1}^\infty m \mathbb{P}(\eta_x = m | X_0 = x).\]</p><p>We need a way to calculate <span>$\mathbb{P}(\eta_x = m | X_0 = x),$</span> for each integer <span>$m\in\mathbb{N}.$</span> When <span>$\eta_x = m,$</span> it means it returns to <span>$x$</span> <span>$m$</span> times and then it does not return anymore. This means that <span>$\mathbb{P}(\eta_x = m | X_0 = x) = q^m.$</span> Thus,</p><p class="math-container">\[    \mathbb{E}[\eta_x | X_0 = x] = \sum_{m=1}^\infty m q^m.\]</p><p>Let <span>$S = \sum_{m=1}^\infty m q^m,$</span> so that <span>$qS = \sum_{m=1}^\infty m q^{m+1} = \sum_{m=2}^\infty (m-1)q^m,$</span> and hence</p><p class="math-container">\[    (1 - q)S = S - qS = q + \sum_{m=1}^\infty q^m = \sum_{m=1}^\infty q^m = \frac{q}{1 - q}.\]</p><p>Thus,</p><p class="math-container">\[    \mathbb{E}[\eta_x | X_0 = x] = \frac{\mathbb{P}(\tau_x &lt; \infty | X_0 = x)}{1 - \mathbb{P}(\tau_x &lt; \infty | X_0 = x)}.\]</p><p>(In <a href="https://doi.org/10.1201/9781315273600">Lawler (2006)</a>, the number of passages includes the initial time <span>$n=1,$</span> so that the formula obtained is <span>$1/(1-q),$</span> instead of <span>$q/(1 - q).$</span>)</p><p>When</p><p class="math-container">\[    q = \mathbb{P}(\tau_x &lt; \infty | X_0 = x) = 1,\]</p><p>then</p><p class="math-container">\[    \mathbb{P}(\tau_x &lt; \infty | X_0 = x) \geq r,\]</p><p>for any <span>$0 &lt; r &lt; 1,$</span> and we get, similarly, that</p><p class="math-container">\[    \mathbb{E}[\eta_x | X_0 = x] \geq \sum_{m=1}^\infty m r^m = \frac{r}{(1 - r)} \rightarrow \infty,\]</p><p>as <span>$r \rightarrow 1,$</span> so that <span>$\mathbb{E}[\eta_x | X_0 = x] = \infty.$</span></p><p>This proves the identity between the expectation and the probability. In particular, the expectation is finite if, and only if, the probability is strictly less than one, which proves the remaining equivalences. □</p><p>These equivalences are true, in general, only in the countable case; see page 222 of <a href="https://doi.org/10.1007/978-1-4757-4145-2">Robert and Casella (2004)</a>.</p><p>Another equivalence with recurrence is the following.</p><div class="admonition is-info" id="Proposition-(characterizations-of-recurrent-and-transient-states)-537f074fb41034b4"><header class="admonition-header">Proposition (characterizations of recurrent and transient states)<a class="admonition-anchor" href="#Proposition-(characterizations-of-recurrent-and-transient-states)-537f074fb41034b4" title="Permalink"></a></header><div class="admonition-body"><p>Given a state <span>$x\in\mathcal{X},$</span> we have the equality</p><p class="math-container">\[    \mathbb{E}[\eta_x | X_0 = x] = \sum_{n=1}^\infty K_n(x, x)\]</p><p>and, therefore, <span>$x$</span> is recurrent, when</p><p class="math-container">\[    \sum_{n=1}^\infty K_n(x, x) = \infty,\]</p><p>and <span>$x$</span> is transient, when</p><p class="math-container">\[    \sum_{n=1}^\infty K_n(x, x) &lt; \infty.\]</p></div></div><p><strong>Proof.</strong> The identity is proved using the definition of <span>$\eta_x.$</span> Indeed,</p><p class="math-container">\[    \begin{align*}
        \mathbb{E}[\eta_x | X_0 = x] &amp; = \mathbb{E}\left[\sum_{n=1}^\infty \mathbb{1}_{\{X_n = x\}} \bigg| X_0 = x\right] \\
        &amp; = \sum_{n=1}^\infty \mathbb{E}\left[\mathbb{1}_{\{X_n = x\}} \bigg| X_0 = x\right] \\
        &amp; = \sum_{n=1}^\infty \mathbb{P}\left[X_n = x \bigg| X_0 = x\right] \\
        &amp; = \sum_{n=1}^\infty K_n(x, x).
    \end{align*}\]</p><p>Now, the characterization of recurrence and transience of <span>$x$</span> follow from this identity and from the corresponding characterizations in terms of the expectation <span>$\mathbb{E}[\eta_x | X_0 = x].$</span> This completes the proof. □</p><p>For instance, we have seen that the random walk <span>$X_{n+1} = X_n + B_n,$</span> where <span>$B_n$</span> are Bernoulli i.i.d. with states <span>$+1$</span> with probability <span>$p$</span> and <span>$-1$</span> with probability <span>$1 - p,$</span> where <span>$0 &lt; p &lt; 1.$</span> For any <span>$x, y\in\mathcal{X}=\mathbb{Z}$</span> and <span>$n\in\mathbb{N},$</span> the transition probability <span>$K_n(x, y)$</span> is zero if <span>$|y - x|$</span> and <span>$n$</span> have different parity or when <span>$n &lt; |y - x|,$</span> and is given by the binomial distribution</p><p class="math-container">\[    p_n(m) = \begin{pmatrix} n \\ i \end{pmatrix} p^i(1-p)^j,\]</p><p>with</p><p class="math-container">\[    i = \frac{n + m}{2}, \quad j = n - i = \frac{n - m}{2}, \quad m = y - x,\]</p><p>when</p><p class="math-container">\[    |m| \leq n, \quad m, n \textrm{ with same parity}\]</p><p>Thus, we can write</p><p class="math-container">\[    K_n(x, y) = \begin{cases}
        \begin{pmatrix} n \\ \frac{n + y - x}{2} \end{pmatrix} p^{(n+y-x)/2}(1-p)^{(n + x - y)/2}, &amp; |x - y| \leq n, \; x - y, n \textrm{ same parity}, \\
        0, &amp; \textrm{otherwise}.
    \end{cases}\]</p><p>In particular, the probability of returning to <span>$x$</span> is</p><p class="math-container">\[    K_n(x, x) = \begin{cases}
        \begin{pmatrix} n \\ \frac{n}{2} \end{pmatrix} p^{n/2}(1-p)^{n/2}, &amp; n \textrm{ is even}, \\
        0, &amp; \textrm{otherwise}.
    \end{cases}\]</p><p>We have</p><p class="math-container">\[    \{X_n = x \textrm{ infinitely often} | X_0 = x\} = \bigcap_{n \in\mathbb{N}}\bigcup_{m \geq n, m\in \mathbb{N}} \{X_m = x | X_0 = x\}.\]</p><p>Thus, by the Borel-Cantelli Lemma,</p><p class="math-container">\[    \mathbb{P}\left(X_n = x \textrm{ infinitely often} | X_0 = x\right) = 0,\]</p><p>since</p><p class="math-container">\[    \begin{align*}
        \sum_{n\in\mathbb{N}} \mathbb{P}\left(X_m = x | X_0 = x\right) &amp; = \sum_{n\in\mathbb{N}, n \textrm{ even}} \begin{pmatrix} n \\ \frac{n}{2} \end{pmatrix} p^{n/2}(1-p)^{n/2} \\
        &amp; = \sum_{n\in\mathbb{N}} \begin{pmatrix} 2n \\ n \end{pmatrix} p^n(1-p)^n = \sum_{n\in\mathbb{N}} \frac{(2n)!}{2(n!)} (p(1-p))^n.
    \end{align*}\]</p><p>Using Stirling&#39;s approximation</p><p class="math-container">\[    n! \approx \sqrt{2\pi n}\left(\frac{n}{e}\right)^n, \quad (2n)! \approx \sqrt{4\pi n}\left(\frac{2n}{e}\right)^{2n},\]</p><p>we have</p><p class="math-container">\[    \frac{(2n)!}{2(n!)} \approx \frac{\sqrt{4\pi n}}{2\sqrt{2\pi n}}\left(\frac{2n}{e}\right)^{2n}\left(\frac{e}{n}\right)^n = \frac{\sqrt{2}}{2}\left(\frac{4n}{e}\right)^n,\]</p><p>and we find</p><p class="math-container">\[    \sum_{n\in\mathbb{N}} \mathbb{P}\left(X_m = x | X_0 = x\right) \approx \frac{\sqrt{2}}{2}\sum_{n\in\mathbb{N}} \left(\frac{4np(1-p)}{e}\right)^n = \infty.\]</p><p>Thus, the chain is recurrent.</p><h3 id="Recurrent-chain"><a class="docs-heading-anchor" href="#Recurrent-chain">Recurrent chain</a><a id="Recurrent-chain-1"></a><a class="docs-heading-anchor-permalink" href="#Recurrent-chain" title="Permalink"></a></h3><p>When every state is recurrent we say that the chain is recurrent.</p><div class="admonition is-info" id="Definition-(recurrent-chain)-51bc761ab0badc9f"><header class="admonition-header">Definition (recurrent chain)<a class="admonition-anchor" href="#Definition-(recurrent-chain)-51bc761ab0badc9f" title="Permalink"></a></header><div class="admonition-body"><p>The Markov chain is called <strong>recurrent</strong> when every state is recurrent, i.e.</p><p class="math-container">\[    \mathbb{P}(X_n = x \textrm{ infinitely often} | X_0 = x) = 1, \quad \forall x\in\mathcal{X}.\]</p></div></div><h2 id="Existence-of-invariant-distribution"><a class="docs-heading-anchor" href="#Existence-of-invariant-distribution">Existence of invariant distribution</a><a id="Existence-of-invariant-distribution-1"></a><a class="docs-heading-anchor-permalink" href="#Existence-of-invariant-distribution" title="Permalink"></a></h2><p>Recurrence is a fundamental property associated with the existence of invariant measures. But the associated invariant measure may be finite or infinite. If it is finite, then we can normalize it and obtain a stationary probability distribution. The condition for finiteness of the invariant measure associated with a recurrent state <span>$x$</span> is that the expectation of <span>$\tau_x,$</span> conditioned to <span>$X_0 = x,$</span> be finite. If such expectation is finite, then necessarily <span>$\tau_x$</span> is finite almost surely and, hence, <span>$x$</span> is recurrent. But a state can be recurrent without this expectation being finite. Some authors call the latter case <em>null recurrence,</em> meaning that the state is recurrent but it is not associated with a stationary probability distribution. Otherwise, it is called <em>positive recurrence.</em></p><div class="admonition is-info" id="Theorem-(recurrence-implies-existence-of-invariant-measure)-1549e601fc3dd65e"><header class="admonition-header">Theorem (recurrence implies existence of invariant measure)<a class="admonition-anchor" href="#Theorem-(recurrence-implies-existence-of-invariant-measure)-1549e601fc3dd65e" title="Permalink"></a></header><div class="admonition-body"><p>Suppose that <span>$x\in\mathcal{X}$</span> is recurrent. Then</p><p class="math-container">\[    {\tilde P}_x(z) = \sum_{n=1}^\infty \mathbb{P}(X_n = z, n \leq \tau_{x} | X_0 = x) = \mathbb{E}\left[\sum_{n=1}^{\tau_x} \mathbb{1}_{X_n = y} \bigg| X_0 = x\right]\]</p><p>defines a non-trivial invariant measure for the Markov chain, which may be finite or infinite, with</p><p class="math-container">\[    {\tilde P}_x(\mathcal{X}) = \mathbb{E}[\tau_{x} | X_0 = x].\]</p><p>Thus, if, moreover,</p><p class="math-container">\[    \mathbb{E}[\tau_{x} | X_0 = x] &lt; \infty,\]</p><p>then this measure is finite and can be normalized to a stationary probability distribution</p><p class="math-container">\[    P_x(z) = \frac{1}{\mathbb{E}[\tau_{x} | X_0 = x]} \sum_{n=1}^\infty \mathbb{P}(X_n = z, \tau_{x} \geq n | X_0 = x).\]</p></div></div><p>The proof below is adapted from Theorem 6.37 of <a href="https://doi.org/10.1007/978-1-4757-4145-2">Robert and Casella (2004)</a>. Notice we are not assuming irreducibility, so we are not claiming that this invariant distribution is unique. In fact, we may have several non-connected recurrent state, each associated with a different invariant measure. We&#39;ll later see the condition of <em>irreducibility</em> to avoid such non-connected states and find a <em>unique</em> invariant probability distribution.</p><p><strong>Proof.</strong> First of all, since the space is discrete and each <span>${\tilde P}_x(z) \geq 0,$</span> for <span>$z\in\mathcal{X},$</span> it follows that <span>${\tilde P}_x$</span> defines indeed a measure on <span>$\mathcal{X}.$</span> Let us check that <span>${\tilde P}_x$</span> is in fact a nontrivial measure and that it is invariant. </p><p>In order to show that it is non-trivial, we prove that <span>${\tilde P}_x(x) = 1.$</span> This will also be a crucial fact in the proof of invariance. This follows from</p><p class="math-container">\[    \begin{align*}
        {\tilde P}_x(x) &amp; = \sum_{n=1}^\infty \mathbb{P}(X_n = x, \tau_{x} \geq n | X_0 = x) \\
        &amp; = \sum_{n=1}^\infty \mathbb{P}(X_n = x, \tau_{x} = n | X_0 = x) \\
        &amp; = \sum_{n=1}^\infty \mathbb{P}(\tau_{x} \geq n | X_0 = x).
    \end{align*}\]</p><p>Thanks to the recurrence assumption on <span>$x,$</span> we have <span>$\tau_x &lt; \infty$</span> almost surely, when conditioned on <span>$X_0 = x,$</span> which means <span>$\tau_x = \infty | X_0 = x$</span> has zero measure. Thus, </p><p class="math-container">\[    \begin{align*}
        {\tilde P}_x(x) &amp; = \sum_{n=1}^\infty \mathbb{P}(n \leq \tau_{x} &lt; \infty | X_0 = x) \\
        &amp; = \mathbb{P}(\cup_{n\in\mathbb{N}} \{\tau_{x} = n | X_0 = x\}) \\
        &amp; = \mathbb{P}(\tau_{x} &lt; \infty | X_0 = x).
    \end{align*}\]</p><p>Using again the fact that <span>$x$</span> is recurrent, we have <span>$\mathbb{P}(\tau_{x} &lt; \infty | X_0 = x) = 1,$</span> which means</p><p class="math-container">\[    {\tilde P}_x(x) = 1,\]</p><p>as we wanted.</p><p>Now, let us check that <span>${\tilde P}_x$</span> is invariant. We need to show that</p><p class="math-container">\[    \sum_{y\in\mathcal{X}} K(y, z){\tilde P}_x(y) = {\tilde P}_x(z),\]</p><p>for every <span>$z\in\mathcal{X}.$</span> For that, we write</p><p class="math-container">\[    \begin{align*}
        \sum_{y\in\mathcal{X}} K(y, z){\tilde P}_x(y) &amp; = \sum_{y\in\mathcal{X}} K(y, z)\sum_{n=1}^\infty \mathbb{P}(X_n = y, \tau_{x} \geq n | X_0 = x) \\
        &amp; = \sum_{n=1}^\infty \sum_{y\in\mathcal{X}} K(y, z) \mathbb{P}(X_n = y, \tau_{x} \geq n | X_0 = x)
    \end{align*}\]</p><p>We split the sum according to <span>$y=x$</span> and <span>$y\neq x,$</span> so that</p><p class="math-container">\[    \begin{align*}
        \sum_{y\in\mathcal{X}} K(y, z){\tilde P}_x(y) &amp; = \sum_{n=1}^\infty K(x, z) \mathbb{P}(X_n = x, \tau_{x} \geq n | X_0 = x) \\
        &amp; \qquad + \sum_{n=1}^\infty \sum_{y\neq x} K(y, z) \mathbb{P}(X_n = y, \tau_{x} \geq n | X_0 = x) \\
        &amp; = K(x, z) {\tilde P}_x(x) + \sum_{n=1}^\infty \sum_{y\neq x} \mathbb{P}(X_{n+1} = z, X_n = y, \tau_{x} \geq n | X_0 = x) \\
    \end{align*}\]</p><p>For <span>$y\neq z,$</span> we have</p><p class="math-container">\[    \mathbb{P}(X_{n+1} = z, X_n = y, \tau_{x} \geq n | X_0 = x) = \mathbb{P}(X_{n+1} = z, X_n = y, \tau_{x} \geq n+1 | X_0 = x).\]</p><p>Thus,</p><p class="math-container">\[    \begin{align*}
        \sum_{y\in\mathcal{X}} K(y, z){\tilde P}_x(y) &amp; = K(x, z) {\tilde P}_x(x) + \sum_{n=1}^\infty \sum_{y\neq x} \mathbb{P}(X_{n+1} = z, X_n = y, \tau_{x} \geq n+1 | X_0 = x) \\
        &amp; = K(x, z) {\tilde P}_x(x) + \sum_{n=1}^\infty \mathbb{P}(X_{n+1} = z, \tau_{x} \geq n+1 | X_0 = x) \\
        &amp; = K(x, z) {\tilde P}_x(x) + \sum_{n=2}^\infty \mathbb{P}(X_{n} = z, \tau_{x} \geq n | X_0 = x),
    \end{align*}\]</p><p>where in the last step we just reindexed the summation. Now we use that <span>${\tilde P}_x(x) = 1$</span> (as proved above) and that </p><p class="math-container">\[    K(x, z) = \mathbb{P}(X_1 = z | X_0 = x) = \mathbb{P}(X_1 = z, \tau_{x} \geq 1 | X_0 = x)\]</p><p>(since <span>$\tau_{x} \geq 1$</span> always), to obtain</p><p class="math-container">\[    \begin{align*}
        \sum_{y\in\mathcal{X}} K(y, z){\tilde P}_x(y) &amp; = \mathbb{P}(X_1 = z, \tau_{x} \geq 1 | X_0 = x) + \sum_{n=2}^\infty \mathbb{P}(X_{n} = z, \tau_{x} \geq n | X_0 = x) \\
        &amp; = \sum_{n=1}^\infty \mathbb{P}(X_{n} = z, \tau_{x} \geq n | X_0 = x) \\
        &amp; = {\tilde P}_x(z),
    \end{align*}\]</p><p>proving the invariance.</p><p>Now we compute <span>${\tilde P}_x(\mathcal{X}).$</span> We have</p><p class="math-container">\[    \begin{align*}
        {\tilde P}_x(\mathcal{X}) &amp; = \sum_{z\in\mathcal{X}} P_x(z) \\
        &amp; = \sum_{z\in\mathcal{X}}\sum_{n=1}^\infty \mathbb{P}(X_n = z, \tau_{x} \geq n | X_0 = x) \\
        &amp; = \sum_{n=1}^\infty \sum_{z\in\mathcal{X}} \mathbb{P}(X_n = z, \tau_{x} \geq n | X_0 = x) \\
        &amp; = \sum_{n=1}^\infty \mathbb{P}( \tau_{x} \geq n | X_0 = x) \\
        &amp; = \sum_{n=1}^\infty \sum_{m=n}^\infty \mathbb{P}(\tau_{x} = m | X_0 = x) \\
        &amp; = \sum_{m=1}^\infty \sum_{n=1}^{m} \mathbb{P}(\tau_{x} = m | X_0 = x) \\
        &amp; = \sum_{m=1}^\infty m \mathbb{P}(\tau_{x} = m | X_0 = x) \\
        &amp; = \mathbb{E}[\tau_{x} | X_0 = x]
    \end{align*}\]</p><p>If we assumed that </p><p class="math-container">\[    \mathbb{E}[\tau_{x} | X_0 = x] &lt; \infty,\]</p><p>it follows that the measure <span>${\tilde P}$</span> is finite, so we can normalize <span>${\tilde P}$</span> by this expectation to obtain the invariant probability distribution</p><p class="math-container">\[    P(z) = \frac{1}{\mathbb{E}[\tau_{x} | X_0 = x]} \sum_{n=1}^\infty \mathbb{P}(X_{n} = z, \tau_{x} \geq n | X_0 = x).\]</p><p>This concludes the proof.□</p><p><strong>Remark.</strong> Notice that <span>${\tilde P}_x(z)$</span> defines a measure regardless of <span>$x$</span> being recurrent or not. And it is always non-trivial, in fact, because <span>$\sum_{y\in \mathbb{Z}} K(x, z) = 1,$</span> there must exist some <span>$z\in\mathcal{Z}$</span> for which <span>$X_0 = x$</span> and <span>$X_1 = z,$</span> with positive probability, and thus, since <span>$\tau_x \geq 1$</span> always,</p><p class="math-container">\[    {\tilde P}_x(z) = \sum_{n=1}^\infty \mathbb{P}(X_n = z, n \leq \tau_{x} | X_0 = x) \geq \mathbb{P}(X_1 = z, \tau_x \geq 1 | X_0 = 1) = \mathbb{P}(X_1 = z | X_0 1) = K(x, z) &gt; 0.\]</p><p>But this measure may not be invariant. The recurrence is needed to assure that <span>${\tilde P}_x$</span> is invariant.</p><p><strong>Remark.</strong> The expression</p><p class="math-container">\[        {\tilde P}_x(z) = \sum_{n=1}^\infty \mathbb{P}(X_n = z, n \leq \tau_{x} | X_0 = x) = \mathbb{E}\left[\sum_{n=1}^{\tau_x} \mathbb{1}_{X_n = y} \bigg| X_0 = x\right]\]</p><p>for the invariant measure appears naturally when we assume that an invariant measure exists. Indeed, for an invariant measure <span>${\tilde P}$</span> and for any two states <span>$x, z\in\mathcal{X},$</span> one can show, by recusively using that the measure is invariant and by splitting the corresponding summation into the state equal to <span>$x$</span> and the states different from <span>$x,$</span> that</p><p class="math-container">\[    {\tilde P}(z) \geq {\tilde P}_x(z){\tilde P}(x).\]</p><p>This inequality will be proved in the following pages. This is then used to prove the uniqueness (up to a multiplicative constant) of the invariant measure (local or global, depending on whether just the chain is reducible or irreducible). In any case, we see, from this calculation, that the expression for <span>${\tilde P}_x(z)$</span> appears naturally from the hypothesis of invariance alone. This expression measures how often the chain visits a certain state <span>$z.$</span> When in statistical equilibrium, this is what we expect as how frequent the state is observed.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../mc_invariance/">« Invariant distributions</a><a class="docs-footer-nextpage" href="../mc_countableX_connections/">Connected states, irreducibility and uniqueness of invariant measures »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.15.0 on <span class="colophon-date" title="Thursday 6 November 2025 15:39">Thursday 6 November 2025</span>. Using Julia version 1.12.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
