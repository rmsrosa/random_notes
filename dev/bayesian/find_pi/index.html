<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Estimating π via frequentist and Bayesian methods · Random notes</title><meta name="title" content="Estimating π via frequentist and Bayesian methods · Random notes"/><meta property="og:title" content="Estimating π via frequentist and Bayesian methods · Random notes"/><meta property="twitter:title" content="Estimating π via frequentist and Bayesian methods · Random notes"/><meta name="description" content="Documentation for Random notes."/><meta property="og:description" content="Documentation for Random notes."/><meta property="twitter:description" content="Documentation for Random notes."/><meta property="og:url" content="https://github.com/rmsrosa/random_notes/bayesian/find_pi/"/><meta property="twitter:url" content="https://github.com/rmsrosa/random_notes/bayesian/find_pi/"/><link rel="canonical" href="https://github.com/rmsrosa/random_notes/bayesian/find_pi/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/style.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="Random notes logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Random notes</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Random Notes</a></li><li><span class="tocitem">Probability Essentials</span><ul><li><a class="tocitem" href="../../probability/kernel_density_estimation/">Kernel Density Estimation</a></li><li><a class="tocitem" href="../../probability/convergence_notions/">Convergence notions</a></li></ul></li><li><span class="tocitem">Discrete-time Markov chains</span><ul><li><a class="tocitem" href="../../markov_chains/mc_definitions/">Essential definitions</a></li><li><a class="tocitem" href="../../markov_chains/mc_invariance/">Invariant distributions</a></li><li><input class="collapse-toggle" id="menuitem-3-3" type="checkbox"/><label class="tocitem" for="menuitem-3-3"><span class="docs-label">Countable-space Markov chains</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../markov_chains/mc_countableX_recurrence/">Recurrence in the countable-space case</a></li><li><a class="tocitem" href="../../markov_chains/mc_countableX_connections/">Connected states, irreducibility and uniqueness of invariant measures</a></li><li><a class="tocitem" href="../../markov_chains/mc_countableX_convergencia/">Aperiodicidade e convergência para a distribuição estacionária</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-4" type="checkbox"/><label class="tocitem" for="menuitem-3-4"><span class="docs-label">Continuous-space Markov chains</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../markov_chains/mc_irreducibility_and_recurrence/">Irreducibility and recurrence in the continuous-space case</a></li></ul></li></ul></li><li><span class="tocitem">Sampling methods</span><ul><li><a class="tocitem" href="../../sampling/overview/">Overview</a></li><li><a class="tocitem" href="../../sampling/prng/">Random number generators</a></li><li><input class="collapse-toggle" id="menuitem-4-3" type="checkbox"/><label class="tocitem" for="menuitem-4-3"><span class="docs-label">Transform methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/invFtransform/">Probability integral transform</a></li><li><a class="tocitem" href="../../sampling/box_muller/">Box-Muller transform</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-4" type="checkbox"/><label class="tocitem" for="menuitem-4-4"><span class="docs-label">Accept-Reject methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/rejection_sampling/">Rejection sampling</a></li><li><a class="tocitem" href="../../sampling/empiricalsup_rejection/">Empirical supremum rejection sampling</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-5" type="checkbox"/><label class="tocitem" for="menuitem-4-5"><span class="docs-label">Markov Chain Monte Carlo (MCMC)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sampling/mcmc/">Overview</a></li><li><a class="tocitem" href="../../sampling/metropolis/">Metropolis and Metropolis-Hastings</a></li><li><a class="tocitem" href="../../sampling/convergence_metropolis/">Convergence of Metropolis-Hastings</a></li><li><a class="tocitem" href="../../sampling/gibbs/">Gibbs sampling</a></li><li><a class="tocitem" href="../../sampling/hmc/">Hamiltonian Monte Carlo (HMC)</a></li></ul></li><li><a class="tocitem" href="../../sampling/langevin_sampling/">Langevin sampling</a></li></ul></li><li><span class="tocitem">Bayesian inference</span><ul><li><input class="collapse-toggle" id="menuitem-5-1" type="checkbox"/><label class="tocitem" for="menuitem-5-1"><span class="docs-label">Bayes Theory</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../bayes/">Bayes Theorem</a></li><li><a class="tocitem" href="../bayes_inference/">Bayesian inference</a></li><li><a class="tocitem" href="../bernstein_vonmises/">Bernstein–von Mises theorem</a></li></ul></li><li><a class="tocitem" href="../bayesian_probprog/">Bayesian probabilistic programming</a></li><li><input class="collapse-toggle" id="menuitem-5-3" type="checkbox" checked/><label class="tocitem" for="menuitem-5-3"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>Estimating π via frequentist and Bayesian methods</a><ul class="internal"><li><a class="tocitem" href="#The-Julia-packages"><span>The Julia packages</span></a></li><li><a class="tocitem" href="#The-frequentist-approach"><span>The frequentist approach</span></a></li><li><a class="tocitem" href="#The-Bayesian-approach"><span>The Bayesian approach</span></a></li><li><a class="tocitem" href="#Performance-of-the-two-methods"><span>Performance of the two methods</span></a></li></ul></li><li><a class="tocitem" href="../linear_regression/">Many Ways to Linear Regression</a></li><li><a class="tocitem" href="../tilapia_alometry/">Alometry law for the Nile Tilapia</a></li><li><a class="tocitem" href="../mortality_tables/">Modeling mortality tables</a></li></ul></li></ul></li><li><span class="tocitem">Generative models</span><ul><li><input class="collapse-toggle" id="menuitem-6-1" type="checkbox"/><label class="tocitem" for="menuitem-6-1"><span class="docs-label">Score matching</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../generative/overview/">Overview</a></li><li><a class="tocitem" href="../../generative/stein_score/">Stein score function</a></li><li><a class="tocitem" href="../../generative/score_matching_aapo/">Score matching of Aapo Hyvärinen</a></li><li><a class="tocitem" href="../../generative/score_matching_neural_network/">Score matching a neural network</a></li><li><a class="tocitem" href="../../generative/parzen_estimation_score_matching/">Score matching with Parzen estimation</a></li><li><a class="tocitem" href="../../generative/denoising_score_matching/">Denoising score matching of Pascal Vincent</a></li><li><a class="tocitem" href="../../generative/sliced_score_matching/">Sliced score matching</a></li><li><a class="tocitem" href="../../generative/1d_FD_score_matching/">1D finite-difference score matching</a></li><li><a class="tocitem" href="../../generative/2d_FD_score_matching/">2D finite-difference score matching</a></li><li><a class="tocitem" href="../../generative/ddpm/">Denoising diffusion probabilistic models</a></li><li><a class="tocitem" href="../../generative/mdsm/">Multiple denoising score matching</a></li><li><a class="tocitem" href="../../generative/probability_flow/">Probability flow</a></li><li><a class="tocitem" href="../../generative/reverse_flow/">Reverse probability flow</a></li><li><a class="tocitem" href="../../generative/score_based_sde/">Score-based SDE model</a></li></ul></li></ul></li><li><span class="tocitem">Sensitivity analysis</span><ul><li><a class="tocitem" href="../../sensitivity/overview/">Overview</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Bayesian inference</a></li><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Estimating π via frequentist and Bayesian methods</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Estimating π via frequentist and Bayesian methods</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/rmsrosa/random_notes" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/rmsrosa/random_notes/blob/main/docs/src/bayesian/find_pi.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Estimating-the-value-of-π-via-frequentist-and-Bayesian-methods"><a class="docs-heading-anchor" href="#Estimating-the-value-of-π-via-frequentist-and-Bayesian-methods">Estimating the value of π via frequentist and Bayesian methods</a><a id="Estimating-the-value-of-π-via-frequentist-and-Bayesian-methods-1"></a><a class="docs-heading-anchor-permalink" href="#Estimating-the-value-of-π-via-frequentist-and-Bayesian-methods" title="Permalink"></a></h1><p>In the frequentist approach, we draw a number of samples uniformly distributed in the unit square and compute how many of them fall into the quarter circle. This yields an estimate for the area of the quarter circle along with confidence intervals. </p><p>In the Bayesian approach, we start with a prior estimating the value of pi and update our prior to refine the estimate and the confidence levels, according to the posterior.</p><h2 id="The-Julia-packages"><a class="docs-heading-anchor" href="#The-Julia-packages">The Julia packages</a><a id="The-Julia-packages-1"></a><a class="docs-heading-anchor-permalink" href="#The-Julia-packages" title="Permalink"></a></h2><p>For the numerical implementation, we need some packages.</p><p>We use <code>Distributions.jl</code> for the common distributions, such as Uniform, Beta, Bernoulli, Normal, etc.</p><pre><code class="language-julia hljs">using Distributions</code></pre><p>For reproducibility, we set the seed for the pseudo random number generators.</p><pre><code class="language-julia hljs">using Random
Random.seed!(12)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Random.TaskLocalRNG()</code></pre><p>For plotting, we use <code>StatsPlots.jl</code></p><pre><code class="language-julia hljs">using StatsPlots</code></pre><h2 id="The-frequentist-approach"><a class="docs-heading-anchor" href="#The-frequentist-approach">The frequentist approach</a><a id="The-frequentist-approach-1"></a><a class="docs-heading-anchor-permalink" href="#The-frequentist-approach" title="Permalink"></a></h2><p>This is a classical example illustrating the Monte Carlo method. We generate a bunch of samples <span>$(x_i, y_i)$</span> from two independent uniform distributions on the interval <span>$[0, 1]$</span> and check whether they belong to the unit circle (quarter circle, more precisely) or not, i.e. whether <span>$x_i^2 + y_i^2 \leq 1.$</span> The distribution uniformly fills up the unit square, which has area one, and some of them will be in the quarter circle. The proportion of those in the circle approximates the area of the quarter circle over that of the unit square, i.e. <span>$\pi/4$</span>. Multiplying it by four, yields an estimate for <span>$\pi$</span>. The more samples we use, the closer we expect the mean to approximate this value.</p><p>We start choosing a maximum number <code>N</code> of samples. We will analyse the estimate for each <code>i</code> up to <code>N</code>, to have an idea how the value and our confidence on it improves with the number of samples.</p><pre><code class="language-julia hljs">N = 10_000</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10000</code></pre><p>Now we sample <code>N</code> pairs of numbers uniformly on the unit square</p><pre><code class="language-julia hljs">positions_f = rand(N, 2)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10000×2 Matrix{Float64}:
 0.625279   0.031483
 0.850311   0.983641
 0.801272   0.2462
 0.318351   0.667168
 0.542473   0.188848
 0.907512   0.826646
 0.475454   0.897492
 0.213723   0.495981
 0.0139892  0.639093
 0.986802   0.162814
 ⋮          
 0.431214   0.560566
 0.948446   0.853894
 0.446732   0.661736
 0.284514   0.695814
 0.770592   0.36783
 0.256993   0.848068
 0.696902   0.568515
 0.870369   0.568688
 0.747474   0.627833</code></pre><p>With the sample at hand, we compute their distance to the origin and check whether they belong to the unit circle or not, giving a sequence <code>x_f</code> of random variables with values <code>1</code> or <code>0</code>, with the respective indication.</p><pre><code class="language-julia hljs">distance_f = sum(abs2, positions_f, dims=2)
x_f = vec(distance_f) .≤ 1</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10000-element BitVector:
 1
 0
 1
 1
 1
 0
 0
 1
 1
 0
 ⋮
 1
 0
 1
 1
 1
 1
 1
 0
 1</code></pre><p>For each <code>n</code> between <code>1</code> and <code>N</code>, we compute the sample mean <code>q_f[n]</code> of <code>x_f[1], …, x_f[n]</code>, and the sample standard error <code>s_f[n]</code>.</p><pre><code class="language-julia hljs">q_f = cumsum(x_f) ./ (1:N)
s_f = [1.0; [√(var(view(x_f, 1:n), mean=q_f[n])/n) for n in 2:N]]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10000-element Vector{Float64}:
 1.0
 0.5
 0.33333333333333337
 0.25
 0.20000000000000004
 0.21081851067789198
 0.20203050891044214
 0.18298126367784998
 0.16666666666666666
 0.1632993161855452
 ⋮
 0.0040796702887405945
 0.004080026686745186
 0.004079672918365696
 0.004079319209146778
 0.0040789655590744035
 0.004078611968134568
 0.004078258436313256
 0.0040786146186127074
 0.00407826117621495</code></pre><p>The sample means approximate the value of <span>$\pi/4$</span>, so we multiply it by <span>$4$</span> to have an estimate of <span>$\pi$</span>. Accordingly, we multipy the standard error by <span>$4$</span>. The 95% confidence interval is estimated by twice the standard error around the mean. This is illustrated in the following plots. Of course, for small samples, we should use the t-Student distribution, but we concentrate on not-so-small samples and just use the normal distribution, relying on the Central Limit Theorem.</p><pre><code class="language-julia hljs">plot(10:N, 4q_f[10:N], ribbon = 8s_f[10:N], label=&quot;estimate&quot;)
hline!(10:N, [π], label=&quot;true value&quot;)</code></pre><img src="a7154a19.svg" alt="Example block output"/><p>A close up of the 10% first samples</p><pre><code class="language-julia hljs">Nrange = 10:min(N, div(N, 10))
plot(Nrange, 4q_f[Nrange], ribbon = 8s_f[Nrange], label=&quot;estimate&quot;)
hline!(Nrange, [π], label=&quot;true value&quot;)</code></pre><img src="1d16da23.svg" alt="Example block output"/><p>The probability distribution for the estimate of <span>$pi$</span> after <code>N</code> samples is illustrated below in a few cases.</p><pre><code class="language-julia hljs">pp = 0.0:0.001:1.0

plt = plot(title=&quot;Evolution of our belief in the value of π\nwith respect to the sample size n&quot;, titlefont=10, xlims=(0.5, 1.0))

for n in (div(N, 1000), div(N, 100), div(N, 10), N)
    plot!(plt, pp, pdf.(Normal(q_f[n], s_f[n]), pp), label=&quot;n=$n&quot;, fill=true, alpha=0.5)
end
vline!(plt, [π/4], color=:black, label=&quot;π/4&quot;)</code></pre><img src="9066e4ee.svg" alt="Example block output"/><h2 id="The-Bayesian-approach"><a class="docs-heading-anchor" href="#The-Bayesian-approach">The Bayesian approach</a><a id="The-Bayesian-approach-1"></a><a class="docs-heading-anchor-permalink" href="#The-Bayesian-approach" title="Permalink"></a></h2><p>In the Bayesian approach, we start guessing the area of the quarter circle, or, more precisely, the probability that it be a certain value within a certain range. It is reasonable to assume it is a little over half the area of the unit circle and not too close to 1, with higher probability of being closer to the middle of these two values. We could use a normal distribution, but a better choice is a Beta distribution since it is conjugate to the likelihood, which is expected to be a Beta distribution <span>$B(\alpha, \beta)$</span> with density <span>$x^\alpha (1 - x)^\beta$</span>, where <span>$\alpha$</span> counts as the number of success draws (within the quarter circle) and <span>$\beta$</span> as the number of failures (outside the quarter circle). </p><p>So we choose as prior the distribution <span>$Beta(\alpha, \beta)$</span> with something like <span>$\alpha = 24$</span> and <span>$\beta = 8$</span>, in which case the mean is <span>$\alpha / (\alpha + \beta) = 24/32 = 3/4 = 0.75$</span> and the variance is <span>$αβ/((α + β)^2(α + β + 1)) = 192/33792 ≈ 0.00568$</span>. These are our <em>hyperparameters.</em> Let us visualize this prior distribution</p><pre><code class="language-julia hljs">prior_distribution = Beta(24,8)

plt = plot(pp, pdf.(prior_distribution, pp), label=nothing, title=&quot;Density of the (prior) distribution $prior_distribution\nmean = $(round(mean(prior_distribution), sigdigits=4)); standard deviation = $(round(std(prior_distribution), sigdigits=5))&quot;, titlefont=10, fill=true, alpha=0.5, legend=:topleft)

vline!(plt, [mean(prior_distribution)], label=&quot;mean $(round(mean(prior_distribution), sigdigits=4))&quot;)

vline!(plt, mean(prior_distribution) .+ [-std(prior_distribution), +std(prior_distribution)], label=&quot;mean +/- std: $(round(mean(prior_distribution), sigdigits=4)) +/- $(round(std(prior_distribution), sigdigits=5))&quot;)</code></pre><img src="27684935.svg" alt="Example block output"/><p>Just for the sake of illustration, we draw some sample from the prior</p><pre><code class="language-julia hljs">priordata = rand(prior_distribution, N)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10000-element Vector{Float64}:
 0.6835609325773336
 0.8560125363697645
 0.7105680278158648
 0.8083715555361315
 0.7723433354123344
 0.777999741614366
 0.7326089583961889
 0.8482604986575226
 0.7379515076326058
 0.6737844805658127
 ⋮
 0.8334370909741351
 0.766383145058206
 0.7492811642347643
 0.7908428858865811
 0.701450949250555
 0.47984694261253097
 0.7582784672736821
 0.6235002135627385
 0.78555946693907</code></pre><p>Now we generate some &quot;real&quot; data. Since we know <span>$\pi$</span> with high precision, we use that to generate the data, which are Bernoulli trials with probability <span>$\pi/4$</span>, and check the mean, which should be close to p_true.</p><pre><code class="language-julia hljs">p_true = π/4</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.7853981633974483</code></pre><pre><code class="language-julia hljs">data = rand(Bernoulli(p_true), N)

mean(data)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.7903</code></pre><p>We update our prior in closed form, since the prior is a Beta distribution, which is a conjugate prior to the Bernoulli distribution. It is updated by simply counting the number of sucesses and the number of failures to the shape parameters <span>$\alpha$</span> and <span>$\beta$</span>, respectively.</p><pre><code class="language-julia hljs">function update_belief(prior_distribution::Beta, data::AbstractArray{Bool})
    # Count the number of successes and failures.
    successes = sum(data)
    failures = length(data) - successes

    # Update our prior belief using the fact that Beta is conjugate distribution.
    return Beta(prior_distribution.α + successes, prior_distribution.β + failures)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">update_belief (generic function with 1 method)</code></pre><p>In order to see the evolution of the posterior with respect to the added evidence, we first we update the prior with a small part of the data</p><pre><code class="language-julia hljs">Ns = div.(N, (1000, 100, 10, 1))
posterior_distributions = Dict(n =&gt; update_belief(prior_distribution, view(data, 1:n)) for n in Ns)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Dict{Int64, Distributions.Beta{Float64}} with 4 entries:
  10000 =&gt; Distributions.Beta{Float64}(α=7927.0, β=2105.0)
  10    =&gt; Distributions.Beta{Float64}(α=33.0, β=9.0)
  1000  =&gt; Distributions.Beta{Float64}(α=826.0, β=206.0)
  100   =&gt; Distributions.Beta{Float64}(α=104.0, β=28.0)</code></pre><p>Now we visualize the posterior, which is the updated distribution.</p><pre><code class="language-julia hljs">plt = plot(title=&quot;Density of the posterior distributions Beta(α&#39;, β&#39;)&quot;, titlefont=10, legend=:topleft, xlims=(0.5, 1.0))
for n in Ns
    distr = posterior_distributions[n]
    plot!(plt, pp, pdf.(distr, pp), label=&quot;N=$n; α&#39;=$(distr.α), β&#39;=$(distr.β), mean=$(round(mean(distr), sigdigits=4))&quot;, fill=true, alpha=0.5)
end
vline!(plt, [π/4], label=nothing, color=:black)</code></pre><img src="3913e482.svg" alt="Example block output"/><h2 id="Performance-of-the-two-methods"><a class="docs-heading-anchor" href="#Performance-of-the-two-methods">Performance of the two methods</a><a id="Performance-of-the-two-methods-1"></a><a class="docs-heading-anchor-permalink" href="#Performance-of-the-two-methods" title="Permalink"></a></h2><p>It is not so relevant to compare the performance of the two methods above since this is a toy problem and not representative of the variety of situations that can be handled by either frequentist or Bayesian approaches. Drawing pseudo random numbers is pretty cheap and the frequentist approach above is much faster, even with the code not optimized for performance. The use of the Bayesian approach is more relevant in more complex cases and where sampling is more expensive. But more important than that is the perspective of the Bayesian approach of treating parameters as random variables and of their distributions as uncertainties, or quantifiers of our belief on their values.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../bayesian_probprog/">« Bayesian probabilistic programming</a><a class="docs-footer-nextpage" href="../linear_regression/">Many Ways to Linear Regression »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.13.0 on <span class="colophon-date" title="Thursday 26 June 2025 15:34">Thursday 26 June 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
