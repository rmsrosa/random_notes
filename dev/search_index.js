var documenterSearchIndex = {"docs":
[{"location":"probability/hmc/#Hamiltonian-Monte-Carlo-(HMC)","page":"Hamiltonian Monte Carlo (HMC)","title":"Hamiltonian Monte Carlo (HMC)","text":"","category":"section"},{"location":"sensitivity/overview/#Sensitivity-analysis","page":"Overview","title":"Sensitivity analysis","text":"","category":"section"},{"location":"probability/gibbs/#Gibbs-sampling","page":"Gibbs sampling","title":"Gibbs sampling","text":"","category":"section"},{"location":"probability/bayes/#Bayes-theorem-and-applications","page":"Bayes Theorem","title":"Bayes theorem and applications","text":"","category":"section"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"In this section, we state Bayes' theorem and discuss some of its applications.","category":"page"},{"location":"probability/bayes/#Bayes'-Theorem","page":"Bayes Theorem","title":"Bayes' Theorem","text":"","category":"section"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Bayes' Theorem concerns the probability of a given event, conditioned to another event, and can be stated as follows.","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"note: Bayes' Theorem\nLet p be a probability distribution on a given sample space and let A and B be two events with p(B)  0. Then    p(AB) = fracp(BA)p(A)p(B)","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"In other words, Bayes' Theorem says that the posterior conditional probability p(AB) of an event A, given the occurrence of another event B, equals the likelihood p(BA) of the second event B given the first event A times the prior p(A) divided by the marginal p(B). The prior here refers to the probability p(A) of A before observing the event B, while the posterior p(AB) refers to the probability of A after the observation of the event B.","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Bayes' theorem has many useful consequences (see e.g. ), but first let us sketch its proof.","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Proof of Bayes's Theorem.When P(A) = 0, then P(AB) = 0 and the result is trivial. When p(A)  0, the result can be obtained from the conditional probability relations    p(AB) = fracp(Acap B)p(B) qquad p(BA) = fracp(Bcap A)p(A)which imply    p(AB)p(B) = p(Acap B) = p(Bcap A) = p(BA)p(A)Solving for p(AB) yields the desired result. â– ","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Very often, we are not given p(B) directly, but we can use the law of total probability to find p(B), according to a decomposition of the sample space Omega, such as Omega = A cup neg A, where neg A = Omega setminus A denotes the event complementary to A. This law has two forms, one in terms of joint probabilities and one in terms of conditional probabilities:","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    beginalign*\n        p(B)  = p(Bcap A) + p(B cap neg A) \n             = p(BA)p(A) + p(Bneg A)p(neg A)\n    endalign*","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"This law also applies to decompositions of the sample space in terms of several disjoint events, i.e. Omega = cup_i A_i, with disjoint p(A_i cap A_j) = 0, for ineq j.","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Using this decomposition, we can write the Bayes' formula as","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"note: Extended version of Bayes' formula\nLet p be a probability distribution on a given sample space and let A and B be two events with p(B)  0. Then    p(AB) = fracp(BA)p(A)p(BA)p(A) + p(Bneg A)p(neg A)","category":"page"},{"location":"probability/bayes/#Screening-test","page":"Bayes Theorem","title":"Screening test","text":"","category":"section"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"There are many applications of Bayes' Theorem in Biomedicine. Let's say, for example, a certain test for a given endemic disease (or illegal drug use, etc.) has a 4% chance of false negative and 0.1% chance of false positive, and suppose that the disease occurs in 1% of the population.","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"If a certain person tests positive, what are their chances of really carrying the disease? This means we want to know the conditional probability of having the disease, given that it tested positive. Let's use the following notation for the relevant events:","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"D denotes the event of having the disease;\nneg D denotes the event of not having the disease;\nP denotes the event of testing positive;\nN denotes the event of testing negative.","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"The chances of a person who tested positive to have the disease can be expressed as the conditional probability p(DP). Using Bayes' theorem, this can be expressed as","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    p(DP) = fracp(PD)p(D)p(P)","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"According to the given information, the probability p(PD) of testing positive while having the disease is 96%, since the false negatives p(ND) amount to 4%. The probability p(D) of having the disease among the general population is 1%. Finally, the probability of testing positive can be obtained from the law of total probability:","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    p(P) = p(PD)p(D) + p(Pneg D)p(neg D) = 96 times 1 + 01 times 99 = 1059","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Thus, according to Bayes' Theorem,","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    p(DP) = fracp(PD)p(D)p(P) = frac96 times 11059 approx 906","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Hence, the chances a person who tested positive has indeed this disease are of about 90%, which is reasonably high.","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"If, however, the false negatives were of the order of 5% and the false positives were of the order of 1%, then the chances p(DP) of a person who tested positive to indeed have the disease would be only of the order of 49%! Pretty low, right? Not quite reliable. PSA tests are one example where this conditional probability is low, of the order of 25%.","category":"page"},{"location":"probability/bayes/#Bayesian-inference-on-defect-item","page":"Bayes Theorem","title":"Bayesian inference on defect item","text":"","category":"section"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Suppose we have a collection of four six-faced dice, with two normal ones with faces numbered one to six, but one with two faces numbered five and none numbered six and one with three faces numbered four and none numbered five nor six. Let's call them dice types D_6, D_5 and D_4, respectively. A friend picks one of the dice at random and throws it repeatedly to find the numbers 3, 1, 4, 5, 1, 5, 2, 5, reading them aloud. What is the most likely type of die your friend picked?","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Your prior is that a normal die is selected with probability 1/2,","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    p(D_6) = frac12","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"while the other two, with probability 1/4:","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    p(D_5) = p(D_4) = frac14","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Now, after learning about the evidence E = (3 1 4 5 1 5 2 5) of the numbers thrown by your friend, you update your prior with this evidence to find the posteriors","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    p(D_i  E) = fracp(ED_i)p(D_i)p(E)","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"For each die, the likelyhood p(ED_i) is","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    p(ED_i) = p(3D_i)p(1D_i)p(4D_i)p(5D_i)p(1D_i)p(5D_i)p(2D_i)p(5D_i)","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Since p(5D_4) = 0, p(5D_5) = 13 and all remaining odds are 16, we find","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    beginalign*\n        p(ED_4)  = 0 \n        p(ED_5)  = left(frac16right)^5left(frac13right)^3 = 8 left(frac16right)^8 \n        p(ED_6)  = left(frac16right)^8\n    endalign*","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"The probability of seeing this evidence is given by the law of total probability","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    p(E) = p(ED_4)p(D_4) + p(ED_5)p(D_5) + p(ED_6)p(D_6) = 0 times frac14 + 8 left(frac16right)^8frac14 + left(frac16right)^8frac12","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Hence,","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    p(E) = frac52 left(frac16right)^8","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Therefore, the posteriors are","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    beginalign*\n        p(D_4E)  = 0 \n        p(D_5E)  = frac8 times 1452 = 45 \n        p(D_6E)  = frac1 times 1252 = 15\n    endalign*","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Therefore, it is four times more likely that your friend picked the defective D_5-type die than the normal die, and, of course, the D_4-type die was surely not picked.","category":"page"},{"location":"probability/bayes/#Monty-Hall-problem","page":"Bayes Theorem","title":"Monty Hall problem","text":"","category":"section"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"The Monty Hall problem is a classic probability puzzle. In a television show, a contender has to choose between three doors, with only one of them giving you a reward. You choose one at random and you have 1/3 chance of choosing the right one. But after you choose this one, the host of the show reveals one of the doors which do not have any reward and asks if you want to choose a different door or keep the same. It turns out that if you switch to the remaining door, your chances rise to 2/3.","category":"page"},{"location":"probability/bayes/#Solution-via-probability-tree","page":"Bayes Theorem","title":"Solution via probability tree","text":"","category":"section"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"At first, you have 1/3 chance of choosing the right one and 2/3 chances of choosing a door without the reward. If you choose the right door and changes it after the host reveals an empty door, then you necessarily change to an empty door. This with a 1/3 chance. If you choose a door without reward and changes it after the host reveals an empty door, then you necessarily change it to the right door. This with a 2/3 chance. Hence, you have a 2/3 chance of success!","category":"page"},{"location":"probability/bayes/#Solving-it-via-the-law-of-total-probability","page":"Bayes Theorem","title":"Solving it via the law of total probability","text":"","category":"section"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Let us do this more formally. Suppose R denotes the door with the reward. Let X be the random variable denoting the player's choice. With a single choice, p(X=R) = 13.","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Now suppose we make two choices, denoted by the random variables X_1 and X_2. In the first strategy, that the player doesn't change his choice, we have X_2 = X_1. In this case, we work with a probability conditioned to X_2 = X_1, and we simplify the notation to p_1(E) = p(EX_2 = X_1), for any possible event E. Then, by the law of total probability,","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    p_1(X_2 = R) = p_1(X_2 = RX_1 = R)p_1(X_1 = R) + p_1(X_2 = RX_1 neq R)p_1(X_1 neq R)","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"If the player doesn't change his choice, then p_1(X_2 = RX_1 = R) = 1 and p_1(X_1 = R) = 13, while p_1(X_2 = RX_1 neq R) = 0, so that p(X_2 = RX_2 = X_1) = p_1(X_2 = R) = 13.","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Now, if the player changes his choice, then we work with the probability p_2(E) = p(EX_2 neq X_1). In this case, p_2(X_2 = RX_1 = R) = 0, while p_2(X_2 = RX_1 neq R) = 1 and p_2(X_1 neq R) = 23, so that p_2(X_2 = R) = 23.","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"In this derivation, we did not make explicit the dependence on the choice of the host. We leave this as an exercise.","category":"page"},{"location":"probability/bayes/#Solving-it-via-Bayes'-rule","page":"Bayes Theorem","title":"Solving it via Bayes' rule","text":"","category":"section"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Suppose now you first pick door X_1, then the host picks door H, and next you choose door X_2. This means we have a random vector (X_1 H X_2) in a sample space with cardinality 27, meaning each choice can be any of the three doors. We are interested in the chances that X_2 is the door with the car, given that all chosen doors are different and that the host does not choose the door with the car. This corresponds to the strategy that the player changes the door. This can be written as the following conditional probability, where R denotes the \"right\" door, with the car:","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    p(X_2 = R  X_2 notin X_1 H H neq R H neq X_1)","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Well, the condition of the show is that the host picks a different door than the first one chosen by the player, that that door is not the right one, and that the second door picked by the player is not the door chosen by the host. Under this rule, the player is free to stick we the first door, i.e. X_2 = X_1, or choose a different one, i.e. X_2 neq X_1. In order to simplify the notation, we write the probability conditioned to the rules of the game as","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    tilde p(E) = p(E  X_2 neq H H neq X_1 H neq R)","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"for any possible event E. Using Bayes' rule,","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    tilde p(X_2 = R  X_2 neq X_1) = fractilde p(X_2 neq X_1  X_2 = R)tilde p(X_2 = R)tilde p(X_2 neq X_1)","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Under the rules of the game,","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    beginalign*\n        tilde p(X_2 neq X_1  X_2 = R)  = 23 \n        tilde p(X_2 = R)  = 12 \n        tilde p(X_2 neq X_1)  = 12\n    endalign*","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Hence,","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    tilde p(X_2 = R  X_2 neq X_1) = frac23 times 1212 = frac23","category":"page"},{"location":"probability/bayes/#Solving-it-via-Bayes'-rule-by-updating-the-prior","page":"Bayes Theorem","title":"Solving it via Bayes' rule by updating the prior","text":"","category":"section"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"We can also use the classic interpretation of Bayes' rule as updating a prior distribution describing your chances of winning, according to new evidence revealed by the host. We'll do this in two ways, the first one, as it is usually presented, of updating the prior of having chosen correctly the first door X_1, and in a more natural way, of updating the prior of choosing correctly the last door opened X_2.","category":"page"},{"location":"probability/bayes/#Updating-the-prior-for-X_1-R","page":"Bayes Theorem","title":"Updating the prior for X_1 = R","text":"","category":"section"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"In this case, the player has, initially, one-third chance of choosing the right door:","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    p(X_1 = R) = frac13","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"This is the player's prior. After that, the host reveals a door, showing it does not have a car, which we state as H neq R. Now we want to update our prior to have a posterior probability","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    p(X_1 = R  H neq R)","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"According to Bayes' rule,","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    p(X_1 = R  H neq R) = fracp(H neq R  X_1 = R) p(X_1 = R)p(H neq R)","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Well, the host always picks the door without the car, so both p(H neq R  X_1 = R) and p(H neq R) are equal to 1. Meanwhile, your prior is p(X_1 = R) = 13. Thus,","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    p(X_1 = R  H neq R) = frac1 times 131 = frac13","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"This means your chances of having chosen the right door at first do not change after the evidence. On the other hand, after the host reveals their choice of door, you only have two alternatives: stick with X_1 or change to the only remaining door. This means","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    p(X_1 = R  H neq R) + p(X_1 neq R  H neq R) = 1","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"i.e.","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    p(X_1 neq R  H neq R) = 1 - frac13 = frac23","category":"page"},{"location":"probability/bayes/#Updating-the-prior-for-X_2-R.","page":"Bayes Theorem","title":"Updating the prior for X_2 = R.","text":"","category":"section"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"A different way of solving this is to update directly the prior probability of selecting the right door X_2 = R, when switching your choice. Without the host opening a door, your chances are still one-third. This conditioned to the rules of the game, H neq X_1, H neq R, and of the strategy X_2 neq X_1. In this case, despite the fact that the host chooses the door without the car, the prior does not use this knowledge and the player does not know which door was chosen by the host and, at first, can choose either one of the remaining two, with still a one-third chance of getting it right:","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    p(X_2 = R  X_2 neq X_1 H neq X_1 H neq R) = frac13","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"As before, in order to simplify the notation, we consider the probability conditioned to the rules of the game and the strategy but without including yet X_2 neq H since we will start with a prior that lacks this knowledge. Hence, we consider the conditional probability","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    tilde p(E) = p(E  X_2 neq X_1 H neq X_1 H neq R)","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Thus, the prior reads","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    tilde p(X_2 = R) = frac13","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"Now, once the door H is revealed, you choose X_2 neq H upon this new evidence. Hence, by Bayes' theorem,","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    tilde p(X_2 = R  X_2 neq H) = fractilde p(X_2 neq H  X_2 = R) tilde p(X_2 = R)tilde p(X_2 neq H)","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"The likelihood tilde p(X_2 neq H  X_2 = R) = 1 because surely X_2neq H when conditioned to X_2 = R and H neq R. The marginal tilde p(X_2 neq H) = 12 because when conditioned to X_2 neq X_1 and H neq X_1, there are two doors left for X_2, one equals to H and the other different than H, hence one out of two possibilities of X_2 neq H. Therefore,","category":"page"},{"location":"probability/bayes/","page":"Bayes Theorem","title":"Bayes Theorem","text":"    tilde p(X_2 = R  X_2 neq H) = frac1 times 1312 = frac23","category":"page"},{"location":"probability/bernstein_vonmises/#Bernsteinâ€“von-Mises-theorem","page":"Bernsteinâ€“von Mises theorem","title":"Bernsteinâ€“von Mises theorem","text":"","category":"section"},{"location":"probability/linear_regression/#Linear-Regression-in-several-ways","page":"Many Ways to Linear Regression","title":"Linear Regression in several ways","text":"","category":"section"},{"location":"probability/linear_regression/","page":"Many Ways to Linear Regression","title":"Many Ways to Linear Regression","text":"The plan is to do a simple linear regression in julia, in several different ways. We'll use plain least squares Base.:\\, the genereral linear model package JuliaStats/GLM.jl and the probabilistic programming package Turing.jl.","category":"page"},{"location":"probability/linear_regression/","page":"Many Ways to Linear Regression","title":"Many Ways to Linear Regression","text":"using Distributions, GLM, Turing, StatsPlots","category":"page"},{"location":"probability/linear_regression/#The-test-data","page":"Many Ways to Linear Regression","title":"The test data","text":"","category":"section"},{"location":"probability/linear_regression/","page":"Many Ways to Linear Regression","title":"Many Ways to Linear Regression","text":"This is a simple test set. We just generate a synthetic sample with a bunch of points approximating a straight line. We actually create two tests, an unperturbed straight line and a perturbed one.","category":"page"},{"location":"probability/linear_regression/","page":"Many Ways to Linear Regression","title":"Many Ways to Linear Regression","text":"num_points = 20\nxx = range(0.0, 1.0, length=num_points)\n\nintercept = 1.0\nslope = 2.0\nÎµ = 0.1\n\nyy = intercept .+ slope * xx\n\nyy_perturbed = yy .+ Îµ * randn(num_points)\n\nplt = plot(title=\"Synthetic data\", titlefont=10, ylims=(0.0, 1.1 * (intercept + slope)))\nscatter!(plt, xx, yy, label=\"unperturbed sample\")\nscatter!(plt, xx, yy_perturbed, label=\"perturbed sample\")","category":"page"},{"location":"probability/linear_regression/#Straightforward-least-squares","page":"Many Ways to Linear Regression","title":"Straightforward least squares","text":"","category":"section"},{"location":"probability/linear_regression/","page":"Many Ways to Linear Regression","title":"Many Ways to Linear Regression","text":"The least square solution hat x to a linear problem Ax = b is the vector hat x that minimizes the sum of the squares of the residuals b - Ax, i.e.","category":"page"},{"location":"probability/linear_regression/","page":"Many Ways to Linear Regression","title":"Many Ways to Linear Regression","text":"    hat x = argmin_x Ax - b^2","category":"page"},{"location":"probability/linear_regression/","page":"Many Ways to Linear Regression","title":"Many Ways to Linear Regression","text":"The solution is obtained by solving the normal equation","category":"page"},{"location":"probability/linear_regression/","page":"Many Ways to Linear Regression","title":"Many Ways to Linear Regression","text":"    (A^t A)hat x = A^t b","category":"page"},{"location":"probability/linear_regression/","page":"Many Ways to Linear Regression","title":"Many Ways to Linear Regression","text":"In julia, hat x can be found by using the function Base.:\\, which is actually a polyalgorithm, meaning it uses different algorithms depending on the shape and type of A. ","category":"page"},{"location":"probability/linear_regression/","page":"Many Ways to Linear Regression","title":"Many Ways to Linear Regression","text":"When A is an invertible square matrix, then hat x = A^-1b is the unique solution of Ax = b.\nWhen A has more rows than columns and the columns are linearly independent, then hat x = (A^tA)^-1A^tb is the unique least square approximation solution of the overdetermined system Ax = b.\nWhen A has more columns than rows and the rows are linearly independent, then hat x = A^t(AA^t)^-1b is the unique least norm solution of the underdetermined system Ax = b.\nIn all other cases, attempting to solve Ab throws an error.","category":"page"},{"location":"probability/linear_regression/","page":"Many Ways to Linear Regression","title":"Many Ways to Linear Regression","text":"First we build the Vandermonde matrix.","category":"page"},{"location":"probability/linear_regression/","page":"Many Ways to Linear Regression","title":"Many Ways to Linear Regression","text":"A = [ones(length(xx)) xx]\nsize(A)","category":"page"},{"location":"probability/linear_regression/","page":"Many Ways to Linear Regression","title":"Many Ways to Linear Regression","text":"Now we solve the least square problem with the unperturbed data, solving it explicitly with hat x = (A^tA)^-1A^tb and via A setminus b, and checking both against the original slope and intercept.","category":"page"},{"location":"probability/linear_regression/","page":"Many Ways to Linear Regression","title":"Many Ways to Linear Regression","text":"betahat = inv(transpose(A) * A) * transpose(A) * yy\n\nbetahat â‰ˆ A \\ yy â‰ˆ [intercept, slope]","category":"page"},{"location":"probability/linear_regression/","page":"Many Ways to Linear Regression","title":"Many Ways to Linear Regression","text":"Now we solve it with the perturbed data","category":"page"},{"location":"probability/linear_regression/","page":"Many Ways to Linear Regression","title":"Many Ways to Linear Regression","text":"betahat = inv(transpose(A) * A) * transpose(A) * yy_perturbed\n\nbetahat â‰ˆ A \\ yy_perturbed","category":"page"},{"location":"probability/linear_regression/","page":"Many Ways to Linear Regression","title":"Many Ways to Linear Regression","text":"intercepthat, slopehat = betahat","category":"page"},{"location":"probability/linear_regression/","page":"Many Ways to Linear Regression","title":"Many Ways to Linear Regression","text":"yy_hat = intercepthat .+ slopehat * xx ","category":"page"},{"location":"probability/linear_regression/","page":"Many Ways to Linear Regression","title":"Many Ways to Linear Regression","text":"plt = plot(title=\"Synthetic data and least square fit\", titlefont=10, ylims=(0.0, 1.1 * (intercept + slope)))\nscatter!(plt, xx, yy, label=\"unperturbed sample\")\nscatter!(plt, xx, yy_perturbed, label=\"perturbed sample\")\nplot!(plt, xx, yy, label=\"unperturbed line\")\nplot!(plt, xx, yy_hat, label=\"fitted line\")","category":"page"},{"location":"probability/linear_regression/#Bayesian-linear-regression-with-Turing.jl","page":"Many Ways to Linear Regression","title":"Bayesian linear regression with Turing.jl","text":"","category":"section"},{"location":"probability/linear_regression/","page":"Many Ways to Linear Regression","title":"Many Ways to Linear Regression","text":"@model function regfit(x, y)\n    ÏƒÂ² ~ InverseGamma()\n    Ïƒ = sqrt(ÏƒÂ²)\n    c ~ Normal(0.0, 10.0)\n    m ~ Normal(0.0, 10.0)\n\n    for i in eachindex(x)\n        v = c + m * x[i]\n        y[i] ~ Normal(v, Ïƒ)\n    end\nend","category":"page"},{"location":"probability/linear_regression/","page":"Many Ways to Linear Regression","title":"Many Ways to Linear Regression","text":"n = 20\nc = 2.0\nm = 1.5\nx = (1:n) ./ n\ny = c .+ m * (x .+ 0.02 * randn(n))\nscatter(x, y, legend=:topleft)","category":"page"},{"location":"probability/linear_regression/","page":"Many Ways to Linear Regression","title":"Many Ways to Linear Regression","text":"Let's use the Hamiltonian Monte Carlo method to infer the parameters of the model.","category":"page"},{"location":"probability/linear_regression/","page":"Many Ways to Linear Regression","title":"Many Ways to Linear Regression","text":"model = regfit(x, y)\n\nchain = sample(model, HMC(0.05, 10), 4_000)\n\nplot(chain)","category":"page"},{"location":"probability/mortality_tables/#Modeling-mortality-tables","page":"Modeling mortality tables","title":"Modeling mortality tables","text":"","category":"section"},{"location":"probability/mortality_tables/","page":"Modeling mortality tables","title":"Modeling mortality tables","text":"using Distributions, Turing, StatsPlots","category":"page"},{"location":"probability/mortality_tables/","page":"Modeling mortality tables","title":"Modeling mortality tables","text":"How come they have F sim Normal(mu_F sigma_F^2) since they will take the log of it? Can't let negative values in. I changed it to a Beta distribution.","category":"page"},{"location":"probability/mortality_tables/","page":"Modeling mortality tables","title":"Modeling mortality tables","text":"@model function heligman_pollard(x, q)\n    A ~ Beta()\n    B ~ Beta()\n    C ~ Beta()\n    D ~ Beta()\n    E ~ Gamma()\n    F ~ Gamma()\n    G ~ Beta()\n    H ~ Gamma()\n    ÏƒÂ² ~ InverseGamma()\n    Ïƒ = sqrt(ÏƒÂ²)\n\n    for i in eachindex(x)\n        Î· = A ^ ((x[i] + B) ^ C) + D * exp( - E * (log(x[i]) - log(F)) ^ 2) + G * H ^ (x[i]) # / (1 + K * G * H ^ (x[i]) )\n\n        y = Î· / (1 + Î·) \n        q[i] ~ Normal(y, Ïƒ)\n    end\nend","category":"page"},{"location":"probability/mortality_tables/","page":"Modeling mortality tables","title":"Modeling mortality tables","text":"@model function gompertz_makeham(x, q)\n    A ~ Beta()\n    B ~ Beta()\n    C ~ Beta()\n    ÏƒÂ² ~ InverseGamma()\n    Ïƒ = sqrt(ÏƒÂ²)\n\n    for i in eachindex(x)\n        Î· = A + B * C ^ (x[i])\n\n        y = Î· / (1 + Î·) \n        q[i] ~ Normal(y, Ïƒ)\n    end\nend","category":"page"},{"location":"probability/mortality_tables/","page":"Modeling mortality tables","title":"Modeling mortality tables","text":"x = collect(0:100)\nq = [\n    0.003522\n    0.000213\n    0.000114\n    0.000093\n    0.000066\n    0.000077\n    0.000068\n    0.000053\n    0.000053\n    0.000055\n    0.000066\n    0.000054\n    0.000054\n    0.000087\n    0.000093\n    0.000107\n    0.000125\n    0.000154\n    0.000207\n    0.000208\n    0.000182\n    0.000198\n    0.000234\n    0.000197\n    0.000213\n    0.000253\n    0.000256\n    0.000294\n    0.000300\n    0.000316\n    0.000377\n    0.000376\n    0.000439\n    0.000478\n    0.000559\n    0.000569\n    0.000636\n    0.000731\n    0.000777\n    0.000804\n    0.000860\n    0.000946\n    0.001064\n    0.001166\n    0.001306\n    0.001435\n    0.001557\n    0.001683\n    0.001915\n    0.002002\n    0.002154\n    0.002394\n    0.002514\n    0.002691\n    0.002858\n    0.003197\n    0.003542\n    0.003823\n    0.004260\n    0.004538\n    0.005112\n    0.005484\n    0.006272\n    0.006691\n    0.007167\n    0.007879\n    0.008552\n    0.009269\n    0.010364\n    0.011147\n    0.012587\n    0.013296\n    0.015106\n    0.016883\n    0.019224\n    0.021169\n    0.023871\n    0.027332\n    0.030672\n    0.035113\n    0.038899\n    0.044112\n    0.049270\n    0.056318\n    0.064188\n    0.072903\n    0.083274\n    0.094737\n    0.106725\n    0.120461\n    0.135122\n    0.152146\n    0.169843\n    0.188654\n    0.206166\n    0.229104\n    0.252690\n    0.276870\n    0.299265\n    0.320210\n    0.3499\n]\nscatter(x, q, yscale=:log10, legend=:topleft)","category":"page"},{"location":"probability/mortality_tables/","page":"Modeling mortality tables","title":"Modeling mortality tables","text":"model = heligman_pollard(x, q)\nmodel = gompertz_makeham(x, q)","category":"page"},{"location":"probability/mortality_tables/","page":"Modeling mortality tables","title":"Modeling mortality tables","text":"chain = sample(model, HMC(0.05, 10), 10_000)\n# chain = sample(model, NUTS(0.65), 20_000)","category":"page"},{"location":"probability/mortality_tables/","page":"Modeling mortality tables","title":"Modeling mortality tables","text":"plot(chain)","category":"page"},{"location":"probability/mortality_tables/","page":"Modeling mortality tables","title":"Modeling mortality tables","text":"# mean(chain.A), mean(chain.B), mean(chain.C)","category":"page"},{"location":"probability/mcmc/#Markov-Chain-Monte-Carlo-(MCMC)","page":"Markov Chain Monte Carlo (MCMC)","title":"Markov Chain Monte Carlo (MCMC)","text":"","category":"section"},{"location":"probability/bayes_inference/#Bayesian-inference","page":"Bayesian inference","title":"Bayesian inference","text":"","category":"section"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"In many situations, we expect some random variable to follow a given distribution but it is not certain what parameters actually define the distribution. For instance, we may have a coin that might be biased but we are unsure about how biased it is. Or we may expect some feature of a population to follow a normal distribution but it is not clear what are its mean and/or standard deviation. In those cases, it is useful to treat those parameters as random variables themselves, leading to what is known as a compound distribution.","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"Then, given a certain feature and a model, we may attempt to fit the model to the available data, which is refereed to a statistical inference problem. We are particularly interested here in Bayesian inference, which amounts to using Bayes' formula in the inference process, by updating a prior knowledge of the distribution according to the evidence given by the data.","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"In loose terms, suppose a compound distribution model has a parameter theta, and we initially believe in a certain prior distribution p(theta) for this parameter. We then observe some data, or evidence, E, and update our belief according to Bayes' formula,","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"    p(theta  E) = fracp(E  theta) p(theta)p(E)","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"After updating, the posterior p(theta  E) may indicate better the most likely values for the parameter theta.","category":"page"},{"location":"probability/bayes_inference/#Bayesian-inference-on-defect-item","page":"Bayesian inference","title":"Bayesian inference on defect item","text":"","category":"section"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"The Bayesian inference on defect item is an example of Bayesian inference. One wants to infer which die was picked by their friend, based on the results of throwing the die a few times. In this case, we have a compound distributions, starting with a categorial distribution with the three categories D_4, D_5, D_6, representing each type of die, with probabilities 14, 14, and 12, respectively, and then, for each category D_i, we have the probabilities p(jD_i) of obtaining each number j=1 ldots 6, with each die type D_i, i = 4 5 6.","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"After a number of throws resulting in a sequence E = (3 1 4 5 1 5 2 5) of events, we want to know the posterior p(D_iE), revealing the most likely die picked in the beginning of the problem.","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"The following animation illustrates the evolution of our belief on the picked die as the die is thrown, along the sequence of events E.","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"using Distributions, StatsPlots\nusing Random # hide\nRandom.seed!(123) # set the seed for reproducibility # hide\n\nE = (3, 1, 4, 5, 1, 5, 2, 5)\n\n# prior p(i) = p(D_i)\nfunction p(i::Int)\n    p = 0/4\n    (i == 4 || i == 5 ) && (p = 1/4)\n    i == 6 && (p = 2/4)\n    return p\nend\n\n# conditional probabilities p(n, i) = p(n | D_i) of each number n for a given die D_i\nfunction p(n::Int, i::Int)\n    q = 0/6\n    i == 4 && (q = 1 â‰¤ n â‰¤ 3 ? 1/6 : n == 4 ? 3/6 : 0/6)\n    i == 5 && (q = 1 â‰¤ n â‰¤ 4 ? 1/6 : n == 5 ? 2/6 : 0/6)\n    i == 6 && 1 â‰¤ n â‰¤ 6 && (q = 1/6)\n    return q\nend\n\n# conditional probabilities p(E, i) = p(E | D_i) of sequence E of events for a given die D_i\nfunction p(E::NTuple{N, Int}, i::Int) where N\n    q = prod(p(e, i) for e in E)\n    return q\nend\n\n# marginal probability of event p(E) = sum_i p(E | D_i)p(D_i)\nfunction p(E::NTuple{N, Int}) where N\n    q = sum(p(E, i) * p(i) for i in 4:6)\n    return q\nend\n\nprior = [1/4, 1/4, 1/2]\n\nposteriors = [p(E[1:n], i) * p(i) / p(E[1:n]) for i in 4:6, n in 1:length(E)]","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"anim = @animate for n in 0:length(E)\n    if n == 0\n        bar(\n            4:6,\n            prior,\n            size=(600, 400),\n            title=\"Updated belief on each die after $n observations\",\n            titlefont = 10,\n            xlabel=\"die type\",\n            xticks=4:6,\n            ylabel=\"probability\",\n            legend=nothing,\n            ylim=(0.0, 1.0)\n        )\n    else\n        bar(\n            4:6,\n            posteriors[:, n],\n            size=(600, 400),\n            title=\"Updated belief on each die after $n observations\",\n            titlefont = 10,\n            xlabel=\"die type\",\n            xticks=4:6,\n            ylabel=\"probability\",\n            legend=nothing,\n            ylim=(0.0, 1.0)\n        )\n    end\nend","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"gif(anim, fps = 0.5) # hide","category":"page"},{"location":"probability/bayes_inference/#A-biased-coin","page":"Bayesian inference","title":"A biased coin","text":"","category":"section"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"The previous example illustrates the case of a finite number of choices for the parameter, namely D_5, D_5, D_6. More often than not, the parameter belongs to a continuum, in either a finite- or infinite-dimensional space.","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"Indeed, let's suppose we are told a coin is biased but we don't know its bias and don't even know whether it is biased towards heads or tails. We let X be the random variable with the result of tossing the coin, which follows a Bernoulli distribution with say probability theta of assuming the value 1, representing heads, and 1-theta of assuming the value 0, representing tails. Thus,","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"    X sim mathrmBernoulli(theta)","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"The bias theta may assume any value between 0 and 1, so we consider it as a random variable denoted Theta. We could assume an uninformative prior, with Theta uniformly distributed between 0 and 1, or an informative prior, assuming, more likely, just a slight bias, near 1/2. In the first case, we take Theta sim mathrmBeta(1 1) = mathrmUniform(0 1), while in the second case, we may assume Theta to be distributed like mathrmBeta(n m), with n m gg 1. In any case, we suppose","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"    Theta sim mathrmBeta(alpha beta)","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"for alpha beta  0. Recall the probability distribution function for Theta is","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"    f_Theta(theta) = f_Theta(theta alpha beta) = fracGamma(alpha + beta)Gamma(alpha)Gamma(beta)theta^alpha - 1(1 - theta)^beta - 1","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"where Gamma = Gamma(z) is the gamma function.","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"Now, suppose we toss the coin a number of times and it lands heads k times and tails m times. So this is our evidence E. The random variable X is discrete, while Theta is continuous. The posterior becomes","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"    f_Theta(theta  E) = fracp(E  theta) f_Theta(theta)p(E)","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"Since E is heads k times and tails m times, we have p(E  theta) propto theta^k (1 - theta)^m. Computing p(E) is usually not a trivial task but in this case can be computed via","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"    beginalign*\n        p(E)  = int_0^1 p(Etheta)f_Theta(theta)mathrmdtheta = left(beginmatrix k + m  k endmatrix right)fracGamma(alpha + beta)Gamma(alpha)Gamma(beta)int_0^1 theta^k (1 - theta)^m theta^alpha -1(1-theta)^beta - 1 mathrmdtheta \n         = left(beginmatrix k + m  k endmatrix right)fracGamma(alpha + beta)Gamma(alpha)Gamma(beta)fracGamma(alpha + k)Gamma(beta + m)Gamma(alpha + k + beta + m)\n    endalign*","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"But we do not need to compute it in this case. Indeed, up to a constant, we have","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"    p(theta  E) propto theta^k (1 - theta)^m theta^alpha - 1(1 - theta)^beta - 1 = theta^k + alpha - 1(1 - theta)^m + beta - 1 sim mathrmBeta(alpha + k beta + m)","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"Hence, updating the prior in this case simply amounts to adding the number of heads and the number of tails to the parameters of the beta distribution.","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"As more and more coins are tossed, we get the expected value of the posterior converging to the actual bias of the coin, with the credible interval narrowing down the uncertainty.","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"The code below (adapted from Introduction to Turing) exemplifies the increased belief we get with more and more data collected.","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"We first generate the data drawing from a Bernoulli distribution using the true bias.","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"using Distributions, StatsPlots\nusing Random # hide\nRandom.seed!(321) # set the seed for reproducibility # hide\n\np_true = 0.6\nN = 2_000\ndata = rand(Bernoulli(p_true), N)\nnothing # hide","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"Next we define an uniformative prior and create a function to update the prior with a given set of data.","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"prior = Beta(1, 1)\n\nfunction update_prior(prior::Beta, data::AbstractVector{Bool})\n    heads = sum(data)\n    tails = length(data) - heads\n\n    posterior = Beta(prior.Î± + heads, prior.Î² + tails)\n    return posterior\nend\nnothing # hide","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"Now we visualize the effect of the size of the data on the posterior.","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"plt = []\n\nfor n in (0, 10, 100, N)\n    push!(\n        plt,\n        plot(\n            update_prior(prior, view(data, 1:n)),\n            title = n == 0 ? \"Uninformative prior\" : \"Posterior with $n observations\",\n            titlefont = 10, # hide\n            legend=nothing, # hide\n            xlim=(0, 1), # hide\n            fill=0, # hide\n            Î±=0.3, # hide\n            w=3, # hide\n        )\n    )\nend\n\nplot(plt..., size=(700, 400)) # hide","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"To complement that, we show two animations with the posterior being updated as more data is used.","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"anim = @animate for n in 0:10\n    plot(\n        update_prior(prior, view(data, 1:n)),\n        size=(500, 250),\n        title=\"Updated belief after $n observations\",\n        xlabel=\"probability of heads\",\n        ylabel=\"\",\n        legend=nothing,\n        xlim=(0, 1),\n        fill=0,\n        Î±=0.3,\n        w=3,\n    )\n    vline!([p_true])\nend\n","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"gif(anim, fps = 1) # hide","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"anim = @animate for n in 0:10:N\n    plot(\n        update_prior(prior, view(data, 1:n)),\n        size=(500, 250),\n        title=\"Updated belief after $n observations\",\n        xlabel=\"probability of heads\",\n        ylabel=\"\",\n        legend=nothing,\n        xlim=(0, 1),\n        fill=0,\n        Î±=0.3,\n        w=3,\n    )\n    vline!([p_true])\nend","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"gif(anim, fps = 24) # hide","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"Here is a static view of the evolution of the posterior mean and 95% credible interval.","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"posterior_means = [mean(update_prior(prior, data[1:n])) for n in 0:N] # hide\nposterior_quantiles = [[quantile(update_prior(prior, data[1:n]), 0.025) for n in 0:N] [quantile(update_prior(prior, data[1:n]), 0.975) for n in 0:N]] # hide\n\nplot(title = \"Evolution of the posterior mean and credible interval\", titlefont=10, xaxis=\"number of observations\", yaxis=\"p\") # hide\nplot!(posterior_quantiles[:,1], fillrange=posterior_quantiles[:,2], alpha=0.0, fillalpha=0.3, label=\"95% credible interval\", color=1) # hide\nhline!([p_true], label=\"true p = $p_true\", color=2) # hide\nplot!(posterior_means, label=\"posterior mean\", color=1) # hide","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"For the sake of comparison, here is the evolution of the sample mean and the associated 95% confidence intervals, according to the frequentist approach.","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"sample_means = [mean(data[1:n]) for n in 2:N] # hide\nstd_error = [sqrt(var(data[1:n])/n) for n in 2:N] # hide\n\nplot(title = \"Evolution of the sample mean and confidence interval\", titlefont=10, xaxis=\"number of observations\", yaxis=\"p\") # hide\nplot!(sample_means, ribbon = 2*std_error, fillalpha=0.3, label=\"sample mean\") # hide\nhline!([p_true], label=\"true p = $p_true\") # hide","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"The data above has been seeded for reproducibility reasons. For the sake of illustration, here is another set of data, where the 95% intervals fall sometimes a little bit short.","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"Random.seed!(123) # set the seed for reproducibility # hide\ndata = rand(Bernoulli(p_true), N) # hide\n\nposterior_means = [mean(update_prior(prior, data[1:n])) for n in 0:N] # hide\nposterior_quantiles = [[quantile(update_prior(prior, data[1:n]), 0.025) for n in 0:N] [quantile(update_prior(prior, data[1:n]), 0.975) for n in 0:N]] # hide\n\nplot(title = \"Evolution of the posterior mean and credible interval\", titlefont=10, xaxis=\"number of observations\", yaxis=\"p\") # hide\nplot!(posterior_quantiles[:,1], fillrange=posterior_quantiles[:,2], alpha=0.0, fillalpha=0.3, label=\"95% credible interval\", color=1) # hide\nhline!([p_true], label=\"true p = $p_true\", color=2) # hide\nplot!(posterior_means, label=\"posterior mean\", color=1) # hide","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"sample_means = [mean(data[1:n]) for n in 2:N] # hide\nstd_error = [sqrt(var(data[1:n])/n) for n in 2:N] # hide\n\nplot(title = \"Evolution of the sample mean and confidence interval\", titlefont=10, xaxis=\"number of observations\", yaxis=\"p\") # hide\nplot!(sample_means, ribbon = 2*std_error, fillalpha=0.3, label=\"sample mean\") # hide\nhline!([p_true], label=\"true p = $p_true\") # hide","category":"page"},{"location":"probability/bayes_inference/#Conjugate-distributions","page":"Bayesian inference","title":"Conjugate distributions","text":"","category":"section"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"This property that multipling a Bernoulli distribution by a Beta prior yields a Beta posterior is an example of conjugate distributions. Conjugate distributions greatly facilitate the updating process in Bayesian statistics. There are a number of other conjugate prior distributions, as can be seen on this table of conjugate distributions.","category":"page"},{"location":"probability/bayes_inference/","page":"Bayesian inference","title":"Bayesian inference","text":"But, in general, without conjugate distributions, common in \"real-world\" problems, updating a prior is a much harder process and requires some fancy computational techniques.","category":"page"},{"location":"probability/probprog/#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"probability/probprog/","page":"Overview","title":"Overview","text":"It is tedious and many times impossible to do inference by hand. Fortunately, many steps can be automated and calculated with a computer, as in many other subjects. In the realm of probability, this is called probabilistic programming. We can use the computer, in any suitable language, to help us i) build Bayesian models, ii) draw samples from them, iii) do inference with them, and so on.","category":"page"},{"location":"probability/probprog/","page":"Overview","title":"Overview","text":"For teaching purposes, it is important to make the code as explicit and clear as possible. But on the field, in actual applications, a more general and extensible framework is useful. For that, there exist a number of packages for probabilistic programming in several languages. In Julia, you can easily access classical tools and packages such as STAN, via Stan.jl, and pyMC, via JuliaPy/PyCall.jl or cjdoris/PythonCall.jl.","category":"page"},{"location":"probability/probprog/","page":"Overview","title":"Overview","text":"But if you want to leverage all the power of Julia, a proper julia-native probabilistic programming package is the best choice, such as Turing.jl, Soss.jl, Gen.jl, and others. See the thread Current state of Julia Probabilistic Programming Languages and functionalities on Discourse for a discussion of the available packages.","category":"page"},{"location":"probability/probprog/","page":"Overview","title":"Overview","text":"Here, we will mainly either compute things explicit, for didactical purposes, or use Turing.jl, with real applications in mind. It is easy to combine Turing.jl with other packages, such as those from SciML, to do Bayesing inference on models involving all sorts of differential equations.","category":"page"},{"location":"README/#How-to-docs","page":"How to docs","title":"How to docs","text":"","category":"section"},{"location":"README/#Necessary-files","page":"How to docs","title":"Necessary files","text":"","category":"section"},{"location":"README/","page":"How to docs","title":"How to docs","text":"Have a docs/ folder with a make.jl file and a scr/ folder and at least an index.md file inside scr/;\nThe make.jl file should have, at least, using Documenter and a makedocs(sitename=\"name of site\").\nThe index.md can have whatever.","category":"page"},{"location":"README/#Directly-with-Documenter","page":"How to docs","title":"Directly with Documenter","text":"","category":"section"},{"location":"README/","page":"How to docs","title":"How to docs","text":"cd to root directory;\nactivate root project;\nmake sure you have Documenter.jl and all the other relevant packages added to the project;\ncreate/update the html docs with > include(\"docs/make.jl\");\nopen the indicated local web page in a browser (usually [http://localhost:8000])","category":"page"},{"location":"README/#Via-LiveServer","page":"How to docs","title":"Via LiveServer","text":"","category":"section"},{"location":"README/","page":"How to docs","title":"How to docs","text":"Have LiveServer.jl installed in the global environment or in /docs;\nDo using LiveServer;\nDo julia> servedocs();\nWait for the browser to pop-up with the local documentation page.","category":"page"},{"location":"#Welcome","page":"Random Notes","title":"Welcome","text":"","category":"section"},{"location":"","page":"Random Notes","title":"Random Notes","text":"These are just random notes about math and the julia language, which eventually may go into some future lecture notes.","category":"page"},{"location":"probability/find_pi/#Estimating-the-value-of-Ï€-via-frequentist-and-Bayesian-methods","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating the value of Ï€ via frequentist and Bayesian methods","text":"","category":"section"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"In the frequentist approach, we draw a number of samples uniformly distributed in the unit square and compute how many of them fall into the quarter circle. This yields an estimate for the area of the quarter circle along with confidence intervals. ","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"In the Bayesian approach, we start with a prior estimating the value of pi and update our prior to refine the estimate and the confidence levels, according to the posterior.","category":"page"},{"location":"probability/find_pi/#The-Julia-packages","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"The Julia packages","text":"","category":"section"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"For the numerical implementation, we need some packages.","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"We use Distributions.jl for the common distributions, such as Uniform, Beta, Bernoulli, Normal, etc.","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"using Distributions","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"For reproducibility, we set the seed for the pseudo random number generators.","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"using Random\nRandom.seed!(12)","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"For plotting, we use StatsPlots.jl","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"using StatsPlots","category":"page"},{"location":"probability/find_pi/#The-frequentist-approach","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"The frequentist approach","text":"","category":"section"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"This is a classical example illustrating the Monte Carlo method. We generate a bunch of samples (x_i y_i) from two independent uniform distributions on the interval 0 1 and check whether they belong to the unit circle (quarter circle, more precisely) or not, i.e. whether x_i^2 + y_i^2 leq 1 The distribution uniformly fills up the unit square, which has area one, and some of them will be in the quarter circle. The proportion of those in the circle approximates the area of the quarter circle over that of the unit square, i.e. pi4. Multiplying it by four, yields an estimate for pi. The more samples we use, the closer we expect the mean to approximate this value.","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"We start choosing a maximum number N of samples. We will analyse the estimate for each i up to N, to have an idea how the value and our confidence on it improves with the number of samples.","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"N = 10_000","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"Now we sample N pairs of numbers uniformly on the unit square","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"positions_f = rand(N, 2)","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"With the sample at hand, we compute their distance to the origin and check whether they belong to the unit circle or not, giving a sequence x_f of random variables with values 1 or 0, with the respective indication.","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"distance_f = sum(abs2, positions_f, dims=2)\nx_f = vec(distance_f) .â‰¤ 1","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"For each n between 1 and N, we compute the sample mean q_f[n] of x_f[1], â€¦, x_f[n], and the sample standard error s_f[n].","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"q_f = cumsum(x_f) ./ (1:N)\ns_f = [1.0; [âˆš(var(view(x_f, 1:n), mean=q_f[n])/n) for n in 2:N]]","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"The sample means approximate the value of pi4, so we multiply it by 4 to have an estimate of pi. Accordingly, we multipy the standard error by 4. The 95% confidence interval is estimated by twice the standard error around the mean. This is illustrated in the following plots. Of course, for small samples, we should use the t-Student distribution, but we concentrate on not-so-small samples and just use the normal distribution, relying on the Central Limit Theorem.","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"plot(10:N, 4q_f[10:N], ribbon = 8s_f[10:N], label=\"estimate\")\nhline!(10:N, [Ï€], label=\"true value\")","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"A close up of the 10% first samples","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"Nrange = 10:min(N, div(N, 10))\nplot(Nrange, 4q_f[Nrange], ribbon = 8s_f[Nrange], label=\"estimate\")\nhline!(Nrange, [Ï€], label=\"true value\")","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"The probability distribution for the estimate of pi after N samples is illustrated below in a few cases.","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"pp = 0.0:0.001:1.0\n\nplt = plot(title=\"Evolution of our belief in the value of Ï€\\nwith respect to the sample size n\", titlefont=10, xlims=(0.5, 1.0))\n    \nfor n in (div(N, 1000), div(N, 100), div(N, 10), N)\n    plot!(plt, pp, pdf.(Normal(q_f[n], s_f[n]), pp), label=\"n=$n\", fill=true)\nend\nvline!(plt, [Ï€/4], color=:black, label=\"Ï€/4\")\ndisplay(plt)","category":"page"},{"location":"probability/find_pi/#The-Bayesian-approach","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"The Bayesian approach","text":"","category":"section"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"In the Bayesian approach, we start guessing the area of the quarter circle, or, more precisely, the probability that it be a certain value within a certain range. It is reasonable to assume it is a little over half the area of the unit circle and not too close to 1, with higher probability of being closer to the middle of these two values. We could use a normal distribution, but a better choice is a Beta distribution since it is conjugate to the likelihood, which is expected to be a Beta distribution B(alpha beta) with density x^alpha (1 - x)^beta, where alpha counts as the number of success draws (within the quarter circle) and beta as the number of failures (outside the quarter circle). ","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"So we choose as prior the distribution Beta(alpha beta) with something like alpha = 24 and beta = 8, in which case the mean is alpha  (alpha + beta) = 2432 = 34 = 075 and the variance is Î±Î²((Î± + Î²)^2(Î± + Î² + 1)) = 19233792  000568. These are our hyperparameters. Let us visualize this prior distribution","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"prior_distribution = Beta(24,8)\n\nplt = plot(pp, pdf.(prior_distribution, pp), label=nothing, title=\"Density of the (prior) distribution $prior_distribution\\nmean = $(round(mean(prior_distribution), sigdigits=4)); standard deviation = $(round(std(prior_distribution), sigdigits=5))\", titlefont=10, fill=true, legend=:topleft)\n\nvline!(plt, [mean(prior_distribution)], label=\"mean $(round(mean(prior_distribution), sigdigits=4))\")\n\nvline!(plt, mean(prior_distribution) .+ [-std(prior_distribution), +std(prior_distribution)], label=\"mean +/- std: $(round(mean(prior_distribution), sigdigits=4)) +/- $(round(std(prior_distribution), sigdigits=5))\")\n\ndisplay(plt)","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"Just for the sake of illustration, we draw some sample from the prior","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"priordata = rand(prior_distribution, N)","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"Now we generate some \"real\" data. Since we know pi with high precision, we use that to generate the data, which are Bernoulli trials with probability pi4, and check the mean, which should be close to p_true.","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"p_true = Ï€/4","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"data = rand(Bernoulli(p_true), N)\n\nmean(data)","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"We update our prior in closed form, since the prior is a Beta distribution, which is a conjugate prior to the Bernoulli distribution. It is updated by simply counting the number of sucesses and the number of failures to the shape parameters alpha and beta, respectively.","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"function update_belief(prior_distribution::Beta, data::AbstractArray{Bool})\n    # Count the number of successes and failures.\n    successes = sum(data)\n    failures = length(data) - successes\n\n    # Update our prior belief using the fact that Beta is conjugate distribution.\n    return Beta(prior_distribution.Î± + successes, prior_distribution.Î² + failures)\nend","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"In order to see the evolution of the posterior with respect to the added evidence, we first we update the prior with a small part of the data","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"Ns = div.(N, (1000, 100, 10, 1))\nposterior_distributions = Dict(n => update_belief(prior_distribution, view(data, 1:n)) for n in Ns)","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"Now we visualize the posterior, which is the updated distribution.","category":"page"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"plt = plot(title=\"Density of the posterior distributions Beta(Î±', Î²')\", titlefont=10, legend=:topleft, xlims=(0.5, 1.0))\nfor n in Ns\n    distr = posterior_distributions[n]\n    plot!(plt, pp, pdf.(distr, pp), label=\"N=$n; Î±'=$(distr.Î±), Î²'=$(distr.Î²), mean=$(round(mean(distr), sigdigits=4))\", fill=true, alpha=0.5)\nend\nvline!(plt, [Ï€/4], label=nothing, color=:black)","category":"page"},{"location":"probability/find_pi/#Performance-of-the-two-methods","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Performance of the two methods","text":"","category":"section"},{"location":"probability/find_pi/","page":"Estimating Ï€ via frequentist and Bayesian methods","title":"Estimating Ï€ via frequentist and Bayesian methods","text":"It is not so relevant to compare the performance of the two methods above since this is a toy problem and not representative of the variety of situations that can be handled by either frequentist or Bayesian approaches. Drawing pseudo random numbers is pretty cheap and the frequentist approach above is much faster, even with the code not optimized for performance. The use of the Bayesian approach is more relevant in more complex cases and where sampling is more expensive. But more important than that is the perspective of the Bayesian approach of treating parameters as random variables and of their distributions as uncertainties, or quantifiers of our belief on their values.","category":"page"}]
}
